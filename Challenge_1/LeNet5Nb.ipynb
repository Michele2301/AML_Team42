{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# Imports\n",
    "%reset -f\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from datasets.CactusDataset import CactusDataset\n",
    "from models.LeNet5 import LeNet5\n",
    "from torch.utils.data import ConcatDataset\n",
    "import os\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "def get_label_distribution(dataset):\n",
    "    label_counts = {}\n",
    "    for _, _, label in dataset:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "    sorted_distribution = sorted(label_counts.items(), key=lambda x: x[0], reverse=True)\n",
    "    # take only the count\n",
    "    return [x[1] for x in sorted_distribution]\n",
    "\n",
    "\n",
    "# load data for displaying\n",
    "dataset = CactusDataset(root_dir='./data/train/train',labels_path='./data/train.csv')\n",
    "# take the first sample from train_dataloader\n",
    "_, train_features, train_labels = dataset[0]\n",
    "image_np = np.array(train_features)\n",
    "print(\"Image shape: \"+str(image_np.shape))\n",
    "print(\"Image python class\"+str(type(train_features)))\n",
    "print(\"Label: \"+str(train_labels))\n",
    "\n",
    "label_distribution = get_label_distribution(dataset) # they are sorted in ascending order\n",
    "print(label_distribution)\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(label_distribution, labels=['no cactus','cactus'], autopct='%1.1f%%')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "transform_dataset = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = CactusDataset(\n",
    "    root_dir=\"./data/train/train\",\n",
    "    labels_path=\"./data/train.csv\",\n",
    "    transform=transform_dataset \n",
    ")\n",
    "_, image, label = dataset[0]\n",
    "print(\"Image python class\" + str(type(image)))\n",
    "print(\"Image shape: \" + str(image.shape))\n",
    "print(\"Label shape: \" + str(label))\n",
    "# show the image\n",
    "plt.imshow(image.permute(1,2,0))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "dataloader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "for i, (img_name, images, labels) in enumerate(dataloader):\n",
    "    print(\"Batch number: \" + str(i))\n",
    "    print(\"Batch image names: \" + str(img_name))\n",
    "    print(\"Batch images shape: \" + str(images.shape))\n",
    "    print(\"Batch labels shape: \" + str(labels.shape))\n",
    "    break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "def compute_mean_std(dataset):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for _, images, _ in dataset:\n",
    "        mean += images.mean()\n",
    "        std += images.std()\n",
    "    mean /= len(dataset)\n",
    "    std /= len(dataset)\n",
    "    return mean, std\n",
    "\n",
    "# --------- DATA AUGMENTATION ---------\n",
    "# filter the dataset to only have no cactus images\n",
    "dataset = CactusDataset(\n",
    "    root_dir=\"./data/train/train\",\n",
    "    labels_path=\"./data/train.csv\",\n",
    "    transform=transform_dataset\n",
    ")\n",
    "\n",
    "no_cactus_dataset = dataset.filter(0)\n",
    "\n",
    "# create a concatenated dataset with an equal number of cactus and no cactus images\n",
    "print(\"**** BEFORE ****\")\n",
    "print(\"Number of no cactus images: \" + str(no_cactus_dataset.__len__()))\n",
    "print(\"Number of cactus images: \" + str(dataset.__len__()-no_cactus_dataset.__len__()))\n",
    "\n",
    "mean, std = compute_mean_std(ConcatDataset([dataset, no_cactus_dataset, no_cactus_dataset]))\n",
    "print(\"Mean: \" + str(mean))\n",
    "print(\"Std: \" + str(std))\n",
    "\n",
    "# merged_dataset_transforms=transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.RandomRotation(20),\n",
    "#     transforms.RandomEqualize(1),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=mean, std=std),\n",
    "# ])\n",
    "\n",
    "# merged_dataset_transforms=transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=mean, std=std),\n",
    "# ])\n",
    "\n",
    "merged_dataset_transforms=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = CactusDataset(\n",
    "    root_dir=\"./data/train/train\",\n",
    "    labels_path=\"./data/train.csv\",\n",
    "    transform=merged_dataset_transforms\n",
    ")\n",
    "\n",
    "added_dataset = CactusDataset(\n",
    "    root_dir = \"./data/test/test\",\n",
    "    labels_path = \"./data/test.csv\",\n",
    "    transform = merged_dataset_transforms\n",
    ")\n",
    "\n",
    "print(\"Added dataset length: \" + str(len(added_dataset)))\n",
    "\n",
    "no_cactus_dataset = dataset.filter(0)\n",
    "dataset_merged = ConcatDataset([dataset,no_cactus_dataset,no_cactus_dataset])\n",
    "\n",
    "dataset_merged_len = 0\n",
    "no_cactus_merged_len = 0\n",
    "for set in dataset_merged.datasets:\n",
    "    dataset_merged_len += set.__len__()\n",
    "    tmp = set.filter(0)\n",
    "    no_cactus_merged_len += tmp.__len__()\n",
    "\n",
    "print(\"\\n**** AFTER ****\")\n",
    "print(\"Number of no cactus images: \" + str(no_cactus_merged_len))\n",
    "print(\"Number of cactus images: \" + str(dataset_merged_len - no_cactus_merged_len))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "# --------- CREATING THE DATA LOADER AND TRAIN/VAL SPLIT ---------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "dataset_used = dataset_merged\n",
    "train_size = int(0.8 * len(dataset_used))\n",
    "print(\"Train size: \" + str(train_size))\n",
    "test_size = len(dataset_used) - train_size\n",
    "print(\"Test size: \" + str(test_size))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset_used, [train_size, test_size])\n",
    "\n",
    "# train_dataset = ConcatDataset([train_dataset, added_dataset])\n",
    "\n",
    "# Retrieve the lengths of the datasets\n",
    "print(\"dataset length: \" + str(len(dataset_used)))\n",
    "print(\"train length:\" + str(len(train_dataset)))\n",
    "print(\"val length:\" + str(len(val_dataset)))\n",
    "\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "\n",
    "#print a sample\n",
    "_, image,label=dataset_used.__getitem__(0)\n",
    "print(\"Image shape: \" + str(image.shape))\n",
    "print(\"Label shape: \" + str(label))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " # Phase 2: defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# !! COMMENT THIS CELL IF NOT USING APPLE SILICON CHIP !!\n",
    "\n",
    "# Metal Performance Shaders Acceleration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "model = LeNet5()\n",
    "model.to(device)\n",
    "print(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "log = True\n",
    "\n",
    "config = {\n",
    "    \"architecture\": \"LeNet5-notransf-notest\",\n",
    "    \"dataset\": \"Cactus\",\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"momentum\": 0.9\n",
    "}\n",
    "\n",
    "if log:\n",
    "    # setting wandb\n",
    "    wandb.login()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "# --------- TRAINING ---------\n",
    "if log:\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project = \"Challenge_1\",\n",
    "    \n",
    "        # track hyperparameters and run metadata\n",
    "        config = {\n",
    "            \"architecture\": config[\"architecture\"],\n",
    "            \"dataset\": config[\"dataset\"],\n",
    "            \"epochs\": config[\"epochs\"],\n",
    "            \"learning_rate\": config[\"learning_rate\"],\n",
    "            \"batch_size\": config[\"batch_size\"],\n",
    "            \"momentum\": config[\"momentum\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True,pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=wandb.config.batch_size, shuffle=True,pin_memory=True)\n",
    "    if not os.path.exists('./weights/lenet5_model.pth'):\n",
    "        model.train_model(train_dataloader, val_dataloader, epochs=wandb.config.epochs, lr=wandb.config.learning_rate, device=device, wandb=wandb, freeze=False)\n",
    "    model.train_model(train_dataloader, val_dataloader, epochs=wandb.config.epochs, lr=wandb.config.learning_rate, device=device, wandb=wandb)\n",
    "    \n",
    "    wandb.finish()\n",
    "else:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True,pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=True,pin_memory=True)\n",
    "    if not os.path.exists('./weights/lenet5_model.pth'):\n",
    "        model.train_model(train_dataloader, val_dataloader, epochs=config[\"epochs\"], lr=config[\"learning_rate\"], device=device, freeze=False)\n",
    "    model.train_model(train_dataloader, val_dataloader, epochs=config[\"epochs\"], lr=config[\"learning_rate\"], device=device)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
