{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Challenge_1: Aerial Cactus-AML",
   "id": "be3c022e1a3c112d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:27.275353Z",
     "start_time": "2024-05-02T14:09:26.998423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "%reset -f\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "from customDatasets.CactusDataset import CactusDataset\n",
    "from models.CactusResNet34 import CactusModel\n",
    "from torch.utils.data import ConcatDataset"
   ],
   "id": "8d12dabd2c81384b",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:27.280673Z",
     "start_time": "2024-05-02T14:09:27.276945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# free gpu\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "64549ce1a581d0ea",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Phase 1: Loading the data, analyzing it, doing feature engineering and creating the Loader",
   "id": "8e719abcd6683eb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:27.298978Z",
     "start_time": "2024-05-02T14:09:27.281939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load data for displaying\n",
    "dataset=CactusDataset(root_dir='./data/train/train',labels_path='./data/train.csv')\n",
    "# take the first sample from train_dataloader\n",
    "_, train_features, train_labels = dataset[0]\n",
    "image_np = np.array(train_features)\n",
    "print(\"Image shape: \"+str(image_np.shape))\n",
    "print(\"Image python class\"+str(type(train_features)))\n",
    "print(\"Label: \"+str(train_labels))"
   ],
   "id": "b8826ca650e99f97",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:27.433415Z",
     "start_time": "2024-05-02T14:09:27.300549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# show the image\n",
    "plt.imshow(image_np)"
   ],
   "id": "71ca919c7003ae70",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:27.768670Z",
     "start_time": "2024-05-02T14:09:27.434715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a transformation to convert PIL image to PyTorch tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "cactus_images = []\n",
    "no_cactus_images = []\n",
    "for i in range(len(dataset)):\n",
    "    _, image, label = dataset[i]\n",
    "    # Apply the transformation to convert PIL image to PyTorch tensor\n",
    "    image = transform(image)\n",
    "    if label == 1 and len(cactus_images) < 3:\n",
    "        cactus_images.append((image, 1))\n",
    "    elif label == 0 and len(no_cactus_images) < 3:\n",
    "        no_cactus_images.append((image, 0))\n",
    "    if len(cactus_images) == 3 and len(no_cactus_images) == 3:\n",
    "        break\n",
    "\n",
    "# Concatenate the vectors\n",
    "images = cactus_images + no_cactus_images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i, (image, label) in enumerate(images):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    # Convert the PyTorch tensor back to numpy array and permute dimensions\n",
    "    ax.imshow(image.permute(1, 2, 0).numpy())\n",
    "    ax.set_title('Cactus' if label == 1 else 'No cactus')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "51d840493366fad2",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:27.773281Z",
     "start_time": "2024-05-02T14:09:27.769723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_graph=False\n",
    "if label_graph:\n",
    "    def get_label_distribution(data):\n",
    "        label_counts = {}\n",
    "        for _, _, label in data:\n",
    "            if label not in label_counts:\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1\n",
    "        sorted_distribution = sorted(label_counts.items(), key=lambda x: x[0])\n",
    "        # take only the count\n",
    "        return [x[1] for x in sorted_distribution]\n",
    "    label_distribution = get_label_distribution(dataset) # they are sorted in ascending order\n",
    "    print(label_distribution)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(label_distribution, labels=['no cactus','cactus'], autopct='%1.1f%%')\n",
    "    plt.show()"
   ],
   "id": "1b137c14a6d6abc7",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:27.782355Z",
     "start_time": "2024-05-02T14:09:27.774420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_gen=False\n",
    "if graph_gen:\n",
    "    # Define the transformation\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    # Load the test dataset\n",
    "    test_dataset = CactusDataset(root_dir='./data/test/test', labels_path=None, transform=transform)\n",
    "    \n",
    "    # Create the data loader for the test dataset\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Load the training dataset\n",
    "    dataset = CactusDataset(root_dir='./data/train/train', labels_path='./data/train.csv', transform=transform)\n",
    "    \n",
    "    # Create the data loader for the training dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Compute the average value for each channel and for each pixel in the training set\n",
    "    mean_train = torch.zeros(3, 32, 32)\n",
    "    for _, images, _ in dataloader:\n",
    "        mean_train += images.mean(dim=0)\n",
    "    mean_train /= len(dataloader)\n",
    "    \n",
    "    # Compute the average value for each channel and for each pixel in the test set\n",
    "    mean_test = torch.zeros(3, 32, 32)\n",
    "    for _, images, _ in test_dataloader:\n",
    "        mean_test += images.mean(dim=0)\n",
    "    mean_test /= len(test_dataloader)\n",
    "    \n",
    "    print(mean_train.shape)\n",
    "    print(mean_test.shape)\n",
    "    \n",
    "    # flatten dimensions 2 and 3\n",
    "    mean_train = mean_train.flatten(1, 2)\n",
    "    mean_test = mean_test.flatten(1, 2)\n",
    "    \n",
    "    print(mean_train.shape)\n",
    "    print(mean_test.shape)\n",
    "    \n",
    "    # create a plot for each channel and put in it x=pixel number and y=value taken from main_train and test_train using histograms\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(15, 10))\n",
    "    for i in range(3):\n",
    "        ax[i].plot(mean_train[i], label='Train')\n",
    "        ax[i].plot(mean_test[i], label='Test')\n",
    "        ax[i].set_title(f'Mean value for channel {i}')\n",
    "        ax[i].set_xlabel('Pixel number')\n",
    "        ax[i].set_ylabel('Mean value')\n",
    "        ax[i].legend()"
   ],
   "id": "e3bce3f1010f08af",
   "execution_count": 36,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:27.790146Z",
     "start_time": "2024-05-02T14:09:27.783626Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Number of train values: \"+str(dataset.__len__()))",
   "id": "b7acaba6545f4ac8",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:28.029467Z",
     "start_time": "2024-05-02T14:09:27.791179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform_dataset = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = CactusDataset(root_dir=\"./data/train/train\",labels_path=\"./data/train.csv\",transform=transform_dataset)\n",
    "_, image, label = dataset[0]\n",
    "print(\"Image python class\"+str(type(image)))\n",
    "print(\"Image shape: \"+str(image.shape))\n",
    "print(\"Label shape: \"+str(label))\n",
    "# show the image\n",
    "plt.imshow(image.permute(1,2,0))"
   ],
   "id": "97db5bff212a55dd",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:28.034852Z",
     "start_time": "2024-05-02T14:09:28.032224Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader=DataLoader(dataset,batch_size=32,shuffle=True)",
   "id": "8a114828e6536b6e",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:28.457589Z",
     "start_time": "2024-05-02T14:09:28.036070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, (img_name, images, labels) in enumerate(dataloader):\n",
    "    print(\"Batch number: \"+str(i))\n",
    "    print(\"Batch image names: \"+str(img_name))\n",
    "    print(\"Batch images shape: \"+str(images.shape))\n",
    "    print(\"Batch labels shape: \"+str(labels.shape))\n",
    "    break"
   ],
   "id": "b423023edf5b599b",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:19:31.148374Z",
     "start_time": "2024-05-02T14:09:28.458943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLIP=True\n",
    "if CLIP:\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    from torch.utils.data import DataLoader\n",
    "    import torch\n",
    "    from torchvision.transforms import ToPILImage\n",
    "    \n",
    "    # Assuming you have defined CLIPProcessor, CLIPModel, and test_dataloader\n",
    "    \n",
    "    test_dataset = CactusDataset(root_dir='./data/test/test', labels_path=None, transform=transforms.Compose([transforms.Resize(224),transforms.ToTensor()]))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    dataset = CactusDataset(root_dir='./data/train/train', labels_path='./data/train.csv', transform=transforms.Compose([transforms.ToTensor()]))   \n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize the CLIP model and processor\n",
    "    model_id = \"openai/clip-vit-base-patch32\"\n",
    "    processor = CLIPProcessor.from_pretrained(model_id)\n",
    "    model = CLIPModel.from_pretrained(model_id)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Define a function to classify images\n",
    "    def classify_images(dataloader, model, processor):\n",
    "        predictions = []\n",
    "        candidate_labels = [(\"a photo without a cactus\",0),(\"a photo with a cactus\",1)]  # Candidate labels for classification\n",
    "        for image_names, images, _ in dataloader:\n",
    "            # Convert images to PIL format\n",
    "            images = [ToPILImage()(image) for image in images]\n",
    "            # Process the images\n",
    "            inputs = processor(text=[t[0] for t in candidate_labels], images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "            # Forward pass through the model\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "    \n",
    "            # Extract logits\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "    \n",
    "            # Get predicted labels\n",
    "            predicted_labels = torch.argmax(logits_per_image, dim=1)\n",
    "    \n",
    "            # Map predicted labels to candidate labels\n",
    "            predicted_classes = [candidate_labels[label] for label in predicted_labels]\n",
    "            # Extend predictions list\n",
    "            predictions.extend(predicted_classes)\n",
    "    \n",
    "        return predictions\n",
    "    \n",
    "    # Classify images\n",
    "    #predictions = classify_images(test_dataloader, model, processor)\n",
    "    predictions_training = classify_images(dataloader, model, processor)\n",
    "    # evaluate the predictions_training and compare them with the provided labels\n",
    "    # get the labels\n",
    "    labels = []\n",
    "    for _, _, label in dataset:\n",
    "        labels.append(label)\n",
    "    # compare the predictions_training with the labels\n",
    "    right=0\n",
    "    for i in range(len(labels)):\n",
    "        if predictions_training[i][1]==labels[i]:\n",
    "            right+=1\n",
    "            \n",
    "    print(right/len(labels))\n"
   ],
   "id": "9becb399e84a10ae",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:19:31.157702Z",
     "start_time": "2024-05-02T14:19:31.149549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clustering=False\n",
    "if clustering:\n",
    "    # using unsurpervised technique to label the images (splitting it in 2 clusters using kmeans)\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    # load testset\n",
    "    test_dataset = CactusDataset(root_dir='./data/test/test', labels_path=None, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "    # use it for kmeans\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    # get the images\n",
    "    images = []\n",
    "    images_names_KMeans = []\n",
    "    for image_name, image, _ in test_dataloader:\n",
    "        images.append(image)\n",
    "        images_names_KMeans.append(image_name)\n",
    "    print(images[0].shape)\n",
    "    print(images_names_KMeans[0])\n",
    "    # concatenate the images_names\n",
    "    # Initialize an empty list to store the concatenated image names\n",
    "    images_names_KMeans_concatenated = []\n",
    "    \n",
    "    # Iterate through the list of lists of image names and concatenate them\n",
    "    for names_list in images_names_KMeans:\n",
    "        images_names_KMeans_concatenated.extend(names_list)\n",
    "    images_names_KMeans = images_names_KMeans_concatenated\n",
    "    images = torch.cat(images, dim=0)\n",
    "    print(images.shape)\n",
    "    images_KMeans = images.view(images.size(0), -1)\n",
    "    print(images_KMeans.shape)\n",
    "    silhouette_scores = []\n",
    "    silhouette_scores_hierarchical = []\n",
    "    for n_clusters in range(2, 11):\n",
    "        # do kmeans\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42,max_iter=1000)\n",
    "        labels = kmeans.fit_predict(images_KMeans)\n",
    "        # get the silhouette score\n",
    "        silhouette = silhouette_score(images_KMeans, labels)\n",
    "        silhouette_scores.append(silhouette)\n",
    "        print(\"Silhouette score: \"+str(silhouette))\n",
    "        # do the same with hierarchical clustering\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "        labels_hierarchical = hierarchical.fit_predict(images_KMeans)\n",
    "        silhouette_hierarchical = silhouette_score(images_KMeans, labels_hierarchical)\n",
    "        print(\"Silhouette score: \"+str(silhouette_hierarchical))\n",
    "        silhouette_scores_hierarchical.append(silhouette_hierarchical)\n",
    "    \n",
    "    # plot them\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax[0].plot(range(2, 11), silhouette_scores)\n",
    "    ax[0].set_title('KMeans')\n",
    "    ax[0].set_xlabel('Number of clusters')\n",
    "    ax[0].set_ylabel('Silhouette score')\n",
    "    ax[1].plot(range(2, 11), silhouette_scores_hierarchical)\n",
    "    ax[1].set_title('Hierarchical')\n",
    "    ax[1].set_xlabel('Number of clusters')\n",
    "    ax[1].set_ylabel('Silhouette score')\n",
    "    plt.show()"
   ],
   "id": "2ebdafdbbf53d33c",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:19:31.172822Z",
     "start_time": "2024-05-02T14:19:31.159237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if clustering:\n",
    "    # take 2 clusters\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    labels = kmeans.fit_predict(images_KMeans)\n",
    "    # get the cluster centers\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    # get the labels\n",
    "    labels_KMeans = kmeans.labels_\n",
    "    print(len(labels_KMeans))\n",
    "    print(labels_KMeans.shape)"
   ],
   "id": "fd86ecedbbc078e9",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:19:31.180364Z",
     "start_time": "2024-05-02T14:19:31.174017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if clustering:\n",
    "    # Load the dataset\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    dataset = CactusDataset(root_dir=\"./data/train/train\", labels_path=\"./data/train.csv\", transform=transforms.Compose([transforms.ToTensor()]))\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize lists to store images, image names, and labels\n",
    "    images = []\n",
    "    images_name = []\n",
    "    img_labels = []\n",
    "    print(\"Number of images: \"+str(len(dataset)))\n",
    "    # Iterate through the dataloader to collect images, names, and labels\n",
    "    for img_name, image, img_label in dataloader:\n",
    "        images.append(image.view(image.size(0), -1))  # Flatten the images\n",
    "        images_name.extend(img_name)\n",
    "        img_labels.extend(img_label)\n",
    "    \n",
    "    print(\"Number of images: \"+str(len(images)))\n",
    "    print(\"shape of images: \"+str(images[0].shape))\n",
    "    # Concatenate the flattened images\n",
    "    images = torch.cat(images, dim=0)\n",
    "    print(\"shape of images: \"+str(images.shape))\n",
    "    # Perform k-means clustering\n",
    "    for n_clusters in range(2, 11):\n",
    "        kmeans = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "        labels = kmeans.fit_predict(images)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = sum(labels == img_labels)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"Number of correct predictions:\", correct)\n",
    "        print(\"Total images:\", len(images))\n",
    "        print(\"Accuracy:\", correct / len(images))"
   ],
   "id": "56ba1e012896961a",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:43.320042Z",
     "start_time": "2024-05-02T14:19:31.181463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------- DATA AUGMENTATION ---------\n",
    "# filter the dataset to only have no cactus images\n",
    "dataset = CactusDataset(root_dir=\"./data/train/train\",labels_path=\"./data/train.csv\",transform=transform_dataset)\n",
    "no_cactus_dataset = dataset.filter(0)\n",
    "# create a concatenated dataset with an equal number of cactus and no cactus images\n",
    "print(\"Number of no cactus images: \"+str(no_cactus_dataset.__len__()))\n",
    "print(\"Number of cactus images: \"+str(dataset.__len__()-no_cactus_dataset.__len__()))\n",
    "def compute_mean_std(dataset):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for _, images, _ in dataset:\n",
    "        mean += images.mean()\n",
    "        std += images.std()\n",
    "    mean /= len(dataset)\n",
    "    std /= len(dataset)\n",
    "    return mean, std\n",
    "mean, std = compute_mean_std(ConcatDataset([dataset, no_cactus_dataset, no_cactus_dataset]))\n",
    "print(\"Mean: \"+str(mean))\n",
    "print(\"Std: \"+str(std))\n",
    "# load test_dataset and we do the same\n",
    "mean_test, std_test = compute_mean_std(CactusDataset(root_dir=\"./data/test/test\",labels_path=None,transform=transform_dataset))\n",
    "print(\"Mean test: \"+str(mean_test))\n",
    "print(\"Std test: \"+str(std_test))"
   ],
   "id": "78da67bcd57de19d",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:43.352748Z",
     "start_time": "2024-05-02T14:20:43.321113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "merged_dataset_transforms=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "dataset=CactusDataset(root_dir=\"./data/train/train\",labels_path=\"./data/train.csv\",transform=merged_dataset_transforms)\n",
    "no_cactus_dataset=dataset.filter(0)\n",
    "dataset_merged=ConcatDataset([dataset,no_cactus_dataset,no_cactus_dataset])"
   ],
   "id": "f44d2be039fa0288",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.213761Z",
     "start_time": "2024-05-02T14:20:43.354019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------- CREATING THE DATA LOADER AND TRAIN/VAL SPLIT ---------\n",
    "torch.manual_seed(42)\n",
    "dataset_used=dataset_merged\n",
    "train_size = int(0.7 * len(dataset_used))\n",
    "print(\"Train size: \" + str(train_size))\n",
    "test_size = len(dataset_used) - train_size\n",
    "print(\"Test size: \" + str(test_size))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset_used, [train_size, test_size])\n",
    "\n",
    "# Retrieve the lengths of the datasets\n",
    "print(\"dataset length: \" + str(len(dataset_used)))\n",
    "\n",
    "# TEST if they apply the transforms\n",
    "# Create DataLoader for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "# convert 6 images from the dataloaders to PIL images (and invert the normalization) and plot them\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Function to convert tensor to PIL image and invert normalization\n",
    "def tensor_to_pil_invert(tensor):\n",
    "    # Clone the tensor to not do changes on the original one\n",
    "    image = tensor.clone()\n",
    "    # Invert the normalization using mean and std from above\n",
    "    image[0] = image[0] * std + mean\n",
    "    image[1] = image[1] * std + mean\n",
    "    image[2] = image[2] * std + mean\n",
    "    # Convert the tensor to a PIL Image\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    return image\n",
    "\n",
    "# Get 6 images from train and validation dataloaders\n",
    "train_batch = next(iter(train_dataloader))\n",
    "val_batch = next(iter(val_dataloader))\n",
    "\n",
    "# Select 6 images from each batch\n",
    "train_images = train_batch[1][:6]  # Assuming images are in the first element of the batch tuple\n",
    "val_images = val_batch[1][:6]\n",
    "\n",
    "# Convert tensors to PIL images and invert normalization\n",
    "train_pil_images = [tensor_to_pil_invert(image) for image in train_images]\n",
    "val_pil_images = [tensor_to_pil_invert(image) for image in val_images]\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "for i, image in enumerate(train_pil_images):\n",
    "    axes[0, i].imshow(image)\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title('Train Image {}'.format(i+1))\n",
    "for i, image in enumerate(val_pil_images):\n",
    "    axes[1, i].imshow(image)\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title('Validation Image {}'.format(i+1))\n",
    "plt.show()\n"
   ],
   "id": "8c02ae741b92ee88",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    " # Phase 2: defining the model"
   ],
   "id": "76cf552ec233ad5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.217239Z",
     "start_time": "2024-05-02T14:20:44.214812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log = False\n",
    "if log:\n",
    "    # setting wandb\n",
    "    wandb.login()"
   ],
   "id": "6eabcfcdb54b2aa9",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.229499Z",
     "start_time": "2024-05-02T14:20:44.218106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if log:\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"Challenge_1\",\n",
    "    \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"architecture\": \"ResNet34_FineTuned\",\n",
    "        \"dataset\": \"Cactus\",\n",
    "        \"freeze_epochs\": 5,\n",
    "        \"freeze_learning_rate\": 0.001,\n",
    "        \"epochs\": 5,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        }\n",
    "    )"
   ],
   "id": "85cdb469a146f456",
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.525964Z",
     "start_time": "2024-05-02T14:20:44.231002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CactusModel()\n",
    "model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(model)\n",
    "print(\"Number of parameters in the model: \"+str(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ],
   "id": "4b94cf203d64f71e",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.531348Z",
     "start_time": "2024-05-02T14:20:44.527132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train the model\n",
    "import os\n",
    "if log:\n",
    "    # Create DataLoader for training and validation\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True,pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=wandb.config.batch_size, shuffle=True,pin_memory=True)\n",
    "    if not os.path.exists('./weights/cactus_model.pth'):\n",
    "        model.train_model(train_dataloader, val_dataloader, epochs=wandb.config.freeze_epochs, lr=wandb.config.freeze_learning_rate, device=device, wandb=wandb, freeze=True)\n",
    "    model.train_model(train_dataloader, val_dataloader, epochs=wandb.config.epochs, lr=wandb.config.learning_rate, device=device, wandb=wandb)"
   ],
   "id": "f1fe2f7e9ad42bc",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.538276Z",
     "start_time": "2024-05-02T14:20:44.532502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "do_cm=False\n",
    "\n",
    "def calculate_cm(dataloader, model, device, threshold=0.5):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (image_names, images, labels) in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "    return confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "if do_cm:\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(\"Generating confusion matrix\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False,pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False,pin_memory=True)\n",
    "    # Get confusion matrices\n",
    "    train_cm = calculate_cm(train_dataloader, model, device)\n",
    "    val_cm = calculate_cm(val_dataloader, model, device)\n",
    "    \n",
    "    print(\"Training Confusion Matrix:\")\n",
    "    print(train_cm)\n",
    "    print(\"Validation Confusion Matrix:\")\n",
    "    print(val_cm)"
   ],
   "id": "2026451bf0f2761b",
   "execution_count": 52,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.546908Z",
     "start_time": "2024-05-02T14:20:44.539642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "do_roc=False\n",
    "def plot_roc_curve(dataloader, model, device):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (image_names, images, labels) in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "if do_roc:\n",
    "    plot_roc_curve(val_dataloader, model, device)"
   ],
   "id": "ed6df6e7dbb696e",
   "execution_count": 53,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.555666Z",
     "start_time": "2024-05-02T14:20:44.548219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate fscore, recall and precision\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "def calculate_metrics(dataloader, model, device):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (image_names, images, labels) in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    fscore = f1_score(all_labels, all_predictions)\n",
    "    return precision, recall, fscore\n",
    "\n",
    "do_f1=False\n",
    "if do_f1:\n",
    "    train_precision, train_recall, train_fscore = calculate_metrics(train_dataloader, model, device)\n",
    "    val_precision, val_recall, val_fscore = calculate_metrics(val_dataloader, model, device)\n",
    "    print(\"Training Precision: {:.2f}\".format(train_precision))\n",
    "    print(\"Training Recall: {:.2f}\".format(train_recall))\n",
    "    print(\"Training F1 Score: {:.2f}\".format(train_fscore))\n",
    "    print(\"Validation Precision: {:.2f}\".format(val_precision))\n",
    "    print(\"Validation Recall: {:.2f}\".format(val_recall))\n",
    "    print(\"Validation F1 Score: {:.2f}\".format(val_fscore))"
   ],
   "id": "36b83381d0ba9d17",
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.563329Z",
     "start_time": "2024-05-02T14:20:44.556804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use it to classify the test\n",
    "test_transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "if log:\n",
    "    # load the test dataset\n",
    "    test_dataset = CactusDataset(root_dir='./data/test/test', labels_path=None, transform=test_transform)\n",
    "    # create the data loader\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    out=model.predict_model(test_dataloader, device, './data/submission.csv')\n",
    "    print(out)\n",
    "\n",
    "if clustering:\n",
    "    # convert out from list((image_name, label)) to dictionary\n",
    "    out = dict(out)\n",
    "    \n",
    "    # compare it with images_KMeans and labels_KMeans\n",
    "    right=0\n",
    "    for i,image_name in enumerate(images_names_KMeans):\n",
    "        # compare it with out\n",
    "        if out[image_name]==labels_KMeans[i]:\n",
    "            right+=1\n",
    "            \n",
    "    print(right/len(images_names_KMeans))"
   ],
   "id": "81bb70d85a457c3e",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:20:44.580035Z",
     "start_time": "2024-05-02T14:20:44.564553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot roc curve\n",
    "import wandb\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "def plot_roc_curve(dataloader, model, device):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (image_names, images, labels) in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def print_accuracy_test(dataloader, model, device):\n",
    "    all_labels = []\n",
    "    all_predictions=model.predict_model(dataloader, device)\n",
    "    # extract all labels\n",
    "    for _, _, label in dataloader:\n",
    "            all_labels.extend(label)\n",
    "    print(len(all_labels))\n",
    "    print(all_labels[:10])\n",
    "    print(all_predictions[:10])\n",
    "    all_predictions = [x[1] for x in all_predictions]\n",
    "    print(len(all_predictions))\n",
    "    accuracy = np.sum(np.array(all_labels) == np.array(all_predictions)) / len(all_labels)\n",
    "    return accuracy\n",
    "\n",
    "def function_for_testing(transform_used):    \n",
    "    wandb.login()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # reset the model\n",
    "    if os.path.exists('./weights/cactus_model.pth'):\n",
    "        os.remove('./weights/cactus_model.pth')\n",
    "    model = CactusModel()\n",
    "    model.to(device)\n",
    "    # load the train dataset\n",
    "    train_dataset = CactusDataset(root_dir='./data/train/train', labels_path='./data/train.csv', transform=transform_used)\n",
    "    torch.manual_seed(42)\n",
    "    dataset_used=train_dataset\n",
    "    train_size = int(0.7 * len(dataset_used))\n",
    "    print(\"Train size: \" + str(train_size))\n",
    "    val_size = int(0.2*len(dataset_used))\n",
    "    print(\"Val size: \" + str(val_size))\n",
    "    test_size = len(dataset_used) - train_size - val_size\n",
    "    print(\"Test size: \" + str(test_size))\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset_used, [train_size, val_size, test_size])\n",
    "    # Create DataLoader for training, validation and test\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,pin_memory=True)\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"Challenge_1\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"architecture\": \"ResNet34_FineTuned\",\n",
    "        \"transform\": str(transform_used),\n",
    "        \"dataset\": \"Cactus\",\n",
    "        \"freeze_epochs\": 5,\n",
    "        \"freeze_learning_rate\": 0.001,\n",
    "        \"epochs\": 5,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        }\n",
    "    )\n",
    "    model.train_model(train_dataloader, val_dataloader, epochs=wandb.config.freeze_epochs, lr=wandb.config.freeze_learning_rate, device=device, wandb=wandb, freeze=True)\n",
    "    model.train_model(train_dataloader, val_dataloader, epochs=wandb.config.epochs, lr=wandb.config.learning_rate, device=device, wandb=wandb)\n",
    "    plot_roc_curve(val_dataloader, model, device)\n",
    "    accuracy = print_accuracy_test(test_dataloader, model, device)\n",
    "    print(\"Accuracy: \"+str(accuracy))    "
   ],
   "id": "542a5e08017a085",
   "execution_count": 56,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:24:19.994704Z",
     "start_time": "2024-05-02T14:20:44.585159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# do the tests\n",
    "# first case: no data augmentation\n",
    "transform_used=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "print(str(transform_used))\n",
    "function_for_testing(transform_used)"
   ],
   "id": "a50e0e9174f025b6",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# second case: data augmentation with only Normalization average=0 and std=1\n",
    "transform_used=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0, std=1)\n",
    "])\n",
    "function_for_testing(transform_used)"
   ],
   "id": "41ec639598bdfe5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# third case: data augmentation with only histogram equalization\n",
    "transform_used=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomEqualize(p=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "function_for_testing(transform_used)"
   ],
   "id": "4d5e7a8c3d3decf",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fourth case: data augmentation well done\n",
    "torch.manual_seed(42)\n",
    "dataset=CactusDataset(root_dir=\"./data/train/train\",labels_path=\"./data/train.csv\",transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "train_size = int(0.7 * len(dataset))\n",
    "rest_size = len(dataset) - train_size\n",
    "average_std_dataset, rest_dataset=torch.utils.data.random_split(dataset, [train_size, rest_size])\n",
    "mean, std = compute_mean_std(average_std_dataset)\n",
    "transform_used=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "function_for_testing(transform_used)"
   ],
   "id": "db20c97fbf04de14",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:25:31.081668Z",
     "start_time": "2024-05-02T14:25:30.915671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speed_space_test=True\n",
    "if speed_space_test:\n",
    "    torch.manual_seed(42)\n",
    "    # load dataset\n",
    "    dataset = CactusDataset(root_dir=\"./data/train/train\",labels_path=\"./data/train.csv\",transform=transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "    # do 70 20 10\n",
    "    from models.LeNet5 import LeNet5\n",
    "    model=LeNet5()\n",
    "    model.to(device)\n",
    "    print(\"Number of parameters in the model: \"+str(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(1)\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "    # Create DataLoader for training, validation and test\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,pin_memory=True)\n",
    "    # start timer\n",
    "    import time\n",
    "    start = time.time()\n",
    "    from torch.profiler import profile, record_function, ProfilerActivity\n",
    "    with profile(activities=[ProfilerActivity.CPU,ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            for batch in val_dataloader:\n",
    "                (_, images, _) = batch\n",
    "                images=images.to(device)\n",
    "                print(images.shape)\n",
    "                model(images)\n",
    "    # print time\n",
    "    print(\"Time: \"+str(time.time()-start))\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ],
   "id": "fbc10da60562a8c1",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# now using custom made resnet34\n",
    "from models.CustomResNet34 import CustomResNet34\n",
    "train_custom=False\n",
    "model = CustomResNet34()\n",
    "model.to(device)\n",
    "print(model)"
   ],
   "id": "4a44fd908f3271e7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if train_custom==True:\n",
    "    wandb.login()"
   ],
   "id": "675f735cca6066cf",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if train_custom==True:\n",
    "    wandb.init(\n",
    "        project=\"Challenge_1\",\n",
    "        config={\n",
    "        \"architecture\": \"ResNet34_CustomMade\",\n",
    "        \"dataset\": \"Cactus\",\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 32,\n",
    "        }\n",
    "    )"
   ],
   "id": "909ec7cc0bfd5989",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train the model\n",
    "if train_custom==True:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True,pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=wandb.config.batch_size, shuffle=True,pin_memory=True)\n",
    "    model.train_model(train_dataloader, val_dataloader, epochs=wandb.config.epochs, lr=wandb.config.learning_rate, device=device, wandb=wandb)"
   ],
   "id": "893f0a935ec37b17",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "do_cm=True\n",
    "\n",
    "def calculate_cm(dataloader, model, device, threshold=0.5):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (image_names, images, labels) in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "    return confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "\n",
    "if do_cm:\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(\"Generating confusion matrix\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False,pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False,pin_memory=True)\n",
    "    # Get confusion matrices\n",
    "    train_cm = calculate_cm(train_dataloader, model, device)\n",
    "    val_cm = calculate_cm(val_dataloader, model, device)\n",
    "    \n",
    "    print(\"Training Confusion Matrix:\")\n",
    "    print(train_cm)\n",
    "    print(\"Validation Confusion Matrix:\")\n",
    "    print(val_cm)"
   ],
   "id": "49e031248af58955",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate roc curve plot\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "def plot_roc_curve(dataloader, model, device):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (image_names, images, labels) in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            outputs = torch.round(outputs)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(val_dataloader, model, device)"
   ],
   "id": "dc520ad0e2eb31a9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate fscore, recall and precision\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "def calculate_metrics(dataloader, model, device):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (image_names, images, labels) in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    fscore = f1_score(all_labels, all_predictions)\n",
    "    return precision, recall, fscore\n",
    "\n",
    "train_precision, train_recall, train_fscore = calculate_metrics(train_dataloader, model, device)\n",
    "val_precision, val_recall, val_fscore = calculate_metrics(val_dataloader, model, device)\n",
    "print(\"Training Precision: {:.2f}\".format(train_precision))\n",
    "print(\"Training Recall: {:.2f}\".format(train_recall))\n",
    "print(\"Training F1 Score: {:.2f}\".format(train_fscore))\n",
    "print(\"Validation Precision: {:.2f}\".format(val_precision))\n",
    "print(\"Validation Recall: {:.2f}\".format(val_recall))\n",
    "print(\"Validation F1 Score: {:.2f}\".format(val_fscore))"
   ],
   "id": "b3f95e00690d3276",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
