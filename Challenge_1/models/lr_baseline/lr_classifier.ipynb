{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import CactusDataset\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "#use_gpu = torch.cuda.is_available()\n",
    "#if use_gpu:\n",
    "#    print(\"Using CUDA\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [
    "data_dir = '../../../data/train/train'\n",
    "csv_file = '../../../data/train.csv'\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "#set the transforms for the images\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "image_dataset = CactusDataset.CactusDataset(csv_file, data_dir, data_transforms)\n",
    "# split into train, eval, test\n",
    "train_size = int(0.7 * len(image_dataset))\n",
    "eval_size = int(0.2 * len(image_dataset))\n",
    "test_size = len(image_dataset) - train_size - eval_size\n",
    "train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(image_dataset, [train_size, eval_size, test_size])\n",
    "\n",
    "image_datasets = {TRAIN: train_dataset, VAL: eval_dataset, TEST: test_dataset}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n",
    "\n",
    "batch_sizes = {TRAIN: dataset_sizes[TRAIN], VAL: dataset_sizes[VAL], TEST: 64}\n",
    "\n",
    "dataloaders = {TRAIN: None, VAL: None, TEST: None}\n",
    "dataloaders[TRAIN] = DataLoader(image_datasets[TRAIN], batch_size=batch_sizes[TRAIN],\n",
    "                                             shuffle=True, num_workers=0)\n",
    "dataloaders[VAL] = DataLoader(image_datasets[VAL], batch_size=batch_sizes[VAL],\n",
    "                                             shuffle=False, num_workers=0)\n",
    "dataloaders[TEST] = DataLoader(image_datasets[TEST], batch_size=batch_sizes[TEST], shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "print(dataset_sizes)\n",
    "class_names = {0: 'No Cactus', 1: 'Cactus'}\n",
    "\n",
    "inputs, classes = next(iter(dataloaders[TRAIN]))\n",
    "print(inputs[0])\n",
    "\n",
    "n_features = 32 * 32 * 3\n",
    "\n",
    "print(n_features)\n",
    "\n",
    "#print nunmber of each class in each dataset\n",
    "for x in [TRAIN, VAL, TEST]:\n",
    "    print(\"Number of {} images: {}\".format(x, dataset_sizes[x]))\n",
    "    for i in range(2):\n",
    "        print(\"Number of {} images of class {}: {}\".format(x, class_names[i], sum([1 for j in image_datasets[x] if j[1] == i])))\n",
    "    print()\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "# Create model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "source": [
    "def train_model(model, criterion, optimizer, model_name, X_train, y_train, X_val, y_val, transforms=None, num_epochs=1000):\n",
    "    # training loop\n",
    "    num_epochs = num_epochs\n",
    "    epoch_best = 0\n",
    "    f1_scores = []\n",
    "    accs_train = []\n",
    "    accs_val = []\n",
    "    aucs = []\n",
    "    best_fpr = []\n",
    "    best_tpr = []\n",
    "    best_auc = 0\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "\n",
    "    #40k epoche runnate\n",
    "    for epoch in range(num_epochs):\n",
    "        #randomly horizontal flip the images in X_train\n",
    "        if transforms is not None:\n",
    "            X_train = transforms(X_train)\n",
    "        #use mini batches\n",
    "        # forward pass and loss\n",
    "        outputs = model(X_train)\n",
    "        #make y_predicted and y_train of same shape\n",
    "        outputs = outputs.view(-1)\n",
    "\n",
    "        preds = outputs.round()\n",
    "        acc_train = (preds == y_train).sum() / y_train.shape[0]\n",
    "        accs_train.append(acc_train)\n",
    "\n",
    "        loss = criterion(outputs, y_train.float())\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # updates\n",
    "        optimizer.step()\n",
    "        \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_val)  # no need to call model.forward()\n",
    "            y_predicted_cls = outputs.round()   # round off to nearest class\n",
    "            #flatten\n",
    "            y_val = y_val.view(-1)\n",
    "            y_predicted_cls = y_predicted_cls.view(-1)\n",
    "\n",
    "            #Compute f1 score\n",
    "            tp = (y_val * y_predicted_cls).sum()\n",
    "            tn = ((1 - y_val) * (1 - y_predicted_cls)).sum()\n",
    "            fp = ((1 - y_val) * y_predicted_cls).sum()\n",
    "            fn = (y_val * (1 - y_predicted_cls)).sum()\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            #Compute accuracy\n",
    "            acc = (y_predicted_cls == y_val).sum() / y_val.shape[0]   # accuracy\n",
    "            accs_val.append(acc)\n",
    "\n",
    "            #Compute AUC\n",
    "            fpr, tpr, _ = roc_curve(y_val, outputs)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            \n",
    "            if roc_auc > best_auc:\n",
    "                best_auc = roc_auc\n",
    "                best_fpr = fpr\n",
    "                best_tpr = tpr\n",
    "                epoch_best = epoch\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {loss.item()}, Accuracy on val: {acc}, F1 score on val: {f1}, AUC on val: {roc_auc}')\n",
    "    return accs_train, accs_val, aucs, f1_scores, best_fpr, best_tpr, best_auc, best_f1, epoch_best"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "#Load dataset, we flatten the images and we put them in a single numpy array in memory of shape (n_images, n_features)\n",
    "X_train = []\n",
    "y_train = []\n",
    "for images, labels in dataloaders[TRAIN]:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_train.append(images)\n",
    "    y_train.append(labels)\n",
    "X_train = torch.cat(X_train, dim=0)\n",
    "y_train = torch.cat(y_train, dim=0)\n",
    "X_val = []\n",
    "y_val = []\n",
    "for images, labels in dataloaders[VAL]:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_val.append(images)\n",
    "    y_val.append(labels)\n",
    "X_val = torch.cat(X_val, dim=0)\n",
    "y_val = torch.cat(y_val, dim=0)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_train[0])\n",
    "n_features = X_train.shape[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "source": [
    "#instantiate model\n",
    "model = LogisticRegression(n_features)\n",
    "# Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "source": [
    "# train model for 5000 epochs\n",
    "accs_train, accs_val, aucs, f1_scores, best_fpr, best_tpr, best_auc, best_f1, epoch_best = train_model(model, criterion, optimizer, 'LR', X_train, y_train, X_val, y_val, num_epochs=5000)\n",
    "print(f'Best AUC: {best_auc}, Best F1: {best_f1}, Best epoch: {epoch_best}')\n",
    "#plot accuracies of val and training\n",
    "plt.plot(accs_train, label='Train')\n",
    "plt.plot(accs_val, label='Val')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "source": [
    "#Load dataset, we flatten the images and we put them in a single numpy array in memory of shape (n_images, n_features)\n",
    "X_train = []\n",
    "y_train = []\n",
    "for images, labels in dataloaders[TRAIN]:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_train.append(images)\n",
    "    y_train.append(labels)\n",
    "X_train = torch.cat(X_train, dim=0)\n",
    "y_train = torch.cat(y_train, dim=0)\n",
    "X_val = []\n",
    "y_val = []\n",
    "for images, labels in dataloaders[VAL]:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_val.append(images)\n",
    "    y_val.append(labels)\n",
    "X_val = torch.cat(X_val, dim=0)\n",
    "y_val = torch.cat(y_val, dim=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "source": [
    "# retain only n_componenets feature with highest variance using PCA\n",
    "n_components = 612\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train.detach().numpy())\n",
    "X_val_pca = pca.transform(X_val.detach().numpy())\n",
    "#transform in tensors\n",
    "X_train_pca = torch.tensor(X_train_pca)\n",
    "X_val_pca = torch.tensor(X_val_pca)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "source": [
    "model = LogisticRegression(X_train_pca.shape[1])\n",
    "# Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "source": [
    "#Train model using pca features\n",
    "accs_train, accs_val, aucs, f1_scores, best_fpr, best_tpr, best_auc, best_f1, epoch_best = train_model(model, criterion, optimizer, 'LR', X_train_pca, y_train, X_val_pca, y_val, num_epochs=5000)\n",
    "print(f'Best AUC: {best_auc}, Best F1: {best_f1}, Best epoch: {epoch_best}')\n",
    "#plot accuracies of val and training\n",
    "plt.plot(accs_train, label='Train')\n",
    "plt.plot(accs_val, label='Val')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR with data standardization and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for images, labels in dataloaders[TRAIN]:\n",
    "    \n",
    "    images = images.view(-1, n_features)\n",
    "    X_train.append(images)\n",
    "    y_train.append(labels)\n",
    "X_train = torch.cat(X_train, dim=0)\n",
    "y_train = torch.cat(y_train, dim=0)\n",
    "X_val = []\n",
    "y_val = []\n",
    "for images, labels in dataloaders[VAL]:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_val.append(images)\n",
    "    y_val.append(labels)\n",
    "X_val = torch.cat(X_val, dim=0)\n",
    "y_val = torch.cat(y_val, dim=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "source": [
    "# retain only n_componenets feature with highest variance using PCA\n",
    "n_components = 612\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train.detach().numpy())\n",
    "X_val_pca = pca.transform(X_val.detach().numpy())\n",
    "#transform in tensors\n",
    "print(X_train_pca.shape)\n",
    "X_train_pca = torch.tensor(X_train_pca)\n",
    "X_val_pca = torch.tensor(X_val_pca)\n",
    "\n",
    "#compute mean of X_train_pca\n",
    "mean = X_train_pca.mean().item()\n",
    "std = X_train_pca.std().item()\n",
    "\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "#standardize X_train_pca and X_val_pca using mean and std\n",
    "X_train_pca = (X_train_pca - mean) / std\n",
    "X_val_pca = (X_val_pca - mean) / std\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "source": [
    "\n",
    "model = LogisticRegression(X_train_pca.shape[1])\n",
    "# Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "source": [
    "\n",
    "#Train model using pca features\n",
    "accs_train, accs_val, aucs, f1_scores, best_fpr, best_tpr, best_auc, best_f1, epoch_best = train_model(model, criterion, optimizer, 'LR', X_train_pca, y_train, X_val_pca, y_val, num_epochs=5000)\n",
    "print(f'Best AUC: {best_auc}, Best F1: {best_f1}, Best epoch: {epoch_best}')\n",
    "#plot accuracies of val and training\n",
    "plt.plot(accs_train, label='Train')\n",
    "plt.plot(accs_val, label='Val')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "source": [
    "# show memory occupation of the model\n",
    "# save model on memory\n",
    "torch.save(model, '../../../data/LRmodel.pth')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "with profile(activities=[ProfilerActivity.CPU], profile_memory=True, record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(X_val_pca)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR with PCA, normalization and histogram equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "source": [
    "data_dir = '../../../data/train/train'\n",
    "csv_file = '../../../data/train.csv'\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "#set the transforms for the images\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "image_dataset = CactusDataset.CactusDataset(csv_file, data_dir, data_transforms)\n",
    "# split into train, eval, test\n",
    "train_size = int(0.7 * len(image_dataset))\n",
    "eval_size = int(0.2 * len(image_dataset))\n",
    "test_size = len(image_dataset) - train_size - eval_size\n",
    "train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(image_dataset, [train_size, eval_size, test_size])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "dataloader = DataLoader(train_dataset, batch_size=train_size, shuffle=True, num_workers=0)\n",
    "for images, labels in dataloader:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_train.append(images)\n",
    "    y_train.append(labels)\n",
    "X_train = torch.cat(X_train, dim=0)\n",
    "y_train = torch.cat(y_train, dim=0)\n",
    "X_val = []\n",
    "y_val = []\n",
    "dataloader = DataLoader(eval_dataset, batch_size=eval_size, shuffle=False, num_workers=0)\n",
    "for images, labels in dataloader:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_val.append(images)\n",
    "    y_val.append(labels)\n",
    "X_val = torch.cat(X_val, dim=0)\n",
    "y_val = torch.cat(y_val, dim=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "source": [
    "pca = PCA(n_components=X_train.shape[1])\n",
    "pca.fit(X_train)\n",
    "#plot number of features vs explained variance\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) > 0.99)\n",
    "plt.scatter(n_components, 0.99, color='r')\n",
    "plt.text(n_components, 0.99, '({}, {})'.format(n_components, 0.99), color='black')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#print n_components for 95% variance\n",
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) > 0.99)\n",
    "print('Number of components for 95% variance: ', n_components)\n",
    "\n",
    "# retain only n_componenets feature with highest variance using PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train.detach().numpy())\n",
    "X_val_pca = pca.transform(X_val.detach().numpy())\n",
    "#transform in tensors\n",
    "print(X_train_pca.shape)\n",
    "X_train_pca = torch.tensor(X_train_pca)\n",
    "X_val_pca = torch.tensor(X_val_pca)\n",
    "\n",
    "#compute mean of X_train_pca\n",
    "mean = X_train_pca.mean().item()\n",
    "std = X_train_pca.std().item()\n",
    "\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "#standardize X_train_pca and X_val_pca using mean and std\n",
    "X_train_pca = (X_train_pca - mean) / std\n",
    "X_val_pca = (X_val_pca - mean) / std\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "source": [
    "model = LogisticRegression(X_train_pca.shape[1])\n",
    "# Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "source": [
    "#Train model using pca features\n",
    "accs_train, accs_val, aucs, f1_scores, best_fpr, best_tpr, best_auc, best_f1, epoch_best = train_model(model, criterion, optimizer, 'LR', X_train_pca, y_train, X_val_pca, y_val, num_epochs=5000)\n",
    "print(f'Best AUC: {best_auc}, Best F1: {best_f1}, Best epoch: {epoch_best}')\n",
    "#plot accuracies of val and training\n",
    "plt.plot(accs_train, label='Train')\n",
    "plt.plot(accs_val, label='Val')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR with oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "source": [
    "data_dir = '../../../data/train/train'\n",
    "csv_file = '../../../data/train.csv'\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "#set the transforms for the images\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "image_dataset = CactusDataset.CactusDataset(csv_file, data_dir, data_transforms)\n",
    "print(image_dataset.get_class_distribution())\n",
    "\n",
    "\n",
    "image_dataset.oversample()\n",
    "print(len(image_dataset))\n",
    "\n",
    "# print number of classes\n",
    "print(image_dataset.get_class_distribution())\n",
    "\n",
    "# split into train, eval, test\n",
    "train_size = int(0.7 * len(image_dataset))\n",
    "eval_size = int(0.2 * len(image_dataset))\n",
    "test_size = len(image_dataset) - train_size - eval_size\n",
    "train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(image_dataset, [train_size, eval_size, test_size])\n",
    "\n",
    "n_features = 32 * 32 * 3"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "dataloader = DataLoader(train_dataset, batch_size=train_size, shuffle=True, num_workers=0)\n",
    "for images, labels in dataloader:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_train.append(images)\n",
    "    y_train.append(labels)\n",
    "X_train = torch.cat(X_train, dim=0)\n",
    "y_train = torch.cat(y_train, dim=0)\n",
    "X_val = []\n",
    "y_val = []\n",
    "dataloader = DataLoader(eval_dataset, batch_size=eval_size, shuffle=False, num_workers=0)\n",
    "for images, labels in dataloader:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_val.append(images)\n",
    "    y_val.append(labels)\n",
    "X_val = torch.cat(X_val, dim=0)\n",
    "y_val = torch.cat(y_val, dim=0)\n",
    "n_features = X_train.shape[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "source": [
    "# retain only n_componenets feature with highest variance using PCA\n",
    "n_components = 612\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train.detach().numpy())\n",
    "X_val_pca = pca.transform(X_val.detach().numpy())\n",
    "#transform in tensors\n",
    "X_train_pca = torch.tensor(X_train_pca)\n",
    "X_val_pca = torch.tensor(X_val_pca)\n",
    "\n",
    "#Standardize\n",
    "mean = X_train_pca.mean().item()\n",
    "std = X_train_pca.std().item()\n",
    "X_train_pca = (X_train_pca - mean) / std\n",
    "X_val_pca = (X_val_pca - mean) / std"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "source": [
    "#instantiate model\n",
    "model = LogisticRegression(n_components)\n",
    "# Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "source": [
    "# train model for 5000 epochs\n",
    "accs_train, accs_val, aucs, f1_scores, best_fpr, best_tpr, best_auc, best_f1, epoch_best = train_model(model, criterion, optimizer, 'LR', X_train_pca, y_train, X_val_pca, y_val, num_epochs=5000)\n",
    "print(f'Best AUC: {best_auc}, Best F1: {best_f1}, Best epoch: {epoch_best}')\n",
    "#plot accuracies of val and training\n",
    "plt.plot(accs_train, label='Train')\n",
    "plt.plot(accs_val, label='Val')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR with PCA and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "data_dir = '../../../data/train/train'\n",
    "csv_file = '../../../data/train.csv'\n",
    "TRAIN = 'train'\n",
    "TRAIN2 = 'train2'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "#set the transforms for the images\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "image_dataset = CactusDataset.CactusDataset(csv_file, data_dir, data_transforms)\n",
    "# split into train, eval, test\n",
    "train_size = int(0.7 * len(image_dataset))\n",
    "eval_size = int(0.2 * len(image_dataset))\n",
    "test_size = len(image_dataset) - train_size - eval_size\n",
    "train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(image_dataset, [train_size, eval_size, test_size])\n",
    "\n",
    "image_datasets = {TRAIN: train_dataset, VAL: eval_dataset, TEST: test_dataset}\n",
    "\n",
    "data_dir = '../../../data/test/test'\n",
    "csv_file = '../../../data/test.csv'\n",
    "new_dataset = CactusDataset.CactusDataset(csv_file, data_dir, data_transforms)\n",
    "\n",
    "#print new_dataset\n",
    "print(len(new_dataset))\n",
    "print(new_dataset.get_class_distribution())\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n",
    "\n",
    "batch_sizes = {TRAIN: dataset_sizes[TRAIN], VAL: dataset_sizes[VAL], TEST: 64}\n",
    "\n",
    "dataloaders = {TRAIN: None, TRAIN2: None, VAL: None, TEST: None}\n",
    "dataloaders[TRAIN] = DataLoader(image_datasets[TRAIN], batch_size=batch_sizes[TRAIN],\n",
    "                                             shuffle=True, num_workers=0)\n",
    "dataloaders[TRAIN2] = DataLoader(new_dataset, batch_size=len(new_dataset), shuffle=False, num_workers=0)\n",
    "dataloaders[VAL] = DataLoader(image_datasets[VAL], batch_size=batch_sizes[VAL],\n",
    "                                             shuffle=False, num_workers=0)\n",
    "dataloaders[TEST] = DataLoader(image_datasets[TEST], batch_size=batch_sizes[TEST], shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "print(dataset_sizes)\n",
    "class_names = {0: 'No Cactus', 1: 'Cactus'}\n",
    "\n",
    "\n",
    "n_features = 32 * 32 * 3\n",
    "\n",
    "print(n_features)\n",
    "\n",
    "#print nunmber of each class in each dataset\n",
    "for x in [TRAIN, VAL, TEST]:\n",
    "    print(\"Number of {} images: {}\".format(x, dataset_sizes[x]))\n",
    "    for i in range(2):\n",
    "        print(\"Number of {} images of class {}: {}\".format(x, class_names[i], sum([1 for j in image_datasets[x] if j[1] == i])))\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for images, labels in dataloaders[TRAIN]:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_train.append(images)\n",
    "    y_train.append(labels)\n",
    "for images, labels in dataloaders[TRAIN2]:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_train.append(images)\n",
    "    y_train.append(labels)\n",
    "X_train = torch.cat(X_train, dim=0)\n",
    "y_train = torch.cat(y_train, dim=0)\n",
    "X_val = []\n",
    "y_val = []\n",
    "for images, labels in dataloaders[VAL]:\n",
    "    images = images.view(-1, n_features)\n",
    "    X_val.append(images)\n",
    "    y_val.append(labels)\n",
    "X_val = torch.cat(X_val, dim=0)\n",
    "y_val = torch.cat(y_val, dim=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "source": [
    "# retain only n_componenets feature with highest variance using PCA\n",
    "n_components = 612\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train.detach().numpy())\n",
    "X_val_pca = pca.transform(X_val.detach().numpy())\n",
    "#transform in tensors\n",
    "print(X_train_pca.shape)\n",
    "X_train_pca = torch.tensor(X_train_pca)\n",
    "X_val_pca = torch.tensor(X_val_pca)\n",
    "\n",
    "#compute mean of X_train_pca\n",
    "mean = X_train_pca.mean().item()\n",
    "std = X_train_pca.std().item()\n",
    "\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "#standardize X_train_pca and X_val_pca using mean and std\n",
    "X_train_pca = (X_train_pca - mean) / std\n",
    "X_val_pca = (X_val_pca - mean) / std\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "source": [
    "\n",
    "model = LogisticRegression(X_train_pca.shape[1])\n",
    "# Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "source": [
    "\n",
    "#Train model using pca features\n",
    "accs_train, accs_val, aucs, f1_scores, best_fpr, best_tpr, best_auc, best_f1, epoch_best = train_model(model, criterion, optimizer, 'LR', X_train_pca, y_train, X_val_pca, y_val, num_epochs=5000)\n",
    "print(f'Best AUC: {best_auc}, Best F1: {best_f1}, Best epoch: {epoch_best}')\n",
    "#plot accuracies of val and training\n",
    "plt.plot(accs_train, label='Train')\n",
    "plt.plot(accs_val, label='Val')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
