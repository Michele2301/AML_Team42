{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:03.438210Z",
     "start_time": "2024-05-19T11:07:00.935414Z"
    }
   },
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "\n",
    "from customDatasets.audioDataset import AudioDataset\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:03.464576Z",
     "start_time": "2024-05-19T11:07:03.440530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# free gpu\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:03.488639Z",
     "start_time": "2024-05-19T11:07:03.465874Z"
    }
   },
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, encoding_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:03.505939Z",
     "start_time": "2024-05-19T11:07:03.490131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test for deciding the mels parameters\n",
    "from utils.audioUtils import AudioUtil\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "audio_file = \"./data/train/normal_id_00_00000000.wav\"\n",
    "\n",
    "aud = AudioUtil.open(audio_file)\n",
    "sig, sr = aud\n",
    "mel = MelSpectrogram(sr, n_fft=1000, hop_length=501, n_mels=128)\n",
    "spec = mel(sig)\n",
    "ampl = AmplitudeToDB(top_db=80)\n",
    "spec = ampl(spec)\n",
    "\n",
    "\n",
    "\n",
    "print(spec.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 320])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:03.583768Z",
     "start_time": "2024-05-19T11:07:03.508536Z"
    }
   },
   "source": [
    "def train_model(model, train_dl, val_dl, test_dl, criterion, optimizer, device, epochs=5,step_size=5):\n",
    "    lr_scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for inputs, labels in train_dl:\n",
    "            model.train()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs.view(inputs.size(0), -1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        lr_scheduler.step()\n",
    "        print(f'Epoch[{epoch + 1}/{epochs}], Train loss: {np.average(train_losses): .4f}')\n",
    "        \n",
    "        \n",
    "        for inputs, labels in val_dl:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs.view(inputs.size(0), -1))\n",
    "                val_losses.append(loss.item())\n",
    "        print(f'Epoch[{epoch + 1}/{epochs}], Val loss: {np.average(val_losses): .4f}')\n",
    " \n",
    "        scores = []\n",
    "        full_labels = []\n",
    "        for inputs, labels in test_dl:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                mse = torch.sum((outputs - inputs.view(inputs.size(0), -1)) ** 2, dim=1) / outputs.shape[1]\n",
    "                scores.append(mse)\n",
    "                full_labels.append(labels)\n",
    "        \n",
    "        full_labels = torch.cat([label for label in full_labels])\n",
    "        scores = torch.cat([score for score in scores])\n",
    "        fpr, tpr, _ = roc_curve(full_labels.cpu().detach(), scores.cpu().detach(), pos_label=0)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print(roc_auc)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:05.424580Z",
     "start_time": "2024-05-19T11:07:03.585013Z"
    }
   },
   "source": [
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 20,\n",
    "    \"num_classes\": 2,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"test_batch_size\": 128,\n",
    "    \"criterion\": nn.MSELoss(),\n",
    "    \"device\":\n",
    "        torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "}\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "data_path = \"./data/train/\"\n",
    "data_path_test = \"./data/test/\"\n",
    "\n",
    "\n",
    "meta_train_df = pd.read_csv(\"./data/train.csv\")\n",
    "meta_test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "train_df = meta_train_df[['filename', 'is_normal', 'machine_id']]\n",
    "train_dataset = AudioDataset(train_df, data_path,in_memory=True, sgram_type=\"mel\")\n",
    "test_df = meta_test_df[['filename', 'is_normal', 'machine_id']]\n",
    "test_dataset = AudioDataset(test_df, data_path_test, in_memory=True, sgram_type=\"mel\")\n",
    "\n",
    "num_items = len(train_dataset)\n",
    "num_train = int(0.8 * num_items)\n",
    "num_val = num_items-num_train\n",
    "\n",
    "train_ds, val_ds = random_split(train_dataset, [num_train, num_val])\n",
    "test_ds = test_dataset\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=CONFIG['train_batch_size'], shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=CONFIG['val_batch_size'], shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=CONFIG[\"test_batch_size\"], shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:06.851270Z",
     "start_time": "2024-05-19T11:07:05.425642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = next(iter(train_dl))[0].shape[1] * next(iter(train_dl))[0].shape[2] * next(iter(train_dl))[0].shape[3]\n",
    "model = Autoencoder(input_size, encoding_dim=128)\n",
    "model = model.to(CONFIG[\"device\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:10.570904Z",
     "start_time": "2024-05-19T11:07:06.852622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute metrics\n",
    "inputs_cat=[]\n",
    "for inputs, labels in train_dl:\n",
    "    inputs_cat.append(inputs)\n",
    "inputs_cat = torch.cat([input for input in inputs_cat])\n",
    "print(inputs_cat.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1896, 1, 320, 128])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:10.769374Z",
     "start_time": "2024-05-19T11:07:10.571993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute the min and max value for each frequency of the batch_sizexchannelxtimexfrequecy\n",
    "min = torch.min(inputs_cat, dim=0).values\n",
    "max = torch.max(inputs_cat, dim=0).values\n",
    "print(max.shape)\n",
    "print(min.shape)\n",
    "train_dataset.min = min\n",
    "train_dataset.max = max\n",
    "test_dataset.min = min\n",
    "test_dataset.max = max"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 320, 128])\n",
      "torch.Size([1, 320, 128])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:07:10.773868Z",
     "start_time": "2024-05-19T11:07:10.770919Z"
    }
   },
   "source": [
    "training=False\n",
    "if training:\n",
    "    train_model(model, train_dl, val_dl, test_dl, CONFIG[\"criterion\"], optimizer, CONFIG[\"device\"], CONFIG[\"epochs\"])"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train different AE for each machine_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:14:02.404925Z",
     "start_time": "2024-05-19T11:13:56.618819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "learning_rate={0:0.001, 2:0.01, 4:0.1}\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 20,\n",
    "    \"num_classes\": 2,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"test_batch_size\": 128,\n",
    "    \"criterion\": nn.MSELoss(),\n",
    "    \"device\":\n",
    "        torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "}\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "data_path = \"./data/train/\"\n",
    "data_path_test = \"./data/test/\"\n",
    "\n",
    "\n",
    "meta_train_df = pd.read_csv(\"./data/train.csv\")\n",
    "meta_test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "train_df = meta_train_df[['filename', 'is_normal', 'machine_id']]\n",
    "\n",
    "train_df_0= train_df[train_df['machine_id'] == 0].reset_index(drop=True)\n",
    "train_dataset_0 = AudioDataset(train_df_0, data_path,in_memory=True, sgram_type=\"mel\")\n",
    "\n",
    "train_df_2= train_df[train_df['machine_id'] == 2].reset_index(drop=True)\n",
    "train_dataset_2 = AudioDataset(train_df_2, data_path,in_memory=True, sgram_type=\"mel\")\n",
    "\n",
    "train_df_4= train_df[train_df['machine_id'] == 4].reset_index(drop=True)\n",
    "train_dataset_4 = AudioDataset(train_df_4, data_path,in_memory=True, sgram_type=\"mel\")\n",
    "\n",
    "test_df = meta_test_df[['filename', 'is_normal', 'machine_id']]\n",
    "\n",
    "test_df_0= test_df[test_df['machine_id'] == 0].reset_index(drop=True)\n",
    "test_dataset_0 = AudioDataset(test_df_0, data_path_test, in_memory=True, sgram_type=\"mel\")\n",
    "\n",
    "test_df_2= test_df[test_df['machine_id'] == 2].reset_index(drop=True)\n",
    "test_dataset_2 = AudioDataset(test_df_2, data_path_test, in_memory=True, sgram_type=\"mel\")\n",
    "\n",
    "test_df_4= test_df[test_df['machine_id'] == 4].reset_index(drop=True)\n",
    "test_dataset_4 = AudioDataset(test_df_4, data_path_test, in_memory=True, sgram_type=\"mel\")\n",
    "\n",
    "num_items = len(train_dataset_0)\n",
    "num_train = int(0.8 * num_items)\n",
    "num_val = num_items-num_train\n",
    "train_ds_0, val_ds_0 = random_split(train_dataset_0, [num_train, num_val])\n",
    "\n",
    "num_items = len(train_dataset_2)\n",
    "num_train = int(0.8 * num_items)\n",
    "num_val = num_items-num_train\n",
    "train_ds_2, val_ds_2 = random_split(train_dataset_2, [num_train, num_val])\n",
    "\n",
    "num_items = len(train_dataset_4)\n",
    "num_train = int(0.8 * num_items)\n",
    "num_val = num_items-num_train\n",
    "train_ds_4, val_ds_4 = random_split(train_dataset_4, [num_train, num_val])\n",
    "\n",
    "test_ds_0 = test_dataset_0\n",
    "test_ds_2 = test_dataset_2\n",
    "test_ds_4 = test_dataset_4\n",
    "\n",
    "train_dl_0 = DataLoader(train_ds_0, batch_size=CONFIG['train_batch_size'], shuffle=True)\n",
    "val_dl_0 = DataLoader(val_ds_0, batch_size=CONFIG['val_batch_size'], shuffle=False)\n",
    "test_dl_0 = DataLoader(test_ds_0, batch_size=CONFIG[\"test_batch_size\"], shuffle=False)\n",
    "\n",
    "train_dl_2 = DataLoader(train_ds_2, batch_size=CONFIG['train_batch_size'], shuffle=True)\n",
    "val_dl_2 = DataLoader(val_ds_2, batch_size=CONFIG['val_batch_size'], shuffle=False)\n",
    "test_dl_2 = DataLoader(test_ds_2, batch_size=CONFIG[\"test_batch_size\"], shuffle=False)\n",
    "\n",
    "train_dl_4 = DataLoader(train_ds_4, batch_size=CONFIG['train_batch_size'], shuffle=True)\n",
    "val_dl_4 = DataLoader(val_ds_4, batch_size=CONFIG['val_batch_size'], shuffle=False)\n",
    "test_dl_4 = DataLoader(test_ds_4, batch_size=CONFIG[\"test_batch_size\"], shuffle=False)\n",
    "\n",
    "# create dict\n",
    "train_dl_dict = {0: train_dl_0, 2: train_dl_2, 4: train_dl_4}\n",
    "val_dl_dict = {0: val_dl_0, 2: val_dl_2, 4: val_dl_4}\n",
    "test_dl_dict = {0: test_dl_0, 2: test_dl_2, 4: test_dl_4}\n",
    "train_dataset_dict={0: train_dataset_0, 2: train_dataset_2, 4: train_dataset_4}\n",
    "test_dataset_dict={0: test_dataset_0, 2: test_dataset_2, 4: test_dataset_4}"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:14:02.408604Z",
     "start_time": "2024-05-19T11:14:02.406328Z"
    }
   },
   "cell_type": "code",
   "source": "# train_ds_0, val_ds_0, test_ds_0",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:14:06.322022Z",
     "start_time": "2024-05-19T11:14:02.410011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute metrics\n",
    "inputs_cat={}\n",
    "for key in train_dl_dict.keys():\n",
    "    inputs_cat[key]=[]\n",
    "    for inputs, labels in train_dl_dict[key]:\n",
    "        inputs_cat[key].append(inputs)\n",
    "    inputs_cat[key] = torch.cat([input for input in inputs_cat[key]])\n",
    "    print(inputs_cat[key].shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([774, 1, 320, 128])\n",
      "torch.Size([774, 1, 320, 128])\n",
      "torch.Size([347, 1, 320, 128])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:14:06.385977Z",
     "start_time": "2024-05-19T11:14:06.323928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute the min and max value for each frequency of the batch_sizexchannelxtimexfrequecy\n",
    "for key in train_dl_dict.keys():\n",
    "    min = torch.min(inputs_cat[key], dim=0).values\n",
    "    max = torch.max(inputs_cat[key], dim=0).values\n",
    "    print(max.shape)\n",
    "    print(min.shape)\n",
    "    train_dataset_dict[key].min = min\n",
    "    train_dataset_dict[key].max = max\n",
    "    test_dataset_dict[key].min = min\n",
    "    test_dataset_dict[key].max = max"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 320, 128])\n",
      "torch.Size([1, 320, 128])\n",
      "torch.Size([1, 320, 128])\n",
      "torch.Size([1, 320, 128])\n",
      "torch.Size([1, 320, 128])\n",
      "torch.Size([1, 320, 128])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T11:16:49.210457Z",
     "start_time": "2024-05-19T11:14:06.387231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training=True\n",
    "models_dict={}\n",
    "if training:\n",
    "    for key in train_dl_dict.keys():\n",
    "        input_size = next(iter(train_dl_dict[key]))[0].shape[1] * next(iter(train_dl_dict[key]))[0].shape[2] * next(iter(train_dl_dict[key]))[0].shape[3]\n",
    "        model = Autoencoder(input_size, encoding_dim=128)\n",
    "        model = model.to(CONFIG[\"device\"])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"][key])\n",
    "        train_model(model, train_dl_dict[key], val_dl_dict[key], test_dl_dict[key], CONFIG[\"criterion\"], optimizer, CONFIG[\"device\"], CONFIG[\"epochs\"],10)\n",
    "        models_dict[key]=model"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/20], Train loss:  0.0321\n",
      "Epoch[1/20], Val loss:  0.0224\n",
      "0.8522191011235956\n",
      "Epoch[2/20], Train loss:  0.0229\n",
      "Epoch[2/20], Val loss:  0.0200\n",
      "0.8562921348314606\n",
      "Epoch[3/20], Train loss:  0.0197\n",
      "Epoch[3/20], Val loss:  0.0182\n",
      "0.888876404494382\n",
      "Epoch[4/20], Train loss:  0.0186\n",
      "Epoch[4/20], Val loss:  0.0175\n",
      "0.8892134831460675\n",
      "Epoch[5/20], Train loss:  0.0171\n",
      "Epoch[5/20], Val loss:  0.0168\n",
      "0.9003932584269663\n",
      "Epoch[6/20], Train loss:  0.0167\n",
      "Epoch[6/20], Val loss:  0.0168\n",
      "0.8949157303370787\n",
      "Epoch[7/20], Train loss:  0.0164\n",
      "Epoch[7/20], Val loss:  0.0158\n",
      "0.9194662921348316\n",
      "Epoch[8/20], Train loss:  0.0155\n",
      "Epoch[8/20], Val loss:  0.0153\n",
      "0.9253089887640451\n",
      "Epoch[9/20], Train loss:  0.0158\n",
      "Epoch[9/20], Val loss:  0.0157\n",
      "0.9151404494382023\n",
      "Epoch[10/20], Train loss:  0.0154\n",
      "Epoch[10/20], Val loss:  0.0158\n",
      "0.949185393258427\n",
      "Epoch[11/20], Train loss:  0.0150\n",
      "Epoch[11/20], Val loss:  0.0150\n",
      "0.9041011235955057\n",
      "Epoch[12/20], Train loss:  0.0150\n",
      "Epoch[12/20], Val loss:  0.0147\n",
      "0.9112921348314607\n",
      "Epoch[13/20], Train loss:  0.0147\n",
      "Epoch[13/20], Val loss:  0.0148\n",
      "0.9087078651685393\n",
      "Epoch[14/20], Train loss:  0.0148\n",
      "Epoch[14/20], Val loss:  0.0149\n",
      "0.9225\n",
      "Epoch[15/20], Train loss:  0.0147\n",
      "Epoch[15/20], Val loss:  0.0148\n",
      "0.9034831460674158\n",
      "Epoch[16/20], Train loss:  0.0147\n",
      "Epoch[16/20], Val loss:  0.0149\n",
      "0.9107303370786517\n",
      "Epoch[17/20], Train loss:  0.0145\n",
      "Epoch[17/20], Val loss:  0.0148\n",
      "0.8928370786516854\n",
      "Epoch[18/20], Train loss:  0.0147\n",
      "Epoch[18/20], Val loss:  0.0148\n",
      "0.9251123595505618\n",
      "Epoch[19/20], Train loss:  0.0145\n",
      "Epoch[19/20], Val loss:  0.0147\n",
      "0.8962359550561798\n",
      "Epoch[20/20], Train loss:  0.0148\n",
      "Epoch[20/20], Val loss:  0.0148\n",
      "0.8865449438202248\n",
      "Epoch[1/20], Train loss:  0.2296\n",
      "Epoch[1/20], Val loss:  0.2388\n",
      "0.7255430711610487\n",
      "Epoch[2/20], Train loss:  0.2403\n",
      "Epoch[2/20], Val loss:  0.2389\n",
      "0.7249438202247191\n",
      "Epoch[3/20], Train loss:  0.2396\n",
      "Epoch[3/20], Val loss:  0.2377\n",
      "0.7125468164794007\n",
      "Epoch[4/20], Train loss:  0.2392\n",
      "Epoch[4/20], Val loss:  0.2374\n",
      "0.7217602996254682\n",
      "Epoch[5/20], Train loss:  0.2381\n",
      "Epoch[5/20], Val loss:  0.2374\n",
      "0.7251685393258427\n",
      "Epoch[6/20], Train loss:  0.2382\n",
      "Epoch[6/20], Val loss:  0.2359\n",
      "0.6933333333333334\n",
      "Epoch[7/20], Train loss:  0.2372\n",
      "Epoch[7/20], Val loss:  0.2356\n",
      "0.6838202247191011\n",
      "Epoch[8/20], Train loss:  0.2377\n",
      "Epoch[8/20], Val loss:  0.2369\n",
      "0.7125468164794007\n",
      "Epoch[9/20], Train loss:  0.2382\n",
      "Epoch[9/20], Val loss:  0.2369\n",
      "0.7345318352059925\n",
      "Epoch[10/20], Train loss:  0.2381\n",
      "Epoch[10/20], Val loss:  0.2362\n",
      "0.7144569288389514\n",
      "Epoch[11/20], Train loss:  0.2377\n",
      "Epoch[11/20], Val loss:  0.2365\n",
      "0.7132958801498127\n",
      "Epoch[12/20], Train loss:  0.2378\n",
      "Epoch[12/20], Val loss:  0.2365\n",
      "0.7210861423220973\n",
      "Epoch[13/20], Train loss:  0.2379\n",
      "Epoch[13/20], Val loss:  0.2363\n",
      "0.7113483146067416\n",
      "Epoch[14/20], Train loss:  0.2380\n",
      "Epoch[14/20], Val loss:  0.2371\n",
      "0.7171910112359551\n",
      "Epoch[15/20], Train loss:  0.2379\n",
      "Epoch[15/20], Val loss:  0.2367\n",
      "0.7155805243445693\n",
      "Epoch[16/20], Train loss:  0.2377\n",
      "Epoch[16/20], Val loss:  0.2365\n",
      "0.7292134831460674\n",
      "Epoch[17/20], Train loss:  0.2380\n",
      "Epoch[17/20], Val loss:  0.2364\n",
      "0.716067415730337\n",
      "Epoch[18/20], Train loss:  0.2379\n",
      "Epoch[18/20], Val loss:  0.2360\n",
      "0.7138951310861422\n",
      "Epoch[19/20], Train loss:  0.2378\n",
      "Epoch[19/20], Val loss:  0.2370\n",
      "0.7278651685393259\n",
      "Epoch[20/20], Train loss:  0.2380\n",
      "Epoch[20/20], Val loss:  0.2363\n",
      "0.7120224719101124\n",
      "Epoch[1/20], Train loss:  0.2169\n",
      "Epoch[1/20], Val loss:  0.2363\n",
      "0.6150561797752809\n",
      "Epoch[2/20], Train loss:  0.2375\n",
      "Epoch[2/20], Val loss:  0.2366\n",
      "0.6209550561797753\n",
      "Epoch[3/20], Train loss:  0.2365\n",
      "Epoch[3/20], Val loss:  0.2359\n",
      "0.5980898876404495\n",
      "Epoch[4/20], Train loss:  0.2361\n",
      "Epoch[4/20], Val loss:  0.2381\n",
      "0.6017977528089886\n",
      "Epoch[5/20], Train loss:  0.2373\n",
      "Epoch[5/20], Val loss:  0.2360\n",
      "0.6265168539325844\n",
      "Epoch[6/20], Train loss:  0.2365\n",
      "Epoch[6/20], Val loss:  0.2374\n",
      "0.6212921348314607\n",
      "Epoch[7/20], Train loss:  0.2374\n",
      "Epoch[7/20], Val loss:  0.2365\n",
      "0.5835393258426966\n",
      "Epoch[8/20], Train loss:  0.2373\n",
      "Epoch[8/20], Val loss:  0.2358\n",
      "0.5841011235955056\n",
      "Epoch[9/20], Train loss:  0.2371\n",
      "Epoch[9/20], Val loss:  0.2382\n",
      "0.5656741573033708\n",
      "Epoch[10/20], Train loss:  0.2362\n",
      "Epoch[10/20], Val loss:  0.2363\n",
      "0.6110674157303372\n",
      "Epoch[11/20], Train loss:  0.2363\n",
      "Epoch[11/20], Val loss:  0.2364\n",
      "0.6056179775280899\n",
      "Epoch[12/20], Train loss:  0.2370\n",
      "Epoch[12/20], Val loss:  0.2380\n",
      "0.5998314606741573\n",
      "Epoch[13/20], Train loss:  0.2370\n",
      "Epoch[13/20], Val loss:  0.2365\n",
      "0.6059550561797753\n",
      "Epoch[14/20], Train loss:  0.2370\n",
      "Epoch[14/20], Val loss:  0.2382\n",
      "0.5718539325842696\n",
      "Epoch[15/20], Train loss:  0.2366\n",
      "Epoch[15/20], Val loss:  0.2371\n",
      "0.5802808988764045\n",
      "Epoch[16/20], Train loss:  0.2376\n",
      "Epoch[16/20], Val loss:  0.2374\n",
      "0.614943820224719\n",
      "Epoch[17/20], Train loss:  0.2368\n",
      "Epoch[17/20], Val loss:  0.2376\n",
      "0.605505617977528\n",
      "Epoch[18/20], Train loss:  0.2369\n",
      "Epoch[18/20], Val loss:  0.2360\n",
      "0.6046067415730336\n",
      "Epoch[19/20], Train loss:  0.2377\n",
      "Epoch[19/20], Val loss:  0.2367\n",
      "0.6082584269662922\n",
      "Epoch[20/20], Train loss:  0.2368\n",
      "Epoch[20/20], Val loss:  0.2375\n",
      "0.6291011235955056\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
