{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:08:07.080170Z",
     "start_time": "2024-05-25T11:08:06.763753Z"
    }
   },
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "\n",
    "from customDatasets.audioDataset import AudioDataset\n"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:08:07.086988Z",
     "start_time": "2024-05-25T11:08:07.081775Z"
    }
   },
   "source": [
    "# free gpu\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:08:07.094477Z",
     "start_time": "2024-05-25T11:08:07.088071Z"
    }
   },
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, encoding_dim),\n",
    "            nn.BatchNorm1d(encoding_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, input_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:08:07.105460Z",
     "start_time": "2024-05-25T11:08:07.096502Z"
    }
   },
   "source": [
    "# test for deciding the mels parameters\n",
    "from utils.audioUtils import AudioUtil\n",
    "from torchaudio import transforms\n",
    "import torch\n",
    "audio_file = \"./data/train/normal_id_00_00000000.wav\"\n",
    "\n",
    "aud = AudioUtil.open(audio_file)\n",
    "sig, sr = aud\n",
    "mel = transforms.MelSpectrogram(sr, n_fft=1000, hop_length=501, n_mels=128)\n",
    "spec = mel(sig)\n",
    "ampl = transforms.AmplitudeToDB(top_db=80)\n",
    "spec = ampl(spec)\n",
    "\n",
    "\n",
    "print(spec.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 320])\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:08:07.118743Z",
     "start_time": "2024-05-25T11:08:07.106870Z"
    }
   },
   "source": [
    "def train_model(model, train_dl, val_dl, test_dl, criterion, optimizer, device, wandb=None, epochs=5,step_size=5):\n",
    "    lr_scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.5)\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for inputs, labels in train_dl:\n",
    "            model.train()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs.view(inputs.size(0), -1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        lr_scheduler.step()\n",
    "        print(f'Epoch[{epoch + 1}/{epochs}], Train loss: {np.average(train_losses): .4f}')\n",
    "        \n",
    "        for inputs, labels in val_dl:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                inputs = inputs.view(inputs.shape[0]*inputs.shape[1], -1)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                val_losses.append(loss.item())\n",
    "        print(f'Epoch[{epoch + 1}/{epochs}], Val loss: {np.average(val_losses): .4f}')\n",
    "        if np.average(val_losses) < best_val_loss:\n",
    "            best_val_loss = np.average(val_losses)\n",
    " \n",
    "        full_scores = []\n",
    "        full_labels = []\n",
    "        for inputs, labels in test_dl:\n",
    "            inputs, labels = inputs.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                tmp_scores = []\n",
    "                for idx in range (10):\n",
    "                    outputs = model(inputs[:, idx, :, :])\n",
    "                    mse = torch.sum((outputs - inputs[:, idx, :, :].view(inputs.size(0), -1)) ** 2, dim=1, keepdim=True) / outputs.shape[1]\n",
    "                    tmp_scores.append(mse)\n",
    "\n",
    "                scores = torch.cat(tmp_scores, dim=1)\n",
    "                scores = torch.max(scores, dim=1).values\n",
    "\n",
    "                full_scores.append(scores)\n",
    "                full_labels.append(labels)\n",
    "        \n",
    "        full_labels = torch.cat([label for label in full_labels])\n",
    "        full_scores = torch.cat([score for score in full_scores])\n",
    "        fpr, tpr, _ = roc_curve(full_labels.cpu().detach(), full_scores.cpu().detach(), pos_label=0)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print(roc_auc)\n",
    "        if wandb:\n",
    "            wandb.log({\"roc_auc test\": roc_auc, \"val_loss\": np.average(val_losses), \"train_loss\": np.average(train_losses)})\n",
    "    return best_val_loss"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:10:05.642691Z",
     "start_time": "2024-05-25T11:08:07.120236Z"
    }
   },
   "source": [
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 500,\n",
    "    \"num_classes\": 2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"test_batch_size\": 128,\n",
    "    \"criterion\": nn.MSELoss(),\n",
    "    \"device\":\n",
    "        torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "}\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "data_path = \"./data/train/\"\n",
    "data_path_test = \"./data/test/\"\n",
    "\n",
    "\n",
    "meta_train_df = pd.read_csv(\"./data/train.csv\")\n",
    "meta_test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "train_df = meta_train_df[['filename', 'is_normal', 'machine_id']]\n",
    "range_train, range_test = train_test_split(range(len(train_df)), test_size=0.2, train_size=0.8, random_state=None, shuffle=True, stratify=meta_train_df['machine_id'])\n",
    "val_df = train_df.iloc[range_test].reset_index(drop=True)\n",
    "train_df = train_df.iloc[range_train].reset_index(drop=True)\n",
    "train_dataset = AudioDataset(train_df, data_path,in_memory=True, sgram_type=\"mel\", augment=True, split_sgram=True)\n",
    "val_dataset = AudioDataset(val_df, data_path,in_memory=True, sgram_type=\"mel\", augment=False, test_mode=True)\n",
    "test_df = meta_test_df[['filename', 'is_normal', 'machine_id']]\n",
    "test_dataset = AudioDataset(test_df, data_path_test, in_memory=True, sgram_type=\"mel\", augment=False, test_mode=True)\n",
    "\n",
    "train_ds = train_dataset\n",
    "val_ds = val_dataset\n",
    "test_ds = test_dataset\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=CONFIG['train_batch_size'], shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=CONFIG['val_batch_size'], shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=CONFIG[\"test_batch_size\"], shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:10:05.738486Z",
     "start_time": "2024-05-25T11:10:05.644178Z"
    }
   },
   "source": [
    "inputs, labels = next(iter(train_dl))\n",
    "\n",
    "print(inputs.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 128])\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:10:06.023286Z",
     "start_time": "2024-05-25T11:10:05.739634Z"
    }
   },
   "source": [
    "input_size = next(iter(train_dl))[0].shape[1] * next(iter(train_dl))[0].shape[2] * next(iter(train_dl))[0].shape[3]\n",
    "model = Autoencoder(input_size, encoding_dim=128)\n",
    "model = model.to(CONFIG[\"device\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=1e-5)"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:10:10.725969Z",
     "start_time": "2024-05-25T11:10:06.024580Z"
    }
   },
   "source": [
    "# compute metrics\n",
    "inputs_cat=[]\n",
    "train_dataset.test_mode = True\n",
    "for inputs, labels in train_dl:\n",
    "    inputs_cat.append(inputs)\n",
    "inputs_cat = torch.cat([input for input in inputs_cat])\n",
    "inputs_cat = inputs_cat.view(-1,inputs_cat.shape[2],inputs_cat.shape[3])\n",
    "print(inputs_cat.shape)\n",
    "train_dataset.test_mode = False"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([32, 10, 32, 128])\n",
      "torch.Size([8, 10, 32, 128])\n",
      "torch.Size([18960, 32, 128])\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:10:11.047093Z",
     "start_time": "2024-05-25T11:10:10.729116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute the mean and std value for each frequency of the batch_sizexchannelxtimexfrequecy\n",
    "mean = torch.mean(inputs_cat, dim=0)\n",
    "std = torch.std(inputs_cat, dim=0)\n",
    "print(mean.shape)\n",
    "print(std.shape)\n",
    "train_dataset.mean = mean\n",
    "train_dataset.std = std\n",
    "val_dataset.mean = mean\n",
    "val_dataset.std = std\n",
    "test_dataset.mean = mean\n",
    "test_dataset.std = std"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:10:11.052993Z",
     "start_time": "2024-05-25T11:10:11.048380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# init wandb\n",
    "wandb.login()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:10:24.114012Z",
     "start_time": "2024-05-25T11:10:11.054267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Challenge_2_AETimeFrames\",\n",
    ")\n",
    "# save all the parameters from the CONFIG dict\n",
    "wandb.config.update(CONFIG)\n",
    "print(wandb.config)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:3jqfoc53) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc575f3b26394cf8a43847454904dd44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "W&B sync reduced upload amount by 2.2%             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>roc_auc test</td><td>█▇▇▇▇▇▇▇▇▇▇▇▇▅▇▃▃▂▅▇▁▄▆▅▅▁▆▄▄▅▃▄▄▅▄▄▄▆▄▅</td></tr><tr><td>train_loss</td><td>█████████████▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>roc_auc test</td><td>0.74243</td></tr><tr><td>train_loss</td><td>0.00807</td></tr><tr><td>val_loss</td><td>0.00788</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-oath-2</strong> at: <a href='https://wandb.ai/ai-ml-monitor/Challenge_2_AETimeFrames/runs/3jqfoc53/workspace' target=\"_blank\">https://wandb.ai/ai-ml-monitor/Challenge_2_AETimeFrames/runs/3jqfoc53/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240525_122856-3jqfoc53/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:3jqfoc53). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113357544440481, max=1.0…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "808300b3bcc442b9a8b09cb6473c13c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/media/michele/HardDisk/SCUOLA/Universita/Magistrale/Secondo_anno/AML/AML_Team42/Challenge_2/wandb/run-20240525_131011-txzpz8ae</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai-ml-monitor/Challenge_2_AETimeFrames/runs/txzpz8ae/workspace' target=\"_blank\">peachy-sun-3</a></strong> to <a href='https://wandb.ai/ai-ml-monitor/Challenge_2_AETimeFrames' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/ai-ml-monitor/Challenge_2_AETimeFrames' target=\"_blank\">https://wandb.ai/ai-ml-monitor/Challenge_2_AETimeFrames</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/ai-ml-monitor/Challenge_2_AETimeFrames/runs/txzpz8ae/workspace' target=\"_blank\">https://wandb.ai/ai-ml-monitor/Challenge_2_AETimeFrames/runs/txzpz8ae/workspace</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'epochs': 500, 'num_classes': 2, 'learning_rate': 0.001, 'train_batch_size': 32, 'val_batch_size': 16, 'test_batch_size': 128, 'criterion': 'MSELoss()', 'device': 'cuda:0'}\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:37:32.882395Z",
     "start_time": "2024-05-25T11:10:24.116239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training=True\n",
    "input_size = next(iter(train_dl))[0].shape[1] * next(iter(train_dl))[0].shape[2] * next(iter(train_dl))[0].shape[3]\n",
    "measures = []\n",
    "# testing emb space size\n",
    "if training:\n",
    "    for emb_space_size in [32, 64, 128, 256, 512]:\n",
    "        model = Autoencoder(encoding_dim=emb_space_size, input_size=input_size)\n",
    "        model = model.to(CONFIG[\"device\"])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "        measures.append(train_model(model, train_dl, val_dl, test_dl, CONFIG[\"criterion\"], optimizer, CONFIG[\"device\"], epochs=50))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50], Train loss:  0.6746\n",
      "Epoch[1/50], Val loss:  0.6003\n",
      "0.7428963795255931\n",
      "Epoch[2/50], Train loss:  0.5692\n",
      "Epoch[2/50], Val loss:  0.5675\n",
      "0.7365334997919267\n",
      "Epoch[3/50], Train loss:  0.5441\n",
      "Epoch[3/50], Val loss:  0.5391\n",
      "0.774173949230129\n",
      "Epoch[4/50], Train loss:  0.5303\n",
      "Epoch[4/50], Val loss:  0.5257\n",
      "0.7635289221806075\n",
      "Epoch[5/50], Train loss:  0.5164\n",
      "Epoch[5/50], Val loss:  0.5053\n",
      "0.7769912609238451\n",
      "Epoch[6/50], Train loss:  0.5071\n",
      "Epoch[6/50], Val loss:  0.4937\n",
      "0.7730836454431961\n",
      "Epoch[7/50], Train loss:  0.4946\n",
      "Epoch[7/50], Val loss:  0.4853\n",
      "0.7694506866416979\n",
      "Epoch[8/50], Train loss:  0.4953\n",
      "Epoch[8/50], Val loss:  0.4869\n",
      "0.7779026217228464\n",
      "Epoch[9/50], Train loss:  0.4941\n",
      "Epoch[9/50], Val loss:  0.4874\n",
      "0.7673574698293801\n",
      "Epoch[10/50], Train loss:  0.4871\n",
      "Epoch[10/50], Val loss:  0.4897\n",
      "0.7904577611319183\n",
      "Epoch[11/50], Train loss:  0.4819\n",
      "Epoch[11/50], Val loss:  0.4732\n",
      "0.7790262172284645\n",
      "Epoch[12/50], Train loss:  0.4775\n",
      "Epoch[12/50], Val loss:  0.4702\n",
      "0.7735289221806076\n",
      "Epoch[13/50], Train loss:  0.4755\n",
      "Epoch[13/50], Val loss:  0.4686\n",
      "0.7766292134831461\n",
      "Epoch[14/50], Train loss:  0.4713\n",
      "Epoch[14/50], Val loss:  0.4695\n",
      "0.7800957136912193\n",
      "Epoch[15/50], Train loss:  0.4791\n",
      "Epoch[15/50], Val loss:  0.4655\n",
      "0.7754473574698294\n",
      "Epoch[16/50], Train loss:  0.4641\n",
      "Epoch[16/50], Val loss:  0.4627\n",
      "0.7748980441115272\n",
      "Epoch[17/50], Train loss:  0.4652\n",
      "Epoch[17/50], Val loss:  0.4641\n",
      "0.7796629213483146\n",
      "Epoch[18/50], Train loss:  0.4634\n",
      "Epoch[18/50], Val loss:  0.4632\n",
      "0.7772825634623386\n",
      "Epoch[19/50], Train loss:  0.4632\n",
      "Epoch[19/50], Val loss:  0.4620\n",
      "0.7874490220557635\n",
      "Epoch[20/50], Train loss:  0.4612\n",
      "Epoch[20/50], Val loss:  0.4607\n",
      "0.777677902621723\n",
      "Epoch[21/50], Train loss:  0.4587\n",
      "Epoch[21/50], Val loss:  0.4622\n",
      "0.7808739076154806\n",
      "Epoch[22/50], Train loss:  0.4561\n",
      "Epoch[22/50], Val loss:  0.4599\n",
      "0.784361215147732\n",
      "Epoch[23/50], Train loss:  0.4622\n",
      "Epoch[23/50], Val loss:  0.4580\n",
      "0.7762921348314606\n",
      "Epoch[24/50], Train loss:  0.4604\n",
      "Epoch[24/50], Val loss:  0.4615\n",
      "0.7855347482313774\n",
      "Epoch[25/50], Train loss:  0.4613\n",
      "Epoch[25/50], Val loss:  0.4580\n",
      "0.7811776945484811\n",
      "Epoch[26/50], Train loss:  0.4555\n",
      "Epoch[26/50], Val loss:  0.4563\n",
      "0.7852101539741989\n",
      "Epoch[27/50], Train loss:  0.4563\n",
      "Epoch[27/50], Val loss:  0.4581\n",
      "0.7796712442779858\n",
      "Epoch[28/50], Train loss:  0.4568\n",
      "Epoch[28/50], Val loss:  0.4551\n",
      "0.7857802746566792\n",
      "Epoch[29/50], Train loss:  0.4543\n",
      "Epoch[29/50], Val loss:  0.4575\n",
      "0.7801997503121099\n",
      "Epoch[30/50], Train loss:  0.4578\n",
      "Epoch[30/50], Val loss:  0.4560\n",
      "0.7844860590928007\n",
      "Epoch[31/50], Train loss:  0.4539\n",
      "Epoch[31/50], Val loss:  0.4536\n",
      "0.7806242197253433\n",
      "Epoch[32/50], Train loss:  0.4516\n",
      "Epoch[32/50], Val loss:  0.4590\n",
      "0.7771161048689139\n",
      "Epoch[33/50], Train loss:  0.4545\n",
      "Epoch[33/50], Val loss:  0.4580\n",
      "0.7868456096545985\n",
      "Epoch[34/50], Train loss:  0.4529\n",
      "Epoch[34/50], Val loss:  0.4534\n",
      "0.7794132334581773\n",
      "Epoch[35/50], Train loss:  0.4525\n",
      "Epoch[35/50], Val loss:  0.4555\n",
      "0.7810736579275905\n",
      "Epoch[36/50], Train loss:  0.4509\n",
      "Epoch[36/50], Val loss:  0.4546\n",
      "0.7818934665002081\n",
      "Epoch[37/50], Train loss:  0.4527\n",
      "Epoch[37/50], Val loss:  0.4534\n",
      "0.7796171452351226\n",
      "Epoch[38/50], Train loss:  0.4518\n",
      "Epoch[38/50], Val loss:  0.4545\n",
      "0.7849687890137328\n",
      "Epoch[39/50], Train loss:  0.4520\n",
      "Epoch[39/50], Val loss:  0.4556\n",
      "0.7847066167290886\n",
      "Epoch[40/50], Train loss:  0.4524\n",
      "Epoch[40/50], Val loss:  0.4571\n",
      "0.7803121098626716\n",
      "Epoch[41/50], Train loss:  0.4522\n",
      "Epoch[41/50], Val loss:  0.4529\n",
      "0.7797836038285476\n",
      "Epoch[42/50], Train loss:  0.4516\n",
      "Epoch[42/50], Val loss:  0.4550\n",
      "0.7821431543903454\n",
      "Epoch[43/50], Train loss:  0.4491\n",
      "Epoch[43/50], Val loss:  0.4542\n",
      "0.7837161880982105\n",
      "Epoch[44/50], Train loss:  0.4525\n",
      "Epoch[44/50], Val loss:  0.4567\n",
      "0.7814523512276321\n",
      "Epoch[45/50], Train loss:  0.4452\n",
      "Epoch[45/50], Val loss:  0.4550\n",
      "0.780374531835206\n",
      "Epoch[46/50], Train loss:  0.4490\n",
      "Epoch[46/50], Val loss:  0.4514\n",
      "0.7819309196837287\n",
      "Epoch[47/50], Train loss:  0.4523\n",
      "Epoch[47/50], Val loss:  0.4543\n",
      "0.7859550561797752\n",
      "Epoch[48/50], Train loss:  0.4525\n",
      "Epoch[48/50], Val loss:  0.4570\n",
      "0.780549313358302\n",
      "Epoch[49/50], Train loss:  0.4469\n",
      "Epoch[49/50], Val loss:  0.4541\n",
      "0.7809280066583437\n",
      "Epoch[50/50], Train loss:  0.4533\n",
      "Epoch[50/50], Val loss:  0.4526\n",
      "0.7828048272992094\n",
      "Epoch[1/50], Train loss:  0.6726\n",
      "Epoch[1/50], Val loss:  0.5865\n",
      "0.7304452767374116\n",
      "Epoch[2/50], Train loss:  0.5758\n",
      "Epoch[2/50], Val loss:  0.5531\n",
      "0.7480690803162714\n",
      "Epoch[3/50], Train loss:  0.5507\n",
      "Epoch[3/50], Val loss:  0.5424\n",
      "0.7767249271743655\n",
      "Epoch[4/50], Train loss:  0.5267\n",
      "Epoch[4/50], Val loss:  0.5198\n",
      "0.7718352059925094\n",
      "Epoch[5/50], Train loss:  0.5146\n",
      "Epoch[5/50], Val loss:  0.5039\n",
      "0.7756138160632542\n",
      "Epoch[6/50], Train loss:  0.4983\n",
      "Epoch[6/50], Val loss:  0.4837\n",
      "0.771223470661673\n",
      "Epoch[7/50], Train loss:  0.4882\n",
      "Epoch[7/50], Val loss:  0.4902\n",
      "0.7850645027049522\n",
      "Epoch[8/50], Train loss:  0.4831\n",
      "Epoch[8/50], Val loss:  0.4845\n",
      "0.7856346233874324\n",
      "Epoch[9/50], Train loss:  0.4827\n",
      "Epoch[9/50], Val loss:  0.4887\n",
      "0.7776154806491886\n",
      "Epoch[10/50], Train loss:  0.4800\n",
      "Epoch[10/50], Val loss:  0.4786\n",
      "0.773766125676238\n",
      "Epoch[11/50], Train loss:  0.4738\n",
      "Epoch[11/50], Val loss:  0.4712\n",
      "0.7813316687473991\n",
      "Epoch[12/50], Train loss:  0.4696\n",
      "Epoch[12/50], Val loss:  0.4667\n",
      "0.7915896795672076\n",
      "Epoch[13/50], Train loss:  0.4686\n",
      "Epoch[13/50], Val loss:  0.4673\n",
      "0.7766833125260091\n",
      "Epoch[14/50], Train loss:  0.4600\n",
      "Epoch[14/50], Val loss:  0.4587\n",
      "0.780253849354973\n",
      "Epoch[15/50], Train loss:  0.4640\n",
      "Epoch[15/50], Val loss:  0.4623\n",
      "0.7892259675405743\n",
      "Epoch[16/50], Train loss:  0.4571\n",
      "Epoch[16/50], Val loss:  0.4564\n",
      "0.7810362047440699\n",
      "Epoch[17/50], Train loss:  0.4584\n",
      "Epoch[17/50], Val loss:  0.4546\n",
      "0.7880149812734083\n",
      "Epoch[18/50], Train loss:  0.4485\n",
      "Epoch[18/50], Val loss:  0.4544\n",
      "0.7773116937161881\n",
      "Epoch[19/50], Train loss:  0.4562\n",
      "Epoch[19/50], Val loss:  0.4539\n",
      "0.7793300041614649\n",
      "Epoch[20/50], Train loss:  0.4549\n",
      "Epoch[20/50], Val loss:  0.4547\n",
      "0.7855139409071993\n",
      "Epoch[21/50], Train loss:  0.4475\n",
      "Epoch[21/50], Val loss:  0.4525\n",
      "0.7784019975031211\n",
      "Epoch[22/50], Train loss:  0.4467\n",
      "Epoch[22/50], Val loss:  0.4512\n",
      "0.7850395339159384\n",
      "Epoch[23/50], Train loss:  0.4499\n",
      "Epoch[23/50], Val loss:  0.4495\n",
      "0.7789679567207657\n",
      "Epoch[24/50], Train loss:  0.4453\n",
      "Epoch[24/50], Val loss:  0.4489\n",
      "0.7851144402829797\n",
      "Epoch[25/50], Train loss:  0.4471\n",
      "Epoch[25/50], Val loss:  0.4524\n",
      "0.7902413649604662\n",
      "Epoch[26/50], Train loss:  0.4482\n",
      "Epoch[26/50], Val loss:  0.4473\n",
      "0.781498127340824\n",
      "Epoch[27/50], Train loss:  0.4424\n",
      "Epoch[27/50], Val loss:  0.4480\n",
      "0.7833416562630046\n",
      "Epoch[28/50], Train loss:  0.4404\n",
      "Epoch[28/50], Val loss:  0.4474\n",
      "0.7853225135247607\n",
      "Epoch[29/50], Train loss:  0.4473\n",
      "Epoch[29/50], Val loss:  0.4466\n",
      "0.7799625468164794\n",
      "Epoch[30/50], Train loss:  0.4459\n",
      "Epoch[30/50], Val loss:  0.4476\n",
      "0.7833957553058676\n",
      "Epoch[31/50], Train loss:  0.4447\n",
      "Epoch[31/50], Val loss:  0.4462\n",
      "0.7836620890553475\n",
      "Epoch[32/50], Train loss:  0.4410\n",
      "Epoch[32/50], Val loss:  0.4496\n",
      "0.7818893050353725\n",
      "Epoch[33/50], Train loss:  0.4410\n",
      "Epoch[33/50], Val loss:  0.4488\n",
      "0.7806242197253432\n",
      "Epoch[34/50], Train loss:  0.4424\n",
      "Epoch[34/50], Val loss:  0.4486\n",
      "0.7842114024136497\n",
      "Epoch[35/50], Train loss:  0.4358\n",
      "Epoch[35/50], Val loss:  0.4449\n",
      "0.7850353724511028\n",
      "Epoch[36/50], Train loss:  0.4427\n",
      "Epoch[36/50], Val loss:  0.4479\n",
      "0.7825821889305036\n",
      "Epoch[37/50], Train loss:  0.4358\n",
      "Epoch[37/50], Val loss:  0.4462\n",
      "0.7849126092384519\n",
      "Epoch[38/50], Train loss:  0.4424\n",
      "Epoch[38/50], Val loss:  0.4470\n",
      "0.7824469413233458\n",
      "Epoch[39/50], Train loss:  0.4418\n",
      "Epoch[39/50], Val loss:  0.4460\n",
      "0.7826633374947982\n",
      "Epoch[40/50], Train loss:  0.4438\n",
      "Epoch[40/50], Val loss:  0.4440\n",
      "0.7831002913025386\n",
      "Epoch[41/50], Train loss:  0.4388\n",
      "Epoch[41/50], Val loss:  0.4461\n",
      "0.7789846025801083\n",
      "Epoch[42/50], Train loss:  0.4370\n",
      "Epoch[42/50], Val loss:  0.4446\n",
      "0.7858385351643778\n",
      "Epoch[43/50], Train loss:  0.4421\n",
      "Epoch[43/50], Val loss:  0.4479\n",
      "0.7847981689554723\n",
      "Epoch[44/50], Train loss:  0.4389\n",
      "Epoch[44/50], Val loss:  0.4440\n",
      "0.7846774864752393\n",
      "Epoch[45/50], Train loss:  0.4454\n",
      "Epoch[45/50], Val loss:  0.4481\n",
      "0.7840615896795673\n",
      "Epoch[46/50], Train loss:  0.4422\n",
      "Epoch[46/50], Val loss:  0.4439\n",
      "0.7810320432792343\n",
      "Epoch[47/50], Train loss:  0.4401\n",
      "Epoch[47/50], Val loss:  0.4461\n",
      "0.7842863087806908\n",
      "Epoch[48/50], Train loss:  0.4425\n",
      "Epoch[48/50], Val loss:  0.4452\n",
      "0.781364960466084\n",
      "Epoch[49/50], Train loss:  0.4427\n",
      "Epoch[49/50], Val loss:  0.4449\n",
      "0.781431543903454\n",
      "Epoch[50/50], Train loss:  0.4409\n",
      "Epoch[50/50], Val loss:  0.4453\n",
      "0.7829379941739492\n",
      "Epoch[1/50], Train loss:  0.6819\n",
      "Epoch[1/50], Val loss:  0.5884\n",
      "0.7387598834789846\n",
      "Epoch[2/50], Train loss:  0.5691\n",
      "Epoch[2/50], Val loss:  0.5352\n",
      "0.7593965875988349\n",
      "Epoch[3/50], Train loss:  0.5440\n",
      "Epoch[3/50], Val loss:  0.5550\n",
      "0.7545776113191843\n",
      "Epoch[4/50], Train loss:  0.5386\n",
      "Epoch[4/50], Val loss:  0.5288\n",
      "0.7494340407823554\n",
      "Epoch[5/50], Train loss:  0.5223\n",
      "Epoch[5/50], Val loss:  0.5122\n",
      "0.7935871826883062\n",
      "Epoch[6/50], Train loss:  0.5051\n",
      "Epoch[6/50], Val loss:  0.4900\n",
      "0.7643029546400332\n",
      "Epoch[7/50], Train loss:  0.4970\n",
      "Epoch[7/50], Val loss:  0.4806\n",
      "0.7693300041614648\n",
      "Epoch[8/50], Train loss:  0.4892\n",
      "Epoch[8/50], Val loss:  0.4819\n",
      "0.7797960882230546\n",
      "Epoch[9/50], Train loss:  0.4868\n",
      "Epoch[9/50], Val loss:  0.4787\n",
      "0.771377444860591\n",
      "Epoch[10/50], Train loss:  0.4841\n",
      "Epoch[10/50], Val loss:  0.4771\n",
      "0.783435289221806\n",
      "Epoch[11/50], Train loss:  0.4746\n",
      "Epoch[11/50], Val loss:  0.4669\n",
      "0.7798876404494381\n",
      "Epoch[12/50], Train loss:  0.4678\n",
      "Epoch[12/50], Val loss:  0.4652\n",
      "0.7715230961298378\n",
      "Epoch[13/50], Train loss:  0.4631\n",
      "Epoch[13/50], Val loss:  0.4642\n",
      "0.7789263420724095\n",
      "Epoch[14/50], Train loss:  0.4677\n",
      "Epoch[14/50], Val loss:  0.4634\n",
      "0.7866500208073242\n",
      "Epoch[15/50], Train loss:  0.4680\n",
      "Epoch[15/50], Val loss:  0.4576\n",
      "0.779317519766958\n",
      "Epoch[16/50], Train loss:  0.4646\n",
      "Epoch[16/50], Val loss:  0.4579\n",
      "0.7913025384935497\n",
      "Epoch[17/50], Train loss:  0.4563\n",
      "Epoch[17/50], Val loss:  0.4544\n",
      "0.7839741989180191\n",
      "Epoch[18/50], Train loss:  0.4631\n",
      "Epoch[18/50], Val loss:  0.4555\n",
      "0.7850270495214314\n",
      "Epoch[19/50], Train loss:  0.4558\n",
      "Epoch[19/50], Val loss:  0.4544\n",
      "0.7839617145235123\n",
      "Epoch[20/50], Train loss:  0.4578\n",
      "Epoch[20/50], Val loss:  0.4515\n",
      "0.783766125676238\n",
      "Epoch[21/50], Train loss:  0.4507\n",
      "Epoch[21/50], Val loss:  0.4522\n",
      "0.7854265501456512\n",
      "Epoch[22/50], Train loss:  0.4518\n",
      "Epoch[22/50], Val loss:  0.4518\n",
      "0.7859425717852684\n",
      "Epoch[23/50], Train loss:  0.4486\n",
      "Epoch[23/50], Val loss:  0.4523\n",
      "0.7871951727007906\n",
      "Epoch[24/50], Train loss:  0.4451\n",
      "Epoch[24/50], Val loss:  0.4506\n",
      "0.7834748231377445\n",
      "Epoch[25/50], Train loss:  0.4493\n",
      "Epoch[25/50], Val loss:  0.4523\n",
      "0.7811610486891385\n",
      "Epoch[26/50], Train loss:  0.4578\n",
      "Epoch[26/50], Val loss:  0.4493\n",
      "0.7856554307116106\n",
      "Epoch[27/50], Train loss:  0.4450\n",
      "Epoch[27/50], Val loss:  0.4490\n",
      "0.7832792342904703\n",
      "Epoch[28/50], Train loss:  0.4435\n",
      "Epoch[28/50], Val loss:  0.4489\n",
      "0.7898293799417396\n",
      "Epoch[29/50], Train loss:  0.4452\n",
      "Epoch[29/50], Val loss:  0.4514\n",
      "0.7854764877236787\n",
      "Epoch[30/50], Train loss:  0.4511\n",
      "Epoch[30/50], Val loss:  0.4500\n",
      "0.7890719933416562\n",
      "Epoch[31/50], Train loss:  0.4435\n",
      "Epoch[31/50], Val loss:  0.4485\n",
      "0.786333749479817\n",
      "Epoch[32/50], Train loss:  0.4497\n",
      "Epoch[32/50], Val loss:  0.4497\n",
      "0.7892093216812319\n",
      "Epoch[33/50], Train loss:  0.4441\n",
      "Epoch[33/50], Val loss:  0.4467\n",
      "0.7867998335414066\n",
      "Epoch[34/50], Train loss:  0.4470\n",
      "Epoch[34/50], Val loss:  0.4486\n",
      "0.7884893882646692\n",
      "Epoch[35/50], Train loss:  0.4476\n",
      "Epoch[35/50], Val loss:  0.4479\n",
      "0.7869912609238452\n",
      "Epoch[36/50], Train loss:  0.4435\n",
      "Epoch[36/50], Val loss:  0.4493\n",
      "0.7861964211402415\n",
      "Epoch[37/50], Train loss:  0.4509\n",
      "Epoch[37/50], Val loss:  0.4500\n",
      "0.7839908447773616\n",
      "Epoch[38/50], Train loss:  0.4431\n",
      "Epoch[38/50], Val loss:  0.4493\n",
      "0.7830087390761548\n",
      "Epoch[39/50], Train loss:  0.4473\n",
      "Epoch[39/50], Val loss:  0.4483\n",
      "0.7867041198501873\n",
      "Epoch[40/50], Train loss:  0.4477\n",
      "Epoch[40/50], Val loss:  0.4497\n",
      "0.7861215147732002\n",
      "Epoch[41/50], Train loss:  0.4447\n",
      "Epoch[41/50], Val loss:  0.4492\n",
      "0.7796837286724927\n",
      "Epoch[42/50], Train loss:  0.4472\n",
      "Epoch[42/50], Val loss:  0.4486\n",
      "0.7803641281731171\n",
      "Epoch[43/50], Train loss:  0.4418\n",
      "Epoch[43/50], Val loss:  0.4458\n",
      "0.784273824386184\n",
      "Epoch[44/50], Train loss:  0.4414\n",
      "Epoch[44/50], Val loss:  0.4496\n",
      "0.784989596337911\n",
      "Epoch[45/50], Train loss:  0.4453\n",
      "Epoch[45/50], Val loss:  0.4490\n",
      "0.7815147732001666\n",
      "Epoch[46/50], Train loss:  0.4430\n",
      "Epoch[46/50], Val loss:  0.4458\n",
      "0.782863087806908\n",
      "Epoch[47/50], Train loss:  0.4420\n",
      "Epoch[47/50], Val loss:  0.4460\n",
      "0.785705368289638\n",
      "Epoch[48/50], Train loss:  0.4440\n",
      "Epoch[48/50], Val loss:  0.4461\n",
      "0.7836537661256762\n",
      "Epoch[49/50], Train loss:  0.4440\n",
      "Epoch[49/50], Val loss:  0.4473\n",
      "0.7866250520183105\n",
      "Epoch[50/50], Train loss:  0.4474\n",
      "Epoch[50/50], Val loss:  0.4468\n",
      "0.7878942987931753\n",
      "Epoch[1/50], Train loss:  0.6725\n",
      "Epoch[1/50], Val loss:  0.5970\n",
      "0.731377444860591\n",
      "Epoch[2/50], Train loss:  0.5703\n",
      "Epoch[2/50], Val loss:  0.5772\n",
      "0.7646608406158968\n",
      "Epoch[3/50], Train loss:  0.5471\n",
      "Epoch[3/50], Val loss:  0.5232\n",
      "0.7665626300457761\n",
      "Epoch[4/50], Train loss:  0.5292\n",
      "Epoch[4/50], Val loss:  0.5139\n",
      "0.7776071577195172\n",
      "Epoch[5/50], Train loss:  0.5171\n",
      "Epoch[5/50], Val loss:  0.5093\n",
      "0.7649313358302122\n",
      "Epoch[6/50], Train loss:  0.5006\n",
      "Epoch[6/50], Val loss:  0.4867\n",
      "0.7745900957136913\n",
      "Epoch[7/50], Train loss:  0.4929\n",
      "Epoch[7/50], Val loss:  0.4885\n",
      "0.7654556803995005\n",
      "Epoch[8/50], Train loss:  0.4907\n",
      "Epoch[8/50], Val loss:  0.4806\n",
      "0.7726841448189763\n",
      "Epoch[9/50], Train loss:  0.4918\n",
      "Epoch[9/50], Val loss:  0.4800\n",
      "0.7809904286308781\n",
      "Epoch[10/50], Train loss:  0.4836\n",
      "Epoch[10/50], Val loss:  0.4810\n",
      "0.7891677070328756\n",
      "Epoch[11/50], Train loss:  0.4742\n",
      "Epoch[11/50], Val loss:  0.4710\n",
      "0.7818684977111944\n",
      "Epoch[12/50], Train loss:  0.4774\n",
      "Epoch[12/50], Val loss:  0.4659\n",
      "0.7779567207657095\n",
      "Epoch[13/50], Train loss:  0.4638\n",
      "Epoch[13/50], Val loss:  0.4675\n",
      "0.7937952559300874\n",
      "Epoch[14/50], Train loss:  0.4619\n",
      "Epoch[14/50], Val loss:  0.4684\n",
      "0.7914232209737828\n",
      "Epoch[15/50], Train loss:  0.4646\n",
      "Epoch[15/50], Val loss:  0.4620\n",
      "0.7981190178942988\n",
      "Epoch[16/50], Train loss:  0.4615\n",
      "Epoch[16/50], Val loss:  0.4573\n",
      "0.7865709529754474\n",
      "Epoch[17/50], Train loss:  0.4573\n",
      "Epoch[17/50], Val loss:  0.4574\n",
      "0.7909321681231793\n",
      "Epoch[18/50], Train loss:  0.4576\n",
      "Epoch[18/50], Val loss:  0.4587\n",
      "0.7802538493549729\n",
      "Epoch[19/50], Train loss:  0.4536\n",
      "Epoch[19/50], Val loss:  0.4554\n",
      "0.7899833541406576\n",
      "Epoch[20/50], Train loss:  0.4543\n",
      "Epoch[20/50], Val loss:  0.4528\n",
      "0.7844777361631294\n",
      "Epoch[21/50], Train loss:  0.4471\n",
      "Epoch[21/50], Val loss:  0.4521\n",
      "0.7833666250520184\n",
      "Epoch[22/50], Train loss:  0.4433\n",
      "Epoch[22/50], Val loss:  0.4509\n",
      "0.7842072409488141\n",
      "Epoch[23/50], Train loss:  0.4468\n",
      "Epoch[23/50], Val loss:  0.4511\n",
      "0.7842030794839785\n",
      "Epoch[24/50], Train loss:  0.4466\n",
      "Epoch[24/50], Val loss:  0.4523\n",
      "0.7941032043279235\n",
      "Epoch[25/50], Train loss:  0.4466\n",
      "Epoch[25/50], Val loss:  0.4476\n",
      "0.7871285892634208\n",
      "Epoch[26/50], Train loss:  0.4483\n",
      "Epoch[26/50], Val loss:  0.4480\n",
      "0.7869121930919685\n",
      "Epoch[27/50], Train loss:  0.4436\n",
      "Epoch[27/50], Val loss:  0.4487\n",
      "0.7869579692051601\n",
      "Epoch[28/50], Train loss:  0.4429\n",
      "Epoch[28/50], Val loss:  0.4486\n",
      "0.7857594673325011\n",
      "Epoch[29/50], Train loss:  0.4424\n",
      "Epoch[29/50], Val loss:  0.4465\n",
      "0.7880565959217645\n",
      "Epoch[30/50], Train loss:  0.4405\n",
      "Epoch[30/50], Val loss:  0.4469\n",
      "0.785230961298377\n",
      "Epoch[31/50], Train loss:  0.4423\n",
      "Epoch[31/50], Val loss:  0.4476\n",
      "0.7841489804411153\n",
      "Epoch[32/50], Train loss:  0.4389\n",
      "Epoch[32/50], Val loss:  0.4457\n",
      "0.7837515605493133\n",
      "Epoch[33/50], Train loss:  0.4413\n",
      "Epoch[33/50], Val loss:  0.4472\n",
      "0.7836287973366625\n",
      "Epoch[34/50], Train loss:  0.4415\n",
      "Epoch[34/50], Val loss:  0.4471\n",
      "0.7844777361631294\n",
      "Epoch[35/50], Train loss:  0.4471\n",
      "Epoch[35/50], Val loss:  0.4450\n",
      "0.7879525593008739\n",
      "Epoch[36/50], Train loss:  0.4425\n",
      "Epoch[36/50], Val loss:  0.4449\n",
      "0.7884852267998336\n",
      "Epoch[37/50], Train loss:  0.4404\n",
      "Epoch[37/50], Val loss:  0.4473\n",
      "0.7854806491885143\n",
      "Epoch[38/50], Train loss:  0.4354\n",
      "Epoch[38/50], Val loss:  0.4453\n",
      "0.7909612983770287\n",
      "Epoch[39/50], Train loss:  0.4444\n",
      "Epoch[39/50], Val loss:  0.4449\n",
      "0.7851186017478153\n",
      "Epoch[40/50], Train loss:  0.4422\n",
      "Epoch[40/50], Val loss:  0.4471\n",
      "0.7881980857261757\n",
      "Epoch[41/50], Train loss:  0.4418\n",
      "Epoch[41/50], Val loss:  0.4458\n",
      "0.7883936745734498\n",
      "Epoch[42/50], Train loss:  0.4456\n",
      "Epoch[42/50], Val loss:  0.4471\n",
      "0.7806616729088638\n",
      "Epoch[43/50], Train loss:  0.4385\n",
      "Epoch[43/50], Val loss:  0.4453\n",
      "0.7873200166458594\n",
      "Epoch[44/50], Train loss:  0.4378\n",
      "Epoch[44/50], Val loss:  0.4450\n",
      "0.783337494798169\n",
      "Epoch[45/50], Train loss:  0.4421\n",
      "Epoch[45/50], Val loss:  0.4472\n",
      "0.7862588431127757\n",
      "Epoch[46/50], Train loss:  0.4421\n",
      "Epoch[46/50], Val loss:  0.4470\n",
      "0.7879525593008739\n",
      "Epoch[47/50], Train loss:  0.4387\n",
      "Epoch[47/50], Val loss:  0.4456\n",
      "0.7862463587182689\n",
      "Epoch[48/50], Train loss:  0.4444\n",
      "Epoch[48/50], Val loss:  0.4448\n",
      "0.7850520183104452\n",
      "Epoch[49/50], Train loss:  0.4395\n",
      "Epoch[49/50], Val loss:  0.4470\n",
      "0.7851727007906784\n",
      "Epoch[50/50], Train loss:  0.4453\n",
      "Epoch[50/50], Val loss:  0.4461\n",
      "0.7881065334997919\n",
      "Epoch[1/50], Train loss:  0.6747\n",
      "Epoch[1/50], Val loss:  0.6005\n",
      "0.7470287141073658\n",
      "Epoch[2/50], Train loss:  0.5770\n",
      "Epoch[2/50], Val loss:  0.5497\n",
      "0.7419434040782356\n",
      "Epoch[3/50], Train loss:  0.5464\n",
      "Epoch[3/50], Val loss:  0.5302\n",
      "0.7719642114024137\n",
      "Epoch[4/50], Train loss:  0.5448\n",
      "Epoch[4/50], Val loss:  0.5185\n",
      "0.7557636287973368\n",
      "Epoch[5/50], Train loss:  0.5255\n",
      "Epoch[5/50], Val loss:  0.5135\n",
      "0.7748772367873492\n",
      "Epoch[6/50], Train loss:  0.5086\n",
      "Epoch[6/50], Val loss:  0.4977\n",
      "0.7680940491052851\n",
      "Epoch[7/50], Train loss:  0.4993\n",
      "Epoch[7/50], Val loss:  0.4913\n",
      "0.7766167290886393\n",
      "Epoch[8/50], Train loss:  0.4983\n",
      "Epoch[8/50], Val loss:  0.5022\n",
      "0.7945568039950062\n",
      "Epoch[9/50], Train loss:  0.4954\n",
      "Epoch[9/50], Val loss:  0.4821\n",
      "0.7781481481481483\n",
      "Epoch[10/50], Train loss:  0.4879\n",
      "Epoch[10/50], Val loss:  0.4909\n",
      "0.7859592176446109\n",
      "Epoch[11/50], Train loss:  0.4824\n",
      "Epoch[11/50], Val loss:  0.4821\n",
      "0.7841531419059509\n",
      "Epoch[12/50], Train loss:  0.4729\n",
      "Epoch[12/50], Val loss:  0.4696\n",
      "0.7818976279650437\n",
      "Epoch[13/50], Train loss:  0.4795\n",
      "Epoch[13/50], Val loss:  0.4735\n",
      "0.7866250520183105\n",
      "Epoch[14/50], Train loss:  0.4697\n",
      "Epoch[14/50], Val loss:  0.4760\n",
      "0.7813108614232209\n",
      "Epoch[15/50], Train loss:  0.4718\n",
      "Epoch[15/50], Val loss:  0.4638\n",
      "0.7826342072409489\n",
      "Epoch[16/50], Train loss:  0.4650\n",
      "Epoch[16/50], Val loss:  0.4639\n",
      "0.783083645443196\n",
      "Epoch[17/50], Train loss:  0.4633\n",
      "Epoch[17/50], Val loss:  0.4605\n",
      "0.7802746566791512\n",
      "Epoch[18/50], Train loss:  0.4605\n",
      "Epoch[18/50], Val loss:  0.4625\n",
      "0.7881148564294631\n",
      "Epoch[19/50], Train loss:  0.4618\n",
      "Epoch[19/50], Val loss:  0.4608\n",
      "0.7894423637120266\n",
      "Epoch[20/50], Train loss:  0.4637\n",
      "Epoch[20/50], Val loss:  0.4691\n",
      "0.797786100707449\n",
      "Epoch[21/50], Train loss:  0.4580\n",
      "Epoch[21/50], Val loss:  0.4592\n",
      "0.7872992093216812\n",
      "Epoch[22/50], Train loss:  0.4545\n",
      "Epoch[22/50], Val loss:  0.4570\n",
      "0.7826966292134832\n",
      "Epoch[23/50], Train loss:  0.4529\n",
      "Epoch[23/50], Val loss:  0.4582\n",
      "0.7824760715771952\n",
      "Epoch[24/50], Train loss:  0.4534\n",
      "Epoch[24/50], Val loss:  0.4560\n",
      "0.7817436537661255\n",
      "Epoch[25/50], Train loss:  0.4504\n",
      "Epoch[25/50], Val loss:  0.4569\n",
      "0.7857012068248023\n",
      "Epoch[26/50], Train loss:  0.4493\n",
      "Epoch[26/50], Val loss:  0.4539\n",
      "0.7849438202247192\n",
      "Epoch[27/50], Train loss:  0.4530\n",
      "Epoch[27/50], Val loss:  0.4564\n",
      "0.7804785684560966\n",
      "Epoch[28/50], Train loss:  0.4524\n",
      "Epoch[28/50], Val loss:  0.4512\n",
      "0.7797960882230546\n",
      "Epoch[29/50], Train loss:  0.4462\n",
      "Epoch[29/50], Val loss:  0.4554\n",
      "0.7863420724094881\n",
      "Epoch[30/50], Train loss:  0.4496\n",
      "Epoch[30/50], Val loss:  0.4504\n",
      "0.7867082813150229\n",
      "Epoch[31/50], Train loss:  0.4484\n",
      "Epoch[31/50], Val loss:  0.4525\n",
      "0.7821140241364961\n",
      "Epoch[32/50], Train loss:  0.4556\n",
      "Epoch[32/50], Val loss:  0.4533\n",
      "0.7844111527257595\n",
      "Epoch[33/50], Train loss:  0.4464\n",
      "Epoch[33/50], Val loss:  0.4548\n",
      "0.7843112775697045\n",
      "Epoch[34/50], Train loss:  0.4446\n",
      "Epoch[34/50], Val loss:  0.4518\n",
      "0.7849979192675822\n",
      "Epoch[35/50], Train loss:  0.4445\n",
      "Epoch[35/50], Val loss:  0.4513\n",
      "0.784935497295048\n",
      "Epoch[36/50], Train loss:  0.4477\n",
      "Epoch[36/50], Val loss:  0.4564\n",
      "0.7841989180191429\n",
      "Epoch[37/50], Train loss:  0.4507\n",
      "Epoch[37/50], Val loss:  0.4529\n",
      "0.7821972534332085\n",
      "Epoch[38/50], Train loss:  0.4453\n",
      "Epoch[38/50], Val loss:  0.4506\n",
      "0.783953391593841\n",
      "Epoch[39/50], Train loss:  0.4508\n",
      "Epoch[39/50], Val loss:  0.4515\n",
      "0.7860133166874741\n",
      "Epoch[40/50], Train loss:  0.4494\n",
      "Epoch[40/50], Val loss:  0.4556\n",
      "0.7816937161880982\n",
      "Epoch[41/50], Train loss:  0.4520\n",
      "Epoch[41/50], Val loss:  0.4487\n",
      "0.785364128173117\n",
      "Epoch[42/50], Train loss:  0.4442\n",
      "Epoch[42/50], Val loss:  0.4512\n",
      "0.7859675405742821\n",
      "Epoch[43/50], Train loss:  0.4516\n",
      "Epoch[43/50], Val loss:  0.4499\n",
      "0.7864544319600499\n",
      "Epoch[44/50], Train loss:  0.4467\n",
      "Epoch[44/50], Val loss:  0.4511\n",
      "0.7860008322929672\n",
      "Epoch[45/50], Train loss:  0.4510\n",
      "Epoch[45/50], Val loss:  0.4510\n",
      "0.7829588014981275\n",
      "Epoch[46/50], Train loss:  0.4489\n",
      "Epoch[46/50], Val loss:  0.4550\n",
      "0.7797128589263422\n",
      "Epoch[47/50], Train loss:  0.4488\n",
      "Epoch[47/50], Val loss:  0.4535\n",
      "0.7820183104452769\n",
      "Epoch[48/50], Train loss:  0.4463\n",
      "Epoch[48/50], Val loss:  0.4513\n",
      "0.7831502288805661\n",
      "Epoch[49/50], Train loss:  0.4510\n",
      "Epoch[49/50], Val loss:  0.4537\n",
      "0.7836620890553474\n",
      "Epoch[50/50], Train loss:  0.4515\n",
      "Epoch[50/50], Val loss:  0.4506\n",
      "0.7844069912609238\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:37:32.889406Z",
     "start_time": "2024-05-25T11:37:32.883543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if training:\n",
    "    emb_spaces=[32, 64, 128, 256, 512]\n",
    "    for emb_space_size, measure in zip(emb_spaces, measures):\n",
    "        print(f\"Emb space size: {emb_space_size}, Train loss: {measure}\")\n",
    "    print(f\"Best emb space size: {emb_spaces[np.argmin([measure for measure in measures])]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emb space size: 32, Train loss: 0.45140895048777263\n",
      "Emb space size: 64, Train loss: 0.4439425359169642\n",
      "Emb space size: 128, Train loss: 0.44581543107827504\n",
      "Emb space size: 256, Train loss: 0.444801339507103\n",
      "Emb space size: 512, Train loss: 0.44873113334178927\n",
      "Best emb space size: 64\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T11:37:32.955241Z",
     "start_time": "2024-05-25T11:37:32.890713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# take the best one and train it for more epochs\n",
    "if training:\n",
    "    emb_space_measures=[32, 64, 128, 256, 512]\n",
    "    model = Autoencoder(encoding_dim=emb_space_measures[np.argmin([measure for measure in measures])], input_size=input_size)\n",
    "    model = model.to(CONFIG[\"device\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T12:29:53.278784Z",
     "start_time": "2024-05-25T11:37:32.956641Z"
    }
   },
   "source": [
    "training=True\n",
    "if training:\n",
    "    train_model(model, train_dl, val_dl, test_dl, CONFIG[\"criterion\"], optimizer, CONFIG[\"device\"], wandb, CONFIG[\"epochs\"], 50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/500], Train loss:  0.6842\n",
      "Epoch[1/500], Val loss:  0.6048\n",
      "0.7135830212234707\n",
      "Epoch[2/500], Train loss:  0.5814\n",
      "Epoch[2/500], Val loss:  0.5660\n",
      "0.7516853932584271\n",
      "Epoch[3/500], Train loss:  0.5628\n",
      "Epoch[3/500], Val loss:  0.5384\n",
      "0.763017062005826\n",
      "Epoch[4/500], Train loss:  0.5421\n",
      "Epoch[4/500], Val loss:  0.5310\n",
      "0.7494298793175198\n",
      "Epoch[5/500], Train loss:  0.5227\n",
      "Epoch[5/500], Val loss:  0.5070\n",
      "0.7659300873907617\n",
      "Epoch[6/500], Train loss:  0.5153\n",
      "Epoch[6/500], Val loss:  0.5094\n",
      "0.7873325010403662\n",
      "Epoch[7/500], Train loss:  0.5099\n",
      "Epoch[7/500], Val loss:  0.4939\n",
      "0.7687723678734915\n",
      "Epoch[8/500], Train loss:  0.5014\n",
      "Epoch[8/500], Val loss:  0.5528\n",
      "0.7629588014981273\n",
      "Epoch[9/500], Train loss:  0.4966\n",
      "Epoch[9/500], Val loss:  0.4867\n",
      "0.7868456096545984\n",
      "Epoch[10/500], Train loss:  0.4931\n",
      "Epoch[10/500], Val loss:  0.4875\n",
      "0.774889721181856\n",
      "Epoch[11/500], Train loss:  0.4860\n",
      "Epoch[11/500], Val loss:  0.4859\n",
      "0.792544735746983\n",
      "Epoch[12/500], Train loss:  0.4783\n",
      "Epoch[12/500], Val loss:  0.4929\n",
      "0.7724427798585103\n",
      "Epoch[13/500], Train loss:  0.4715\n",
      "Epoch[13/500], Val loss:  0.4740\n",
      "0.7832084893882647\n",
      "Epoch[14/500], Train loss:  0.4657\n",
      "Epoch[14/500], Val loss:  0.4684\n",
      "0.7860341240116521\n",
      "Epoch[15/500], Train loss:  0.4600\n",
      "Epoch[15/500], Val loss:  0.4670\n",
      "0.7940324594257179\n",
      "Epoch[16/500], Train loss:  0.4613\n",
      "Epoch[16/500], Val loss:  0.4612\n",
      "0.7887141073657927\n",
      "Epoch[17/500], Train loss:  0.4573\n",
      "Epoch[17/500], Val loss:  0.4610\n",
      "0.7893632958801498\n",
      "Epoch[18/500], Train loss:  0.4602\n",
      "Epoch[18/500], Val loss:  0.4580\n",
      "0.7960923845193508\n",
      "Epoch[19/500], Train loss:  0.4540\n",
      "Epoch[19/500], Val loss:  0.4558\n",
      "0.7894090719933416\n",
      "Epoch[20/500], Train loss:  0.4513\n",
      "Epoch[20/500], Val loss:  0.4579\n",
      "0.7982854764877236\n",
      "Epoch[21/500], Train loss:  0.4487\n",
      "Epoch[21/500], Val loss:  0.4594\n",
      "0.7921556387848523\n",
      "Epoch[22/500], Train loss:  0.4487\n",
      "Epoch[22/500], Val loss:  0.4537\n",
      "0.7928797336662504\n",
      "Epoch[23/500], Train loss:  0.4449\n",
      "Epoch[23/500], Val loss:  0.4487\n",
      "0.7942322097378277\n",
      "Epoch[24/500], Train loss:  0.4484\n",
      "Epoch[24/500], Val loss:  0.4488\n",
      "0.8041781106949647\n",
      "Epoch[25/500], Train loss:  0.4430\n",
      "Epoch[25/500], Val loss:  0.4473\n",
      "0.7976820640865585\n",
      "Epoch[26/500], Train loss:  0.4359\n",
      "Epoch[26/500], Val loss:  0.4484\n",
      "0.7960216396171453\n",
      "Epoch[27/500], Train loss:  0.4355\n",
      "Epoch[27/500], Val loss:  0.4412\n",
      "0.7955930087390761\n",
      "Epoch[28/500], Train loss:  0.4364\n",
      "Epoch[28/500], Val loss:  0.4431\n",
      "0.7995838535164378\n",
      "Epoch[29/500], Train loss:  0.4394\n",
      "Epoch[29/500], Val loss:  0.4450\n",
      "0.7960590928006658\n",
      "Epoch[30/500], Train loss:  0.4335\n",
      "Epoch[30/500], Val loss:  0.4362\n",
      "0.8007990012484395\n",
      "Epoch[31/500], Train loss:  0.4330\n",
      "Epoch[31/500], Val loss:  0.4406\n",
      "0.8036912193091968\n",
      "Epoch[32/500], Train loss:  0.4363\n",
      "Epoch[32/500], Val loss:  0.4360\n",
      "0.8009571369121931\n",
      "Epoch[33/500], Train loss:  0.4309\n",
      "Epoch[33/500], Val loss:  0.4426\n",
      "0.7943487307532251\n",
      "Epoch[34/500], Train loss:  0.4286\n",
      "Epoch[34/500], Val loss:  0.4349\n",
      "0.8002580108198086\n",
      "Epoch[35/500], Train loss:  0.4270\n",
      "Epoch[35/500], Val loss:  0.4466\n",
      "0.8152517686225551\n",
      "Epoch[36/500], Train loss:  0.4250\n",
      "Epoch[36/500], Val loss:  0.4371\n",
      "0.8197086974615064\n",
      "Epoch[37/500], Train loss:  0.4241\n",
      "Epoch[37/500], Val loss:  0.4343\n",
      "0.7987515605493134\n",
      "Epoch[38/500], Train loss:  0.4255\n",
      "Epoch[38/500], Val loss:  0.4315\n",
      "0.8025260091552227\n",
      "Epoch[39/500], Train loss:  0.4236\n",
      "Epoch[39/500], Val loss:  0.4375\n",
      "0.8157511444028298\n",
      "Epoch[40/500], Train loss:  0.4246\n",
      "Epoch[40/500], Val loss:  0.4365\n",
      "0.8128506034124012\n",
      "Epoch[41/500], Train loss:  0.4280\n",
      "Epoch[41/500], Val loss:  0.4332\n",
      "0.8119225967540574\n",
      "Epoch[42/500], Train loss:  0.4182\n",
      "Epoch[42/500], Val loss:  0.4460\n",
      "0.8164710778193924\n",
      "Epoch[43/500], Train loss:  0.4219\n",
      "Epoch[43/500], Val loss:  0.4278\n",
      "0.8107906783187682\n",
      "Epoch[44/500], Train loss:  0.4203\n",
      "Epoch[44/500], Val loss:  0.4273\n",
      "0.8089762796504368\n",
      "Epoch[45/500], Train loss:  0.4190\n",
      "Epoch[45/500], Val loss:  0.4333\n",
      "0.8173699542238868\n",
      "Epoch[46/500], Train loss:  0.4226\n",
      "Epoch[46/500], Val loss:  0.4528\n",
      "0.7825426550145651\n",
      "Epoch[47/500], Train loss:  0.4160\n",
      "Epoch[47/500], Val loss:  0.4272\n",
      "0.8163087806908031\n",
      "Epoch[48/500], Train loss:  0.4203\n",
      "Epoch[48/500], Val loss:  0.4296\n",
      "0.8251851851851852\n",
      "Epoch[49/500], Train loss:  0.4144\n",
      "Epoch[49/500], Val loss:  0.4333\n",
      "0.8215522263836871\n",
      "Epoch[50/500], Train loss:  0.4134\n",
      "Epoch[50/500], Val loss:  0.4285\n",
      "0.8209321681231793\n",
      "Epoch[51/500], Train loss:  0.4099\n",
      "Epoch[51/500], Val loss:  0.4183\n",
      "0.8321514773200167\n",
      "Epoch[52/500], Train loss:  0.4029\n",
      "Epoch[52/500], Val loss:  0.4234\n",
      "0.8200041614648357\n",
      "Epoch[53/500], Train loss:  0.4052\n",
      "Epoch[53/500], Val loss:  0.4191\n",
      "0.8179816895547232\n",
      "Epoch[54/500], Train loss:  0.4025\n",
      "Epoch[54/500], Val loss:  0.4187\n",
      "0.8219142738243863\n",
      "Epoch[55/500], Train loss:  0.4023\n",
      "Epoch[55/500], Val loss:  0.4272\n",
      "0.8192134831460676\n",
      "Epoch[56/500], Train loss:  0.4038\n",
      "Epoch[56/500], Val loss:  0.4169\n",
      "0.8275156054931336\n",
      "Epoch[57/500], Train loss:  0.4066\n",
      "Epoch[57/500], Val loss:  0.4196\n",
      "0.8276487723678734\n",
      "Epoch[58/500], Train loss:  0.4040\n",
      "Epoch[58/500], Val loss:  0.4187\n",
      "0.8269205160216396\n",
      "Epoch[59/500], Train loss:  0.4046\n",
      "Epoch[59/500], Val loss:  0.4199\n",
      "0.828481065334998\n",
      "Epoch[60/500], Train loss:  0.4000\n",
      "Epoch[60/500], Val loss:  0.4191\n",
      "0.8334581772784021\n",
      "Epoch[61/500], Train loss:  0.3988\n",
      "Epoch[61/500], Val loss:  0.4156\n",
      "0.8297045359966709\n",
      "Epoch[62/500], Train loss:  0.4050\n",
      "Epoch[62/500], Val loss:  0.4166\n",
      "0.8323761964211402\n",
      "Epoch[63/500], Train loss:  0.3995\n",
      "Epoch[63/500], Val loss:  0.4160\n",
      "0.8301248439450687\n",
      "Epoch[64/500], Train loss:  0.4019\n",
      "Epoch[64/500], Val loss:  0.4213\n",
      "0.8253058676654182\n",
      "Epoch[65/500], Train loss:  0.4029\n",
      "Epoch[65/500], Val loss:  0.4166\n",
      "0.8312817311693717\n",
      "Epoch[66/500], Train loss:  0.4000\n",
      "Epoch[66/500], Val loss:  0.4216\n",
      "0.8217977528089887\n",
      "Epoch[67/500], Train loss:  0.4000\n",
      "Epoch[67/500], Val loss:  0.4152\n",
      "0.8349812734082396\n",
      "Epoch[68/500], Train loss:  0.4035\n",
      "Epoch[68/500], Val loss:  0.4197\n",
      "0.8417977528089887\n",
      "Epoch[69/500], Train loss:  0.4029\n",
      "Epoch[69/500], Val loss:  0.4156\n",
      "0.8294132334581773\n",
      "Epoch[70/500], Train loss:  0.4012\n",
      "Epoch[70/500], Val loss:  0.4182\n",
      "0.8304078235538911\n",
      "Epoch[71/500], Train loss:  0.4027\n",
      "Epoch[71/500], Val loss:  0.4188\n",
      "0.8396878901373283\n",
      "Epoch[72/500], Train loss:  0.4031\n",
      "Epoch[72/500], Val loss:  0.4139\n",
      "0.8365834373699541\n",
      "Epoch[73/500], Train loss:  0.3997\n",
      "Epoch[73/500], Val loss:  0.4180\n",
      "0.8304327923429048\n",
      "Epoch[74/500], Train loss:  0.3968\n",
      "Epoch[74/500], Val loss:  0.4144\n",
      "0.8369662921348315\n",
      "Epoch[75/500], Train loss:  0.4016\n",
      "Epoch[75/500], Val loss:  0.4134\n",
      "0.8302122347066168\n",
      "Epoch[76/500], Train loss:  0.3989\n",
      "Epoch[76/500], Val loss:  0.4200\n",
      "0.8322555139409071\n",
      "Epoch[77/500], Train loss:  0.3963\n",
      "Epoch[77/500], Val loss:  0.4215\n",
      "0.8349937578027464\n",
      "Epoch[78/500], Train loss:  0.3959\n",
      "Epoch[78/500], Val loss:  0.4159\n",
      "0.8350436953807739\n",
      "Epoch[79/500], Train loss:  0.3999\n",
      "Epoch[79/500], Val loss:  0.4147\n",
      "0.8370079067831877\n",
      "Epoch[80/500], Train loss:  0.3991\n",
      "Epoch[80/500], Val loss:  0.4184\n",
      "0.8367374115688723\n",
      "Epoch[81/500], Train loss:  0.3974\n",
      "Epoch[81/500], Val loss:  0.4178\n",
      "0.8364794007490637\n",
      "Epoch[82/500], Train loss:  0.3975\n",
      "Epoch[82/500], Val loss:  0.4123\n",
      "0.838414481897628\n",
      "Epoch[83/500], Train loss:  0.3963\n",
      "Epoch[83/500], Val loss:  0.4125\n",
      "0.8351935081148564\n",
      "Epoch[84/500], Train loss:  0.3960\n",
      "Epoch[84/500], Val loss:  0.4178\n",
      "0.8402871410736579\n",
      "Epoch[85/500], Train loss:  0.3959\n",
      "Epoch[85/500], Val loss:  0.4127\n",
      "0.8417062005826051\n",
      "Epoch[86/500], Train loss:  0.3937\n",
      "Epoch[86/500], Val loss:  0.4126\n",
      "0.8356762380357886\n",
      "Epoch[87/500], Train loss:  0.3978\n",
      "Epoch[87/500], Val loss:  0.4115\n",
      "0.8390844777361632\n",
      "Epoch[88/500], Train loss:  0.3966\n",
      "Epoch[88/500], Val loss:  0.4163\n",
      "0.8412567623803578\n",
      "Epoch[89/500], Train loss:  0.3962\n",
      "Epoch[89/500], Val loss:  0.4166\n",
      "0.8360632542655015\n",
      "Epoch[90/500], Train loss:  0.3966\n",
      "Epoch[90/500], Val loss:  0.4160\n",
      "0.8312359550561799\n",
      "Epoch[91/500], Train loss:  0.3958\n",
      "Epoch[91/500], Val loss:  0.4174\n",
      "0.8292800665834374\n",
      "Epoch[92/500], Train loss:  0.3953\n",
      "Epoch[92/500], Val loss:  0.4143\n",
      "0.8442114024136496\n",
      "Epoch[93/500], Train loss:  0.3942\n",
      "Epoch[93/500], Val loss:  0.4182\n",
      "0.8422388680815647\n",
      "Epoch[94/500], Train loss:  0.3958\n",
      "Epoch[94/500], Val loss:  0.4156\n",
      "0.8382729920932168\n",
      "Epoch[95/500], Train loss:  0.3954\n",
      "Epoch[95/500], Val loss:  0.4132\n",
      "0.8340116521015398\n",
      "Epoch[96/500], Train loss:  0.3927\n",
      "Epoch[96/500], Val loss:  0.4094\n",
      "0.8435580524344569\n",
      "Epoch[97/500], Train loss:  0.3941\n",
      "Epoch[97/500], Val loss:  0.4098\n",
      "0.8447648772367873\n",
      "Epoch[98/500], Train loss:  0.3930\n",
      "Epoch[98/500], Val loss:  0.4073\n",
      "0.8406866416978777\n",
      "Epoch[99/500], Train loss:  0.3906\n",
      "Epoch[99/500], Val loss:  0.4205\n",
      "0.8333874323761965\n",
      "Epoch[100/500], Train loss:  0.3917\n",
      "Epoch[100/500], Val loss:  0.4236\n",
      "0.8221639617145236\n",
      "Epoch[101/500], Train loss:  0.3901\n",
      "Epoch[101/500], Val loss:  0.4080\n",
      "0.8447149396587599\n",
      "Epoch[102/500], Train loss:  0.3836\n",
      "Epoch[102/500], Val loss:  0.4061\n",
      "0.8404993757802746\n",
      "Epoch[103/500], Train loss:  0.3880\n",
      "Epoch[103/500], Val loss:  0.4115\n",
      "0.8451560549313358\n",
      "Epoch[104/500], Train loss:  0.3898\n",
      "Epoch[104/500], Val loss:  0.4121\n",
      "0.8347274240532667\n",
      "Epoch[105/500], Train loss:  0.3822\n",
      "Epoch[105/500], Val loss:  0.4074\n",
      "0.8436620890553475\n",
      "Epoch[106/500], Train loss:  0.3836\n",
      "Epoch[106/500], Val loss:  0.4058\n",
      "0.8452684144818975\n",
      "Epoch[107/500], Train loss:  0.3887\n",
      "Epoch[107/500], Val loss:  0.4079\n",
      "0.8391593841032043\n",
      "Epoch[108/500], Train loss:  0.3933\n",
      "Epoch[108/500], Val loss:  0.4084\n",
      "0.8503162713275072\n",
      "Epoch[109/500], Train loss:  0.3890\n",
      "Epoch[109/500], Val loss:  0.4089\n",
      "0.8441448189762797\n",
      "Epoch[110/500], Train loss:  0.3872\n",
      "Epoch[110/500], Val loss:  0.4114\n",
      "0.8471743653766126\n",
      "Epoch[111/500], Train loss:  0.3886\n",
      "Epoch[111/500], Val loss:  0.4104\n",
      "0.8473699542238868\n",
      "Epoch[112/500], Train loss:  0.3863\n",
      "Epoch[112/500], Val loss:  0.4080\n",
      "0.8427673741156887\n",
      "Epoch[113/500], Train loss:  0.3873\n",
      "Epoch[113/500], Val loss:  0.4092\n",
      "0.8497295047856845\n",
      "Epoch[114/500], Train loss:  0.3870\n",
      "Epoch[114/500], Val loss:  0.4076\n",
      "0.8492925509779441\n",
      "Epoch[115/500], Train loss:  0.3852\n",
      "Epoch[115/500], Val loss:  0.4117\n",
      "0.8469205160216396\n",
      "Epoch[116/500], Train loss:  0.3851\n",
      "Epoch[116/500], Val loss:  0.4080\n",
      "0.8473075322513524\n",
      "Epoch[117/500], Train loss:  0.3878\n",
      "Epoch[117/500], Val loss:  0.4105\n",
      "0.8524594257178527\n",
      "Epoch[118/500], Train loss:  0.3879\n",
      "Epoch[118/500], Val loss:  0.4043\n",
      "0.8510986267166043\n",
      "Epoch[119/500], Train loss:  0.3836\n",
      "Epoch[119/500], Val loss:  0.4053\n",
      "0.8485934248855598\n",
      "Epoch[120/500], Train loss:  0.3848\n",
      "Epoch[120/500], Val loss:  0.4093\n",
      "0.8483270911360798\n",
      "Epoch[121/500], Train loss:  0.3850\n",
      "Epoch[121/500], Val loss:  0.4053\n",
      "0.8505493133583022\n",
      "Epoch[122/500], Train loss:  0.3842\n",
      "Epoch[122/500], Val loss:  0.4061\n",
      "0.8489346650020807\n",
      "Epoch[123/500], Train loss:  0.3839\n",
      "Epoch[123/500], Val loss:  0.4052\n",
      "0.8484019975031212\n",
      "Epoch[124/500], Train loss:  0.3824\n",
      "Epoch[124/500], Val loss:  0.4089\n",
      "0.8455597170203911\n",
      "Epoch[125/500], Train loss:  0.3844\n",
      "Epoch[125/500], Val loss:  0.4066\n",
      "0.8498293799417396\n",
      "Epoch[126/500], Train loss:  0.3840\n",
      "Epoch[126/500], Val loss:  0.4058\n",
      "0.8514024136496046\n",
      "Epoch[127/500], Train loss:  0.3856\n",
      "Epoch[127/500], Val loss:  0.4056\n",
      "0.8447315855181023\n",
      "Epoch[128/500], Train loss:  0.3838\n",
      "Epoch[128/500], Val loss:  0.4040\n",
      "0.8500416146483563\n",
      "Epoch[129/500], Train loss:  0.3841\n",
      "Epoch[129/500], Val loss:  0.4072\n",
      "0.8518310445276738\n",
      "Epoch[130/500], Train loss:  0.3855\n",
      "Epoch[130/500], Val loss:  0.4066\n",
      "0.8460174781523097\n",
      "Epoch[131/500], Train loss:  0.3851\n",
      "Epoch[131/500], Val loss:  0.4052\n",
      "0.8507116104868914\n",
      "Epoch[132/500], Train loss:  0.3796\n",
      "Epoch[132/500], Val loss:  0.4062\n",
      "0.8534290470245526\n",
      "Epoch[133/500], Train loss:  0.3849\n",
      "Epoch[133/500], Val loss:  0.4079\n",
      "0.8462463587182688\n",
      "Epoch[134/500], Train loss:  0.3824\n",
      "Epoch[134/500], Val loss:  0.4025\n",
      "0.846708281315023\n",
      "Epoch[135/500], Train loss:  0.3817\n",
      "Epoch[135/500], Val loss:  0.4047\n",
      "0.8502455264253017\n",
      "Epoch[136/500], Train loss:  0.3824\n",
      "Epoch[136/500], Val loss:  0.4025\n",
      "0.8477403245942572\n",
      "Epoch[137/500], Train loss:  0.3825\n",
      "Epoch[137/500], Val loss:  0.4058\n",
      "0.8495214315439034\n",
      "Epoch[138/500], Train loss:  0.3823\n",
      "Epoch[138/500], Val loss:  0.4063\n",
      "0.8498293799417393\n",
      "Epoch[139/500], Train loss:  0.3817\n",
      "Epoch[139/500], Val loss:  0.4059\n",
      "0.8473803578859758\n",
      "Epoch[140/500], Train loss:  0.3877\n",
      "Epoch[140/500], Val loss:  0.4040\n",
      "0.8530961298377029\n",
      "Epoch[141/500], Train loss:  0.3863\n",
      "Epoch[141/500], Val loss:  0.4080\n",
      "0.8474365376612567\n",
      "Epoch[142/500], Train loss:  0.3829\n",
      "Epoch[142/500], Val loss:  0.4067\n",
      "0.8578193924261341\n",
      "Epoch[143/500], Train loss:  0.3814\n",
      "Epoch[143/500], Val loss:  0.4034\n",
      "0.8492925509779443\n",
      "Epoch[144/500], Train loss:  0.3789\n",
      "Epoch[144/500], Val loss:  0.4066\n",
      "0.8546317103620475\n",
      "Epoch[145/500], Train loss:  0.3802\n",
      "Epoch[145/500], Val loss:  0.4042\n",
      "0.852322097378277\n",
      "Epoch[146/500], Train loss:  0.3856\n",
      "Epoch[146/500], Val loss:  0.4054\n",
      "0.8557012068248024\n",
      "Epoch[147/500], Train loss:  0.3818\n",
      "Epoch[147/500], Val loss:  0.4052\n",
      "0.8478693300041615\n",
      "Epoch[148/500], Train loss:  0.3803\n",
      "Epoch[148/500], Val loss:  0.4022\n",
      "0.853699542238868\n",
      "Epoch[149/500], Train loss:  0.3815\n",
      "Epoch[149/500], Val loss:  0.4049\n",
      "0.8526383687057844\n",
      "Epoch[150/500], Train loss:  0.3842\n",
      "Epoch[150/500], Val loss:  0.4021\n",
      "0.8535205992509365\n",
      "Epoch[151/500], Train loss:  0.3785\n",
      "Epoch[151/500], Val loss:  0.4043\n",
      "0.8588098210570121\n",
      "Epoch[152/500], Train loss:  0.3764\n",
      "Epoch[152/500], Val loss:  0.4054\n",
      "0.8527007906783188\n",
      "Epoch[153/500], Train loss:  0.3773\n",
      "Epoch[153/500], Val loss:  0.4023\n",
      "0.8549479816895548\n",
      "Epoch[154/500], Train loss:  0.3818\n",
      "Epoch[154/500], Val loss:  0.4036\n",
      "0.8555139409071992\n",
      "Epoch[155/500], Train loss:  0.3755\n",
      "Epoch[155/500], Val loss:  0.4026\n",
      "0.8499625468164794\n",
      "Epoch[156/500], Train loss:  0.3819\n",
      "Epoch[156/500], Val loss:  0.4052\n",
      "0.8485226799833542\n",
      "Epoch[157/500], Train loss:  0.3792\n",
      "Epoch[157/500], Val loss:  0.4014\n",
      "0.8545235122763212\n",
      "Epoch[158/500], Train loss:  0.3769\n",
      "Epoch[158/500], Val loss:  0.4027\n",
      "0.8535164377861008\n",
      "Epoch[159/500], Train loss:  0.3779\n",
      "Epoch[159/500], Val loss:  0.4004\n",
      "0.8533541406575115\n",
      "Epoch[160/500], Train loss:  0.3756\n",
      "Epoch[160/500], Val loss:  0.4014\n",
      "0.8503620474406992\n",
      "Epoch[161/500], Train loss:  0.3778\n",
      "Epoch[161/500], Val loss:  0.4008\n",
      "0.8538451935081148\n",
      "Epoch[162/500], Train loss:  0.3779\n",
      "Epoch[162/500], Val loss:  0.4016\n",
      "0.854615064502705\n",
      "Epoch[163/500], Train loss:  0.3818\n",
      "Epoch[163/500], Val loss:  0.4023\n",
      "0.8547191011235955\n",
      "Epoch[164/500], Train loss:  0.3820\n",
      "Epoch[164/500], Val loss:  0.4084\n",
      "0.8452600915522264\n",
      "Epoch[165/500], Train loss:  0.3791\n",
      "Epoch[165/500], Val loss:  0.3989\n",
      "0.8542904702455264\n",
      "Epoch[166/500], Train loss:  0.3861\n",
      "Epoch[166/500], Val loss:  0.4063\n",
      "0.85749063670412\n",
      "Epoch[167/500], Train loss:  0.3759\n",
      "Epoch[167/500], Val loss:  0.4038\n",
      "0.8541573033707865\n",
      "Epoch[168/500], Train loss:  0.3793\n",
      "Epoch[168/500], Val loss:  0.4018\n",
      "0.852059925093633\n",
      "Epoch[169/500], Train loss:  0.3751\n",
      "Epoch[169/500], Val loss:  0.4005\n",
      "0.8545651269246775\n",
      "Epoch[170/500], Train loss:  0.3761\n",
      "Epoch[170/500], Val loss:  0.4030\n",
      "0.8555056179775281\n",
      "Epoch[171/500], Train loss:  0.3739\n",
      "Epoch[171/500], Val loss:  0.4033\n",
      "0.8550728256346234\n",
      "Epoch[172/500], Train loss:  0.3762\n",
      "Epoch[172/500], Val loss:  0.4018\n",
      "0.8556263004577611\n",
      "Epoch[173/500], Train loss:  0.3759\n",
      "Epoch[173/500], Val loss:  0.4023\n",
      "0.854119850187266\n",
      "Epoch[174/500], Train loss:  0.3765\n",
      "Epoch[174/500], Val loss:  0.4019\n",
      "0.8534623387432377\n",
      "Epoch[175/500], Train loss:  0.3773\n",
      "Epoch[175/500], Val loss:  0.4024\n",
      "0.8521889305035373\n",
      "Epoch[176/500], Train loss:  0.3750\n",
      "Epoch[176/500], Val loss:  0.4024\n",
      "0.8507449022055764\n",
      "Epoch[177/500], Train loss:  0.3766\n",
      "Epoch[177/500], Val loss:  0.3990\n",
      "0.8536329588014981\n",
      "Epoch[178/500], Train loss:  0.3752\n",
      "Epoch[178/500], Val loss:  0.3999\n",
      "0.8548314606741574\n",
      "Epoch[179/500], Train loss:  0.3790\n",
      "Epoch[179/500], Val loss:  0.4025\n",
      "0.8521306699958386\n",
      "Epoch[180/500], Train loss:  0.3761\n",
      "Epoch[180/500], Val loss:  0.4000\n",
      "0.853325010403662\n",
      "Epoch[181/500], Train loss:  0.3774\n",
      "Epoch[181/500], Val loss:  0.3999\n",
      "0.8527548897211819\n",
      "Epoch[182/500], Train loss:  0.3744\n",
      "Epoch[182/500], Val loss:  0.4023\n",
      "0.856067415730337\n",
      "Epoch[183/500], Train loss:  0.3736\n",
      "Epoch[183/500], Val loss:  0.4027\n",
      "0.8518352059925093\n",
      "Epoch[184/500], Train loss:  0.3769\n",
      "Epoch[184/500], Val loss:  0.3996\n",
      "0.8559425717852684\n",
      "Epoch[185/500], Train loss:  0.3747\n",
      "Epoch[185/500], Val loss:  0.4021\n",
      "0.8552267998335414\n",
      "Epoch[186/500], Train loss:  0.3763\n",
      "Epoch[186/500], Val loss:  0.4037\n",
      "0.8536329588014981\n",
      "Epoch[187/500], Train loss:  0.3770\n",
      "Epoch[187/500], Val loss:  0.4025\n",
      "0.8544569288389514\n",
      "Epoch[188/500], Train loss:  0.3778\n",
      "Epoch[188/500], Val loss:  0.4024\n",
      "0.8530420307948399\n",
      "Epoch[189/500], Train loss:  0.3792\n",
      "Epoch[189/500], Val loss:  0.4007\n",
      "0.859417394923013\n",
      "Epoch[190/500], Train loss:  0.3770\n",
      "Epoch[190/500], Val loss:  0.4044\n",
      "0.8538410320432792\n",
      "Epoch[191/500], Train loss:  0.3764\n",
      "Epoch[191/500], Val loss:  0.4008\n",
      "0.8530378693300041\n",
      "Epoch[192/500], Train loss:  0.3777\n",
      "Epoch[192/500], Val loss:  0.4002\n",
      "0.853712026633375\n",
      "Epoch[193/500], Train loss:  0.3761\n",
      "Epoch[193/500], Val loss:  0.4004\n",
      "0.8569579692051602\n",
      "Epoch[194/500], Train loss:  0.3788\n",
      "Epoch[194/500], Val loss:  0.4035\n",
      "0.8562796504369539\n",
      "Epoch[195/500], Train loss:  0.3782\n",
      "Epoch[195/500], Val loss:  0.4011\n",
      "0.8566208905534748\n",
      "Epoch[196/500], Train loss:  0.3749\n",
      "Epoch[196/500], Val loss:  0.4018\n",
      "0.854923012900541\n",
      "Epoch[197/500], Train loss:  0.3770\n",
      "Epoch[197/500], Val loss:  0.4007\n",
      "0.8517769454848108\n",
      "Epoch[198/500], Train loss:  0.3769\n",
      "Epoch[198/500], Val loss:  0.3991\n",
      "0.8574406991260924\n",
      "Epoch[199/500], Train loss:  0.3734\n",
      "Epoch[199/500], Val loss:  0.3989\n",
      "0.8594215563878485\n",
      "Epoch[200/500], Train loss:  0.3778\n",
      "Epoch[200/500], Val loss:  0.4066\n",
      "0.8529796088223054\n",
      "Epoch[201/500], Train loss:  0.3753\n",
      "Epoch[201/500], Val loss:  0.4015\n",
      "0.8562713275072826\n",
      "Epoch[202/500], Train loss:  0.3725\n",
      "Epoch[202/500], Val loss:  0.3998\n",
      "0.856641697877653\n",
      "Epoch[203/500], Train loss:  0.3768\n",
      "Epoch[203/500], Val loss:  0.4005\n",
      "0.8565834373699541\n",
      "Epoch[204/500], Train loss:  0.3740\n",
      "Epoch[204/500], Val loss:  0.4014\n",
      "0.8581689554723263\n",
      "Epoch[205/500], Train loss:  0.3750\n",
      "Epoch[205/500], Val loss:  0.3990\n",
      "0.8574739908447775\n",
      "Epoch[206/500], Train loss:  0.3732\n",
      "Epoch[206/500], Val loss:  0.3980\n",
      "0.8565459841864336\n",
      "Epoch[207/500], Train loss:  0.3778\n",
      "Epoch[207/500], Val loss:  0.3999\n",
      "0.8587973366625052\n",
      "Epoch[208/500], Train loss:  0.3764\n",
      "Epoch[208/500], Val loss:  0.4047\n",
      "0.853699542238868\n",
      "Epoch[209/500], Train loss:  0.3768\n",
      "Epoch[209/500], Val loss:  0.4012\n",
      "0.8520391177694548\n",
      "Epoch[210/500], Train loss:  0.3747\n",
      "Epoch[210/500], Val loss:  0.4017\n",
      "0.8581647940074907\n",
      "Epoch[211/500], Train loss:  0.3765\n",
      "Epoch[211/500], Val loss:  0.4010\n",
      "0.8573949230129005\n",
      "Epoch[212/500], Train loss:  0.3750\n",
      "Epoch[212/500], Val loss:  0.4008\n",
      "0.8572950478568456\n",
      "Epoch[213/500], Train loss:  0.3742\n",
      "Epoch[213/500], Val loss:  0.4023\n",
      "0.8552725759467332\n",
      "Epoch[214/500], Train loss:  0.3717\n",
      "Epoch[214/500], Val loss:  0.4008\n",
      "0.8586433624635873\n",
      "Epoch[215/500], Train loss:  0.3731\n",
      "Epoch[215/500], Val loss:  0.3982\n",
      "0.856695796920516\n",
      "Epoch[216/500], Train loss:  0.3755\n",
      "Epoch[216/500], Val loss:  0.3995\n",
      "0.8613358302122347\n",
      "Epoch[217/500], Train loss:  0.3743\n",
      "Epoch[217/500], Val loss:  0.4002\n",
      "0.856367041198502\n",
      "Epoch[218/500], Train loss:  0.3762\n",
      "Epoch[218/500], Val loss:  0.3992\n",
      "0.8549105285060341\n",
      "Epoch[219/500], Train loss:  0.3728\n",
      "Epoch[219/500], Val loss:  0.4020\n",
      "0.8531002913025385\n",
      "Epoch[220/500], Train loss:  0.3771\n",
      "Epoch[220/500], Val loss:  0.3980\n",
      "0.8561714523512276\n",
      "Epoch[221/500], Train loss:  0.3754\n",
      "Epoch[221/500], Val loss:  0.3998\n",
      "0.8598959633791095\n",
      "Epoch[222/500], Train loss:  0.3769\n",
      "Epoch[222/500], Val loss:  0.3977\n",
      "0.8579442363712025\n",
      "Epoch[223/500], Train loss:  0.3803\n",
      "Epoch[223/500], Val loss:  0.4025\n",
      "0.8542363712026634\n",
      "Epoch[224/500], Train loss:  0.3723\n",
      "Epoch[224/500], Val loss:  0.3991\n",
      "0.8572034956304619\n",
      "Epoch[225/500], Train loss:  0.3724\n",
      "Epoch[225/500], Val loss:  0.3984\n",
      "0.8587307532251351\n",
      "Epoch[226/500], Train loss:  0.3790\n",
      "Epoch[226/500], Val loss:  0.3998\n",
      "0.8566375364128174\n",
      "Epoch[227/500], Train loss:  0.3714\n",
      "Epoch[227/500], Val loss:  0.3989\n",
      "0.8578443612151478\n",
      "Epoch[228/500], Train loss:  0.3762\n",
      "Epoch[228/500], Val loss:  0.3983\n",
      "0.8571785268414482\n",
      "Epoch[229/500], Train loss:  0.3722\n",
      "Epoch[229/500], Val loss:  0.4002\n",
      "0.8583895131086142\n",
      "Epoch[230/500], Train loss:  0.3749\n",
      "Epoch[230/500], Val loss:  0.3988\n",
      "0.8627632126508531\n",
      "Epoch[231/500], Train loss:  0.3749\n",
      "Epoch[231/500], Val loss:  0.3990\n",
      "0.8613940907199333\n",
      "Epoch[232/500], Train loss:  0.3735\n",
      "Epoch[232/500], Val loss:  0.4001\n",
      "0.8558510195588847\n",
      "Epoch[233/500], Train loss:  0.3728\n",
      "Epoch[233/500], Val loss:  0.4009\n",
      "0.8577985851019558\n",
      "Epoch[234/500], Train loss:  0.3756\n",
      "Epoch[234/500], Val loss:  0.3988\n",
      "0.858722430295464\n",
      "Epoch[235/500], Train loss:  0.3739\n",
      "Epoch[235/500], Val loss:  0.3995\n",
      "0.858734914689971\n",
      "Epoch[236/500], Train loss:  0.3741\n",
      "Epoch[236/500], Val loss:  0.4022\n",
      "0.8570620058260509\n",
      "Epoch[237/500], Train loss:  0.3722\n",
      "Epoch[237/500], Val loss:  0.3988\n",
      "0.8597045359966708\n",
      "Epoch[238/500], Train loss:  0.3712\n",
      "Epoch[238/500], Val loss:  0.4017\n",
      "0.8597461506450269\n",
      "Epoch[239/500], Train loss:  0.3734\n",
      "Epoch[239/500], Val loss:  0.3999\n",
      "0.8618393674573452\n",
      "Epoch[240/500], Train loss:  0.3743\n",
      "Epoch[240/500], Val loss:  0.3985\n",
      "0.859471493965876\n",
      "Epoch[241/500], Train loss:  0.3769\n",
      "Epoch[241/500], Val loss:  0.3995\n",
      "0.8654057428214734\n",
      "Epoch[242/500], Train loss:  0.3697\n",
      "Epoch[242/500], Val loss:  0.4003\n",
      "0.8589513108614233\n",
      "Epoch[243/500], Train loss:  0.3748\n",
      "Epoch[243/500], Val loss:  0.3985\n",
      "0.8611444028297961\n",
      "Epoch[244/500], Train loss:  0.3669\n",
      "Epoch[244/500], Val loss:  0.3975\n",
      "0.858073241781107\n",
      "Epoch[245/500], Train loss:  0.3765\n",
      "Epoch[245/500], Val loss:  0.3983\n",
      "0.8616146483562215\n",
      "Epoch[246/500], Train loss:  0.3715\n",
      "Epoch[246/500], Val loss:  0.4013\n",
      "0.8561548064918851\n",
      "Epoch[247/500], Train loss:  0.3704\n",
      "Epoch[247/500], Val loss:  0.3982\n",
      "0.8616562630045776\n",
      "Epoch[248/500], Train loss:  0.3705\n",
      "Epoch[248/500], Val loss:  0.3998\n",
      "0.8586433624635872\n",
      "Epoch[249/500], Train loss:  0.3743\n",
      "Epoch[249/500], Val loss:  0.4006\n",
      "0.8585934248855597\n",
      "Epoch[250/500], Train loss:  0.3748\n",
      "Epoch[250/500], Val loss:  0.3975\n",
      "0.8629671244277987\n",
      "Epoch[251/500], Train loss:  0.3718\n",
      "Epoch[251/500], Val loss:  0.3986\n",
      "0.8575738660008323\n",
      "Epoch[252/500], Train loss:  0.3744\n",
      "Epoch[252/500], Val loss:  0.3965\n",
      "0.8590595089471494\n",
      "Epoch[253/500], Train loss:  0.3732\n",
      "Epoch[253/500], Val loss:  0.3969\n",
      "0.8592759051186016\n",
      "Epoch[254/500], Train loss:  0.3749\n",
      "Epoch[254/500], Val loss:  0.4002\n",
      "0.8525010403662088\n",
      "Epoch[255/500], Train loss:  0.3742\n",
      "Epoch[255/500], Val loss:  0.4022\n",
      "0.8554348730753225\n",
      "Epoch[256/500], Train loss:  0.3698\n",
      "Epoch[256/500], Val loss:  0.3992\n",
      "0.8593674573449854\n",
      "Epoch[257/500], Train loss:  0.3712\n",
      "Epoch[257/500], Val loss:  0.4006\n",
      "0.8538784852267999\n",
      "Epoch[258/500], Train loss:  0.3739\n",
      "Epoch[258/500], Val loss:  0.3970\n",
      "0.8588139825218477\n",
      "Epoch[259/500], Train loss:  0.3672\n",
      "Epoch[259/500], Val loss:  0.3966\n",
      "0.8569704535996671\n",
      "Epoch[260/500], Train loss:  0.3705\n",
      "Epoch[260/500], Val loss:  0.4003\n",
      "0.8586933000416146\n",
      "Epoch[261/500], Train loss:  0.3717\n",
      "Epoch[261/500], Val loss:  0.3988\n",
      "0.8585726175613815\n",
      "Epoch[262/500], Train loss:  0.3728\n",
      "Epoch[262/500], Val loss:  0.4007\n",
      "0.8547107781939243\n",
      "Epoch[263/500], Train loss:  0.3743\n",
      "Epoch[263/500], Val loss:  0.3994\n",
      "0.864119850187266\n",
      "Epoch[264/500], Train loss:  0.3704\n",
      "Epoch[264/500], Val loss:  0.3967\n",
      "0.8594631710362046\n",
      "Epoch[265/500], Train loss:  0.3723\n",
      "Epoch[265/500], Val loss:  0.3997\n",
      "0.8606283811901789\n",
      "Epoch[266/500], Train loss:  0.3752\n",
      "Epoch[266/500], Val loss:  0.3963\n",
      "0.862883895131086\n",
      "Epoch[267/500], Train loss:  0.3739\n",
      "Epoch[267/500], Val loss:  0.3996\n",
      "0.859571369121931\n",
      "Epoch[268/500], Train loss:  0.3716\n",
      "Epoch[268/500], Val loss:  0.3973\n",
      "0.8590636704119851\n",
      "Epoch[269/500], Train loss:  0.3713\n",
      "Epoch[269/500], Val loss:  0.3996\n",
      "0.8587973366625052\n",
      "Epoch[270/500], Train loss:  0.3697\n",
      "Epoch[270/500], Val loss:  0.3991\n",
      "0.8582313774448607\n",
      "Epoch[271/500], Train loss:  0.3710\n",
      "Epoch[271/500], Val loss:  0.3977\n",
      "0.8629879317519767\n",
      "Epoch[272/500], Train loss:  0.3769\n",
      "Epoch[272/500], Val loss:  0.3967\n",
      "0.8629796088223054\n",
      "Epoch[273/500], Train loss:  0.3702\n",
      "Epoch[273/500], Val loss:  0.3983\n",
      "0.8602580108198086\n",
      "Epoch[274/500], Train loss:  0.3719\n",
      "Epoch[274/500], Val loss:  0.3961\n",
      "0.8600915522263837\n",
      "Epoch[275/500], Train loss:  0.3722\n",
      "Epoch[275/500], Val loss:  0.3977\n",
      "0.857973366625052\n",
      "Epoch[276/500], Train loss:  0.3731\n",
      "Epoch[276/500], Val loss:  0.3998\n",
      "0.8657303370786518\n",
      "Epoch[277/500], Train loss:  0.3745\n",
      "Epoch[277/500], Val loss:  0.3957\n",
      "0.8590928006658344\n",
      "Epoch[278/500], Train loss:  0.3725\n",
      "Epoch[278/500], Val loss:  0.4051\n",
      "0.8590220557636288\n",
      "Epoch[279/500], Train loss:  0.3723\n",
      "Epoch[279/500], Val loss:  0.3965\n",
      "0.8595672076570953\n",
      "Epoch[280/500], Train loss:  0.3736\n",
      "Epoch[280/500], Val loss:  0.3958\n",
      "0.8575655430711611\n",
      "Epoch[281/500], Train loss:  0.3703\n",
      "Epoch[281/500], Val loss:  0.3972\n",
      "0.8614398668331253\n",
      "Epoch[282/500], Train loss:  0.3745\n",
      "Epoch[282/500], Val loss:  0.4025\n",
      "0.8597877652933832\n",
      "Epoch[283/500], Train loss:  0.3693\n",
      "Epoch[283/500], Val loss:  0.3983\n",
      "0.8607740324594257\n",
      "Epoch[284/500], Train loss:  0.3742\n",
      "Epoch[284/500], Val loss:  0.3993\n",
      "0.8619184352892217\n",
      "Epoch[285/500], Train loss:  0.3703\n",
      "Epoch[285/500], Val loss:  0.3957\n",
      "0.8613191843528922\n",
      "Epoch[286/500], Train loss:  0.3720\n",
      "Epoch[286/500], Val loss:  0.3972\n",
      "0.8601706200582605\n",
      "Epoch[287/500], Train loss:  0.3708\n",
      "Epoch[287/500], Val loss:  0.4034\n",
      "0.8561548064918851\n",
      "Epoch[288/500], Train loss:  0.3744\n",
      "Epoch[288/500], Val loss:  0.3985\n",
      "0.8582896379525593\n",
      "Epoch[289/500], Train loss:  0.3751\n",
      "Epoch[289/500], Val loss:  0.4015\n",
      "0.8577902621722846\n",
      "Epoch[290/500], Train loss:  0.3708\n",
      "Epoch[290/500], Val loss:  0.3984\n",
      "0.8569121930919684\n",
      "Epoch[291/500], Train loss:  0.3720\n",
      "Epoch[291/500], Val loss:  0.3972\n",
      "0.8603495630461923\n",
      "Epoch[292/500], Train loss:  0.3722\n",
      "Epoch[292/500], Val loss:  0.3969\n",
      "0.8624843945068664\n",
      "Epoch[293/500], Train loss:  0.3712\n",
      "Epoch[293/500], Val loss:  0.3977\n",
      "0.859130253849355\n",
      "Epoch[294/500], Train loss:  0.3696\n",
      "Epoch[294/500], Val loss:  0.3969\n",
      "0.8582022471910113\n",
      "Epoch[295/500], Train loss:  0.3757\n",
      "Epoch[295/500], Val loss:  0.4004\n",
      "0.8550145651269248\n",
      "Epoch[296/500], Train loss:  0.3725\n",
      "Epoch[296/500], Val loss:  0.3969\n",
      "0.8590969621306699\n",
      "Epoch[297/500], Train loss:  0.3722\n",
      "Epoch[297/500], Val loss:  0.3979\n",
      "0.8613691219309196\n",
      "Epoch[298/500], Train loss:  0.3685\n",
      "Epoch[298/500], Val loss:  0.3967\n",
      "0.8618684977111943\n",
      "Epoch[299/500], Train loss:  0.3697\n",
      "Epoch[299/500], Val loss:  0.3977\n",
      "0.8566500208073241\n",
      "Epoch[300/500], Train loss:  0.3748\n",
      "Epoch[300/500], Val loss:  0.3980\n",
      "0.8617769454848108\n",
      "Epoch[301/500], Train loss:  0.3708\n",
      "Epoch[301/500], Val loss:  0.3966\n",
      "0.8603079483978362\n",
      "Epoch[302/500], Train loss:  0.3704\n",
      "Epoch[302/500], Val loss:  0.3977\n",
      "0.8587557220141491\n",
      "Epoch[303/500], Train loss:  0.3725\n",
      "Epoch[303/500], Val loss:  0.3983\n",
      "0.8607074490220558\n",
      "Epoch[304/500], Train loss:  0.3739\n",
      "Epoch[304/500], Val loss:  0.3974\n",
      "0.8624760715771952\n",
      "Epoch[305/500], Train loss:  0.3735\n",
      "Epoch[305/500], Val loss:  0.3964\n",
      "0.8595089471493966\n",
      "Epoch[306/500], Train loss:  0.3758\n",
      "Epoch[306/500], Val loss:  0.3969\n",
      "0.8662671660424469\n",
      "Epoch[307/500], Train loss:  0.3696\n",
      "Epoch[307/500], Val loss:  0.3958\n",
      "0.8597295047856846\n",
      "Epoch[308/500], Train loss:  0.3767\n",
      "Epoch[308/500], Val loss:  0.3962\n",
      "0.8624261339991677\n",
      "Epoch[309/500], Train loss:  0.3703\n",
      "Epoch[309/500], Val loss:  0.4004\n",
      "0.859658759883479\n",
      "Epoch[310/500], Train loss:  0.3699\n",
      "Epoch[310/500], Val loss:  0.3966\n",
      "0.8623845193508115\n",
      "Epoch[311/500], Train loss:  0.3692\n",
      "Epoch[311/500], Val loss:  0.3995\n",
      "0.8597711194340407\n",
      "Epoch[312/500], Train loss:  0.3700\n",
      "Epoch[312/500], Val loss:  0.3975\n",
      "0.8595464003329172\n",
      "Epoch[313/500], Train loss:  0.3706\n",
      "Epoch[313/500], Val loss:  0.3955\n",
      "0.8581398252184769\n",
      "Epoch[314/500], Train loss:  0.3717\n",
      "Epoch[314/500], Val loss:  0.3961\n",
      "0.8591843528922181\n",
      "Epoch[315/500], Train loss:  0.3692\n",
      "Epoch[315/500], Val loss:  0.3987\n",
      "0.8583853516437787\n",
      "Epoch[316/500], Train loss:  0.3701\n",
      "Epoch[316/500], Val loss:  0.3985\n",
      "0.8602288805659591\n",
      "Epoch[317/500], Train loss:  0.3721\n",
      "Epoch[317/500], Val loss:  0.3974\n",
      "0.8604744069912609\n",
      "Epoch[318/500], Train loss:  0.3694\n",
      "Epoch[318/500], Val loss:  0.4004\n",
      "0.8600083229296712\n",
      "Epoch[319/500], Train loss:  0.3695\n",
      "Epoch[319/500], Val loss:  0.4000\n",
      "0.8581731169371618\n",
      "Epoch[320/500], Train loss:  0.3729\n",
      "Epoch[320/500], Val loss:  0.3976\n",
      "0.8598959633791096\n",
      "Epoch[321/500], Train loss:  0.3708\n",
      "Epoch[321/500], Val loss:  0.3975\n",
      "0.856221389929255\n",
      "Epoch[322/500], Train loss:  0.3741\n",
      "Epoch[322/500], Val loss:  0.3959\n",
      "0.8619101123595506\n",
      "Epoch[323/500], Train loss:  0.3689\n",
      "Epoch[323/500], Val loss:  0.3970\n",
      "0.8606367041198503\n",
      "Epoch[324/500], Train loss:  0.3710\n",
      "Epoch[324/500], Val loss:  0.3992\n",
      "0.8582230545151894\n",
      "Epoch[325/500], Train loss:  0.3675\n",
      "Epoch[325/500], Val loss:  0.3985\n",
      "0.8584311277569705\n",
      "Epoch[326/500], Train loss:  0.3725\n",
      "Epoch[326/500], Val loss:  0.3982\n",
      "0.8576446109030378\n",
      "Epoch[327/500], Train loss:  0.3696\n",
      "Epoch[327/500], Val loss:  0.3946\n",
      "0.8603828547648773\n",
      "Epoch[328/500], Train loss:  0.3705\n",
      "Epoch[328/500], Val loss:  0.3983\n",
      "0.8573200166458594\n",
      "Epoch[329/500], Train loss:  0.3728\n",
      "Epoch[329/500], Val loss:  0.3972\n",
      "0.864207240948814\n",
      "Epoch[330/500], Train loss:  0.3730\n",
      "Epoch[330/500], Val loss:  0.3954\n",
      "0.8604952143154392\n",
      "Epoch[331/500], Train loss:  0.3740\n",
      "Epoch[331/500], Val loss:  0.3993\n",
      "0.8616729088639201\n",
      "Epoch[332/500], Train loss:  0.3709\n",
      "Epoch[332/500], Val loss:  0.3975\n",
      "0.8627715355805243\n",
      "Epoch[333/500], Train loss:  0.3693\n",
      "Epoch[333/500], Val loss:  0.3959\n",
      "0.8602663337494799\n",
      "Epoch[334/500], Train loss:  0.3723\n",
      "Epoch[334/500], Val loss:  0.3981\n",
      "0.8583936745734498\n",
      "Epoch[335/500], Train loss:  0.3704\n",
      "Epoch[335/500], Val loss:  0.3970\n",
      "0.8613399916770703\n",
      "Epoch[336/500], Train loss:  0.3705\n",
      "Epoch[336/500], Val loss:  0.3971\n",
      "0.863116937161881\n",
      "Epoch[337/500], Train loss:  0.3718\n",
      "Epoch[337/500], Val loss:  0.3990\n",
      "0.8610153974198917\n",
      "Epoch[338/500], Train loss:  0.3716\n",
      "Epoch[338/500], Val loss:  0.4024\n",
      "0.852829796088223\n",
      "Epoch[339/500], Train loss:  0.3709\n",
      "Epoch[339/500], Val loss:  0.3967\n",
      "0.8642571785268415\n",
      "Epoch[340/500], Train loss:  0.3667\n",
      "Epoch[340/500], Val loss:  0.3982\n",
      "0.8613857677902622\n",
      "Epoch[341/500], Train loss:  0.3688\n",
      "Epoch[341/500], Val loss:  0.3965\n",
      "0.8571993341656262\n",
      "Epoch[342/500], Train loss:  0.3703\n",
      "Epoch[342/500], Val loss:  0.3965\n",
      "0.8665043695380773\n",
      "Epoch[343/500], Train loss:  0.3715\n",
      "Epoch[343/500], Val loss:  0.3969\n",
      "0.8596379525593009\n",
      "Epoch[344/500], Train loss:  0.3737\n",
      "Epoch[344/500], Val loss:  0.3988\n",
      "0.8627174365376613\n",
      "Epoch[345/500], Train loss:  0.3724\n",
      "Epoch[345/500], Val loss:  0.3998\n",
      "0.8589929255097795\n",
      "Epoch[346/500], Train loss:  0.3690\n",
      "Epoch[346/500], Val loss:  0.3990\n",
      "0.8590095713691218\n",
      "Epoch[347/500], Train loss:  0.3705\n",
      "Epoch[347/500], Val loss:  0.3942\n",
      "0.8587723678734915\n",
      "Epoch[348/500], Train loss:  0.3712\n",
      "Epoch[348/500], Val loss:  0.3962\n",
      "0.8600416146483563\n",
      "Epoch[349/500], Train loss:  0.3683\n",
      "Epoch[349/500], Val loss:  0.3992\n",
      "0.8620890553474823\n",
      "Epoch[350/500], Train loss:  0.3702\n",
      "Epoch[350/500], Val loss:  0.3975\n",
      "0.8599583853516438\n",
      "Epoch[351/500], Train loss:  0.3695\n",
      "Epoch[351/500], Val loss:  0.3972\n",
      "0.8632667498959634\n",
      "Epoch[352/500], Train loss:  0.3712\n",
      "Epoch[352/500], Val loss:  0.3961\n",
      "0.8610944652517687\n",
      "Epoch[353/500], Train loss:  0.3692\n",
      "Epoch[353/500], Val loss:  0.3995\n",
      "0.8561007074490221\n",
      "Epoch[354/500], Train loss:  0.3714\n",
      "Epoch[354/500], Val loss:  0.3961\n",
      "0.8619017894298793\n",
      "Epoch[355/500], Train loss:  0.3703\n",
      "Epoch[355/500], Val loss:  0.3957\n",
      "0.8606949646275489\n",
      "Epoch[356/500], Train loss:  0.3689\n",
      "Epoch[356/500], Val loss:  0.3955\n",
      "0.8630545151893466\n",
      "Epoch[357/500], Train loss:  0.3742\n",
      "Epoch[357/500], Val loss:  0.3964\n",
      "0.8600291302538495\n",
      "Epoch[358/500], Train loss:  0.3715\n",
      "Epoch[358/500], Val loss:  0.3961\n",
      "0.8577652933832709\n",
      "Epoch[359/500], Train loss:  0.3704\n",
      "Epoch[359/500], Val loss:  0.3948\n",
      "0.8619808572617562\n",
      "Epoch[360/500], Train loss:  0.3740\n",
      "Epoch[360/500], Val loss:  0.3955\n",
      "0.8602330420307949\n",
      "Epoch[361/500], Train loss:  0.3737\n",
      "Epoch[361/500], Val loss:  0.3982\n",
      "0.8646982937994174\n",
      "Epoch[362/500], Train loss:  0.3694\n",
      "Epoch[362/500], Val loss:  0.3963\n",
      "0.8633541406575115\n",
      "Epoch[363/500], Train loss:  0.3696\n",
      "Epoch[363/500], Val loss:  0.3969\n",
      "0.8636787349146902\n",
      "Epoch[364/500], Train loss:  0.3705\n",
      "Epoch[364/500], Val loss:  0.3959\n",
      "0.8647773616312942\n",
      "Epoch[365/500], Train loss:  0.3712\n",
      "Epoch[365/500], Val loss:  0.3966\n",
      "0.8611735330836454\n",
      "Epoch[366/500], Train loss:  0.3723\n",
      "Epoch[366/500], Val loss:  0.3967\n",
      "0.8600873907615483\n",
      "Epoch[367/500], Train loss:  0.3706\n",
      "Epoch[367/500], Val loss:  0.3970\n",
      "0.8573325010403662\n",
      "Epoch[368/500], Train loss:  0.3751\n",
      "Epoch[368/500], Val loss:  0.3957\n",
      "0.8585934248855597\n",
      "Epoch[369/500], Train loss:  0.3683\n",
      "Epoch[369/500], Val loss:  0.3980\n",
      "0.8598335414065751\n",
      "Epoch[370/500], Train loss:  0.3741\n",
      "Epoch[370/500], Val loss:  0.3999\n",
      "0.8605201831044527\n",
      "Epoch[371/500], Train loss:  0.3694\n",
      "Epoch[371/500], Val loss:  0.4006\n",
      "0.8595255930087392\n",
      "Epoch[372/500], Train loss:  0.3710\n",
      "Epoch[372/500], Val loss:  0.3984\n",
      "0.8577694548481065\n",
      "Epoch[373/500], Train loss:  0.3706\n",
      "Epoch[373/500], Val loss:  0.3978\n",
      "0.8639783603828548\n",
      "Epoch[374/500], Train loss:  0.3676\n",
      "Epoch[374/500], Val loss:  0.3948\n",
      "0.860649188514357\n",
      "Epoch[375/500], Train loss:  0.3710\n",
      "Epoch[375/500], Val loss:  0.3992\n",
      "0.8582521847690388\n",
      "Epoch[376/500], Train loss:  0.3703\n",
      "Epoch[376/500], Val loss:  0.3962\n",
      "0.8623970037453185\n",
      "Epoch[377/500], Train loss:  0.3744\n",
      "Epoch[377/500], Val loss:  0.3979\n",
      "0.8650436953807741\n",
      "Epoch[378/500], Train loss:  0.3713\n",
      "Epoch[378/500], Val loss:  0.3982\n",
      "0.8610195588847275\n",
      "Epoch[379/500], Train loss:  0.3695\n",
      "Epoch[379/500], Val loss:  0.3962\n",
      "0.8638243861839366\n",
      "Epoch[380/500], Train loss:  0.3730\n",
      "Epoch[380/500], Val loss:  0.3947\n",
      "0.858635039533916\n",
      "Epoch[381/500], Train loss:  0.3743\n",
      "Epoch[381/500], Val loss:  0.3949\n",
      "0.8628464419475655\n",
      "Epoch[382/500], Train loss:  0.3733\n",
      "Epoch[382/500], Val loss:  0.3944\n",
      "0.858668331252601\n",
      "Epoch[383/500], Train loss:  0.3702\n",
      "Epoch[383/500], Val loss:  0.3948\n",
      "0.8598252184769037\n",
      "Epoch[384/500], Train loss:  0.3728\n",
      "Epoch[384/500], Val loss:  0.3978\n",
      "0.8581939242613399\n",
      "Epoch[385/500], Train loss:  0.3691\n",
      "Epoch[385/500], Val loss:  0.3983\n",
      "0.8600249687890138\n",
      "Epoch[386/500], Train loss:  0.3714\n",
      "Epoch[386/500], Val loss:  0.3988\n",
      "0.8632625884311278\n",
      "Epoch[387/500], Train loss:  0.3715\n",
      "Epoch[387/500], Val loss:  0.3989\n",
      "0.8573533083645443\n",
      "Epoch[388/500], Train loss:  0.3717\n",
      "Epoch[388/500], Val loss:  0.4000\n",
      "0.8612692467748647\n",
      "Epoch[389/500], Train loss:  0.3725\n",
      "Epoch[389/500], Val loss:  0.3977\n",
      "0.8640532667498959\n",
      "Epoch[390/500], Train loss:  0.3709\n",
      "Epoch[390/500], Val loss:  0.3984\n",
      "0.8601040366208905\n",
      "Epoch[391/500], Train loss:  0.3722\n",
      "Epoch[391/500], Val loss:  0.3955\n",
      "0.8624511027881814\n",
      "Epoch[392/500], Train loss:  0.3709\n",
      "Epoch[392/500], Val loss:  0.3974\n",
      "0.8590054099042864\n",
      "Epoch[393/500], Train loss:  0.3718\n",
      "Epoch[393/500], Val loss:  0.3974\n",
      "0.8628714107365792\n",
      "Epoch[394/500], Train loss:  0.3677\n",
      "Epoch[394/500], Val loss:  0.3997\n",
      "0.8600749063670412\n",
      "Epoch[395/500], Train loss:  0.3724\n",
      "Epoch[395/500], Val loss:  0.3969\n",
      "0.8573574698293799\n",
      "Epoch[396/500], Train loss:  0.3707\n",
      "Epoch[396/500], Val loss:  0.3981\n",
      "0.8588014981273409\n",
      "Epoch[397/500], Train loss:  0.3720\n",
      "Epoch[397/500], Val loss:  0.4005\n",
      "0.8594049105285061\n",
      "Epoch[398/500], Train loss:  0.3718\n",
      "Epoch[398/500], Val loss:  0.3976\n",
      "0.8617145235122763\n",
      "Epoch[399/500], Train loss:  0.3733\n",
      "Epoch[399/500], Val loss:  0.3970\n",
      "0.8615106117353308\n",
      "Epoch[400/500], Train loss:  0.3682\n",
      "Epoch[400/500], Val loss:  0.3957\n",
      "0.862467748647524\n",
      "Epoch[401/500], Train loss:  0.3695\n",
      "Epoch[401/500], Val loss:  0.3966\n",
      "0.8581190178942988\n",
      "Epoch[402/500], Train loss:  0.3739\n",
      "Epoch[402/500], Val loss:  0.3967\n",
      "0.85812734082397\n",
      "Epoch[403/500], Train loss:  0.3726\n",
      "Epoch[403/500], Val loss:  0.3978\n",
      "0.8600540990428631\n",
      "Epoch[404/500], Train loss:  0.3718\n",
      "Epoch[404/500], Val loss:  0.3970\n",
      "0.8592051602163963\n",
      "Epoch[405/500], Train loss:  0.3723\n",
      "Epoch[405/500], Val loss:  0.3942\n",
      "0.8611360799001249\n",
      "Epoch[406/500], Train loss:  0.3699\n",
      "Epoch[406/500], Val loss:  0.3985\n",
      "0.8610736579275906\n",
      "Epoch[407/500], Train loss:  0.3709\n",
      "Epoch[407/500], Val loss:  0.3981\n",
      "0.8601206824802332\n",
      "Epoch[408/500], Train loss:  0.3723\n",
      "Epoch[408/500], Val loss:  0.4011\n",
      "0.8553932584269663\n",
      "Epoch[409/500], Train loss:  0.3665\n",
      "Epoch[409/500], Val loss:  0.3984\n",
      "0.857973366625052\n",
      "Epoch[410/500], Train loss:  0.3725\n",
      "Epoch[410/500], Val loss:  0.3977\n",
      "0.8614398668331252\n",
      "Epoch[411/500], Train loss:  0.3685\n",
      "Epoch[411/500], Val loss:  0.3955\n",
      "0.8588639200998751\n",
      "Epoch[412/500], Train loss:  0.3700\n",
      "Epoch[412/500], Val loss:  0.3944\n",
      "0.8618393674573451\n",
      "Epoch[413/500], Train loss:  0.3715\n",
      "Epoch[413/500], Val loss:  0.3959\n",
      "0.8618310445276738\n",
      "Epoch[414/500], Train loss:  0.3727\n",
      "Epoch[414/500], Val loss:  0.3963\n",
      "0.8617519766957968\n",
      "Epoch[415/500], Train loss:  0.3717\n",
      "Epoch[415/500], Val loss:  0.3983\n",
      "0.8631585518102373\n",
      "Epoch[416/500], Train loss:  0.3704\n",
      "Epoch[416/500], Val loss:  0.3960\n",
      "0.8613524760715773\n",
      "Epoch[417/500], Train loss:  0.3710\n",
      "Epoch[417/500], Val loss:  0.3995\n",
      "0.8585018726591761\n",
      "Epoch[418/500], Train loss:  0.3705\n",
      "Epoch[418/500], Val loss:  0.3989\n",
      "0.8624511027881814\n",
      "Epoch[419/500], Train loss:  0.3714\n",
      "Epoch[419/500], Val loss:  0.3990\n",
      "0.8603578859758637\n",
      "Epoch[420/500], Train loss:  0.3671\n",
      "Epoch[420/500], Val loss:  0.3955\n",
      "0.860495214315439\n",
      "Epoch[421/500], Train loss:  0.3719\n",
      "Epoch[421/500], Val loss:  0.3971\n",
      "0.8621972534332085\n",
      "Epoch[422/500], Train loss:  0.3699\n",
      "Epoch[422/500], Val loss:  0.3980\n",
      "0.8611485642946317\n",
      "Epoch[423/500], Train loss:  0.3673\n",
      "Epoch[423/500], Val loss:  0.3969\n",
      "0.8610486891385769\n",
      "Epoch[424/500], Train loss:  0.3711\n",
      "Epoch[424/500], Val loss:  0.3967\n",
      "0.8662255513940907\n",
      "Epoch[425/500], Train loss:  0.3710\n",
      "Epoch[425/500], Val loss:  0.3957\n",
      "0.8625759467332501\n",
      "Epoch[426/500], Train loss:  0.3703\n",
      "Epoch[426/500], Val loss:  0.3974\n",
      "0.8570287141073658\n",
      "Epoch[427/500], Train loss:  0.3690\n",
      "Epoch[427/500], Val loss:  0.3950\n",
      "0.8605909280066584\n",
      "Epoch[428/500], Train loss:  0.3697\n",
      "Epoch[428/500], Val loss:  0.3962\n",
      "0.862213899292551\n",
      "Epoch[429/500], Train loss:  0.3672\n",
      "Epoch[429/500], Val loss:  0.3986\n",
      "0.8588472742405326\n",
      "Epoch[430/500], Train loss:  0.3701\n",
      "Epoch[430/500], Val loss:  0.3955\n",
      "0.8581523096129837\n",
      "Epoch[431/500], Train loss:  0.3679\n",
      "Epoch[431/500], Val loss:  0.3981\n",
      "0.8638743237619643\n",
      "Epoch[432/500], Train loss:  0.3760\n",
      "Epoch[432/500], Val loss:  0.4017\n",
      "0.8557469829379942\n",
      "Epoch[433/500], Train loss:  0.3702\n",
      "Epoch[433/500], Val loss:  0.3984\n",
      "0.8572034956304618\n",
      "Epoch[434/500], Train loss:  0.3685\n",
      "Epoch[434/500], Val loss:  0.3943\n",
      "0.8614148980441116\n",
      "Epoch[435/500], Train loss:  0.3715\n",
      "Epoch[435/500], Val loss:  0.3959\n",
      "0.8601165210153976\n",
      "Epoch[436/500], Train loss:  0.3706\n",
      "Epoch[436/500], Val loss:  0.3964\n",
      "0.861231793591344\n",
      "Epoch[437/500], Train loss:  0.3712\n",
      "Epoch[437/500], Val loss:  0.3964\n",
      "0.862488555971702\n",
      "Epoch[438/500], Train loss:  0.3681\n",
      "Epoch[438/500], Val loss:  0.3974\n",
      "0.8632334581772785\n",
      "Epoch[439/500], Train loss:  0.3712\n",
      "Epoch[439/500], Val loss:  0.4000\n",
      "0.8604494382022472\n",
      "Epoch[440/500], Train loss:  0.3756\n",
      "Epoch[440/500], Val loss:  0.3999\n",
      "0.8561860174781523\n",
      "Epoch[441/500], Train loss:  0.3706\n",
      "Epoch[441/500], Val loss:  0.3971\n",
      "0.8638743237619643\n",
      "Epoch[442/500], Train loss:  0.3686\n",
      "Epoch[442/500], Val loss:  0.3964\n",
      "0.8607324178110696\n",
      "Epoch[443/500], Train loss:  0.3721\n",
      "Epoch[443/500], Val loss:  0.3972\n",
      "0.8605534748231377\n",
      "Epoch[444/500], Train loss:  0.3694\n",
      "Epoch[444/500], Val loss:  0.3995\n",
      "0.8632209737827715\n",
      "Epoch[445/500], Train loss:  0.3718\n",
      "Epoch[445/500], Val loss:  0.3973\n",
      "0.8643612151477318\n",
      "Epoch[446/500], Train loss:  0.3686\n",
      "Epoch[446/500], Val loss:  0.3980\n",
      "0.8591302538493549\n",
      "Epoch[447/500], Train loss:  0.3671\n",
      "Epoch[447/500], Val loss:  0.4006\n",
      "0.864735746982938\n",
      "Epoch[448/500], Train loss:  0.3700\n",
      "Epoch[448/500], Val loss:  0.3981\n",
      "0.8574157303370787\n",
      "Epoch[449/500], Train loss:  0.3718\n",
      "Epoch[449/500], Val loss:  0.3973\n",
      "0.8615439034540159\n",
      "Epoch[450/500], Train loss:  0.3692\n",
      "Epoch[450/500], Val loss:  0.3945\n",
      "0.8573075322513526\n",
      "Epoch[451/500], Train loss:  0.3715\n",
      "Epoch[451/500], Val loss:  0.3983\n",
      "0.861398252184769\n",
      "Epoch[452/500], Train loss:  0.3676\n",
      "Epoch[452/500], Val loss:  0.3975\n",
      "0.8643695380774032\n",
      "Epoch[453/500], Train loss:  0.3675\n",
      "Epoch[453/500], Val loss:  0.3941\n",
      "0.8639658759883478\n",
      "Epoch[454/500], Train loss:  0.3730\n",
      "Epoch[454/500], Val loss:  0.3977\n",
      "0.8672534332084896\n",
      "Epoch[455/500], Train loss:  0.3707\n",
      "Epoch[455/500], Val loss:  0.3979\n",
      "0.8632875572201415\n",
      "Epoch[456/500], Train loss:  0.3680\n",
      "Epoch[456/500], Val loss:  0.4009\n",
      "0.8594798168955471\n",
      "Epoch[457/500], Train loss:  0.3710\n",
      "Epoch[457/500], Val loss:  0.3957\n",
      "0.8624802330420309\n",
      "Epoch[458/500], Train loss:  0.3720\n",
      "Epoch[458/500], Val loss:  0.3998\n",
      "0.8601123595505616\n",
      "Epoch[459/500], Train loss:  0.3714\n",
      "Epoch[459/500], Val loss:  0.3980\n",
      "0.8610112359550562\n",
      "Epoch[460/500], Train loss:  0.3675\n",
      "Epoch[460/500], Val loss:  0.3994\n",
      "0.8596213066999583\n",
      "Epoch[461/500], Train loss:  0.3694\n",
      "Epoch[461/500], Val loss:  0.3969\n",
      "0.8639450686641698\n",
      "Epoch[462/500], Train loss:  0.3736\n",
      "Epoch[462/500], Val loss:  0.3976\n",
      "0.8612650853100292\n",
      "Epoch[463/500], Train loss:  0.3709\n",
      "Epoch[463/500], Val loss:  0.3980\n",
      "0.8608489388264668\n",
      "Epoch[464/500], Train loss:  0.3774\n",
      "Epoch[464/500], Val loss:  0.4006\n",
      "0.8604119850187265\n",
      "Epoch[465/500], Train loss:  0.3708\n",
      "Epoch[465/500], Val loss:  0.3971\n",
      "0.8631876820640866\n",
      "Epoch[466/500], Train loss:  0.3703\n",
      "Epoch[466/500], Val loss:  0.3972\n",
      "0.8621681231793592\n",
      "Epoch[467/500], Train loss:  0.3739\n",
      "Epoch[467/500], Val loss:  0.3962\n",
      "0.8623761964211403\n",
      "Epoch[468/500], Train loss:  0.3693\n",
      "Epoch[468/500], Val loss:  0.3999\n",
      "0.8598543487307534\n",
      "Epoch[469/500], Train loss:  0.3669\n",
      "Epoch[469/500], Val loss:  0.3972\n",
      "0.8582771535580525\n",
      "Epoch[470/500], Train loss:  0.3684\n",
      "Epoch[470/500], Val loss:  0.3977\n",
      "0.8586142322097378\n",
      "Epoch[471/500], Train loss:  0.3708\n",
      "Epoch[471/500], Val loss:  0.3976\n",
      "0.860516021639617\n",
      "Epoch[472/500], Train loss:  0.3695\n",
      "Epoch[472/500], Val loss:  0.3965\n",
      "0.8609800249687891\n",
      "Epoch[473/500], Train loss:  0.3724\n",
      "Epoch[473/500], Val loss:  0.3960\n",
      "0.8637245110278818\n",
      "Epoch[474/500], Train loss:  0.3691\n",
      "Epoch[474/500], Val loss:  0.3964\n",
      "0.8591968372867249\n",
      "Epoch[475/500], Train loss:  0.3732\n",
      "Epoch[475/500], Val loss:  0.3981\n",
      "0.862875572201415\n",
      "Epoch[476/500], Train loss:  0.3716\n",
      "Epoch[476/500], Val loss:  0.3953\n",
      "0.860561797752809\n",
      "Epoch[477/500], Train loss:  0.3685\n",
      "Epoch[477/500], Val loss:  0.3962\n",
      "0.8597045359966708\n",
      "Epoch[478/500], Train loss:  0.3678\n",
      "Epoch[478/500], Val loss:  0.4009\n",
      "0.8600832292967124\n",
      "Epoch[479/500], Train loss:  0.3690\n",
      "Epoch[479/500], Val loss:  0.3985\n",
      "0.8579359134415314\n",
      "Epoch[480/500], Train loss:  0.3707\n",
      "Epoch[480/500], Val loss:  0.3971\n",
      "0.8611277569704536\n",
      "Epoch[481/500], Train loss:  0.3687\n",
      "Epoch[481/500], Val loss:  0.3971\n",
      "0.8613566375364129\n",
      "Epoch[482/500], Train loss:  0.3694\n",
      "Epoch[482/500], Val loss:  0.3968\n",
      "0.862501040366209\n",
      "Epoch[483/500], Train loss:  0.3727\n",
      "Epoch[483/500], Val loss:  0.3990\n",
      "0.8609280066583438\n",
      "Epoch[484/500], Train loss:  0.3710\n",
      "Epoch[484/500], Val loss:  0.3962\n",
      "0.8629088639200999\n",
      "Epoch[485/500], Train loss:  0.3661\n",
      "Epoch[485/500], Val loss:  0.3966\n",
      "0.863029546400333\n",
      "Epoch[486/500], Train loss:  0.3694\n",
      "Epoch[486/500], Val loss:  0.3972\n",
      "0.8594382022471909\n",
      "Epoch[487/500], Train loss:  0.3676\n",
      "Epoch[487/500], Val loss:  0.3966\n",
      "0.8612526009155222\n",
      "Epoch[488/500], Train loss:  0.3715\n",
      "Epoch[488/500], Val loss:  0.3983\n",
      "0.8588805659592178\n",
      "Epoch[489/500], Train loss:  0.3708\n",
      "Epoch[489/500], Val loss:  0.3983\n",
      "0.8638868081564711\n",
      "Epoch[490/500], Train loss:  0.3721\n",
      "Epoch[490/500], Val loss:  0.3958\n",
      "0.8637744486059094\n",
      "Epoch[491/500], Train loss:  0.3679\n",
      "Epoch[491/500], Val loss:  0.3963\n",
      "0.862742405326675\n",
      "Epoch[492/500], Train loss:  0.3672\n",
      "Epoch[492/500], Val loss:  0.3981\n",
      "0.8636121514773201\n",
      "Epoch[493/500], Train loss:  0.3691\n",
      "Epoch[493/500], Val loss:  0.3975\n",
      "0.8606367041198504\n",
      "Epoch[494/500], Train loss:  0.3706\n",
      "Epoch[494/500], Val loss:  0.3960\n",
      "0.8625260091552227\n",
      "Epoch[495/500], Train loss:  0.3686\n",
      "Epoch[495/500], Val loss:  0.3966\n",
      "0.8584935497295048\n",
      "Epoch[496/500], Train loss:  0.3686\n",
      "Epoch[496/500], Val loss:  0.3962\n",
      "0.8636579275905121\n",
      "Epoch[497/500], Train loss:  0.3721\n",
      "Epoch[497/500], Val loss:  0.4012\n",
      "0.8579442363712027\n",
      "Epoch[498/500], Train loss:  0.3680\n",
      "Epoch[498/500], Val loss:  0.3971\n",
      "0.8588847274240533\n",
      "Epoch[499/500], Train loss:  0.3679\n",
      "Epoch[499/500], Val loss:  0.3947\n",
      "0.8609030378693299\n",
      "Epoch[500/500], Train loss:  0.3685\n",
      "Epoch[500/500], Val loss:  0.3972\n",
      "0.8630253849354973\n"
     ]
    }
   ],
   "execution_count": 67
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
