{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T07:29:02.760838Z",
     "start_time": "2024-05-18T07:29:02.510951Z"
    }
   },
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "\n",
    "from customDatasets.audioDataset import AudioDataset\n"
   ],
   "execution_count": 75,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:29:02.768262Z",
     "start_time": "2024-05-18T07:29:02.762082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# free gpu\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "f968c3c21dc371bb",
   "execution_count": 76,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:29:02.778474Z",
     "start_time": "2024-05-18T07:29:02.769897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConvolutionalAE(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(ConvolutionalAE, self).__init__()\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (320, 128)\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=(1,2), padding=(2,2)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # (320, 64)\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=(1,2), padding=(2,2)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # (320, 32)\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=(2,2), padding=(2,2)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # (160, 16)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=(2,2), padding=(1,1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            # (80, 8)\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=(2,2), padding=(1,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            # (40, 4, 512)\n",
    "        )\n",
    "               \n",
    "        # inflates the latent space to the shape of the last layer of the encoder\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(512*40*4, self.encoding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.encoding_dim, 512*40*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "            # (512, 40, 4)\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=(2,2), padding=(1,1),output_padding=(1,1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            # (256, 80, 8)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=(2,2), padding=(1,1),output_padding=(1,1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # (128, 160, 16)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=(2,2), padding=(1,1),output_padding=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # (64, 320, 32)\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=5, stride=(1,2), padding=(2,2),output_padding=(0,1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # (32, 320, 64)\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=5, stride=(1,2), padding=(2,2),output_padding=(0,1)),\n",
    "            # (1, 320, 128)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        encoded = encoded.view(-1, 512*40*4)\n",
    "        x=self.fc(encoded)\n",
    "        x = x.view(-1, 512, 40, 4)\n",
    "        decoded = self.decoder(x)\n",
    "        \n",
    "        return decoded"
   ],
   "id": "86b9d0ccc25a1707",
   "execution_count": 77,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:29:02.788192Z",
     "start_time": "2024-05-18T07:29:02.780558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_dl, val_dl, test_dl, criterion, optimizer, device, epochs=5, step_size=5):\n",
    "    lr_scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for inputs, labels in train_dl:\n",
    "            model.train()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        lr_scheduler.step()\n",
    "        print(f'Epoch[{epoch + 1}/{epochs}], Train loss: {np.average(train_losses): .4f}')\n",
    "        \n",
    "        \n",
    "        for inputs, labels in val_dl:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                val_losses.append(loss.item())\n",
    "        print(f'Epoch[{epoch + 1}/{epochs}], Val loss: {np.average(val_losses): .4f}')\n",
    " \n",
    "        scores = []\n",
    "        full_labels = []\n",
    "        for inputs, labels in test_dl:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                mse = torch.sum((outputs-inputs)**2,dim=(1,2,3))/(inputs.shape[1]*inputs.shape[2]*inputs.shape[3])            \n",
    "                scores.append(mse)\n",
    "                full_labels.append(labels)\n",
    "        \n",
    "        full_labels = torch.cat([label for label in full_labels])\n",
    "        scores = torch.cat([score for score in scores])\n",
    "        fpr, tpr, _ = roc_curve(full_labels.cpu().detach(), scores.cpu().detach(), pos_label=0)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print(roc_auc)\n",
    "        \n",
    "    return np.average(train_losses),np.average(val_losses),roc_auc"
   ],
   "id": "f56f0745d3302d87",
   "execution_count": 78,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:29:02.801914Z",
     "start_time": "2024-05-18T07:29:02.789416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 10,\n",
    "    \"num_classes\": 2,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"test_batch_size\": 128,\n",
    "    \"criterion\": nn.MSELoss(),\n",
    "    \"device\":\n",
    "        torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "}\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "data_path = \"./data/train/\"\n",
    "data_path_test = \"./data/test/\"\n",
    "\n",
    "\n",
    "meta_train_df = pd.read_csv(\"./data/train.csv\")\n",
    "meta_test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "train_df = meta_train_df[['filename', 'is_normal', 'machine_id']]\n",
    "train_dataset = AudioDataset(train_df, data_path)\n",
    "test_df = meta_test_df[['filename', 'is_normal', 'machine_id']]\n",
    "test_dataset = AudioDataset(test_df, data_path_test)\n",
    "\n",
    "num_items = len(train_dataset)\n",
    "num_train = int(0.8 * num_items)\n",
    "num_val = num_items-num_train\n",
    "\n",
    "train_ds, val_ds = random_split(train_dataset, [num_train, num_val])\n",
    "test_ds = test_dataset\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=CONFIG['train_batch_size'], shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=CONFIG['val_batch_size'], shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=CONFIG[\"test_batch_size\"], shuffle=True)"
   ],
   "id": "36ce87d24bd72ad5",
   "execution_count": 79,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:29:03.093705Z",
     "start_time": "2024-05-18T07:29:02.803179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ConvolutionalAE(encoding_dim=128)\n",
    "model = model.to(CONFIG[\"device\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "for batch in train_dl:\n",
    "    inputs, labels = batch\n",
    "    print(inputs.shape)\n",
    "    inputs = inputs.to(CONFIG[\"device\"])\n",
    "    outputs = model(inputs)\n",
    "    print(outputs.shape)\n",
    "    break"
   ],
   "id": "8b01c9d83af18dc7",
   "execution_count": 80,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:29:10.953062Z",
     "start_time": "2024-05-18T07:29:03.094895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute metrics\n",
    "inputs_cat=[]\n",
    "for inputs, labels in train_dl:\n",
    "    inputs_cat.append(inputs)\n",
    "inputs_cat = torch.cat([input for input in inputs_cat])\n",
    "print(inputs_cat.shape)"
   ],
   "id": "ed6681960638cdc",
   "execution_count": 81,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:29:11.167609Z",
     "start_time": "2024-05-18T07:29:10.954258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute the min and max value for each frequency of the batch_sizexchannelxtimexfrequecy\n",
    "min = torch.min(inputs_cat, dim=0).values\n",
    "max = torch.max(inputs_cat, dim=0).values\n",
    "print(max.shape)\n",
    "print(min.shape)\n",
    "train_dataset.min = min\n",
    "train_dataset.max = max\n",
    "test_dataset.min = min\n",
    "test_dataset.max = max\n",
    "measures = []"
   ],
   "id": "612f0834c93b8867",
   "execution_count": 82,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:48:56.573650Z",
     "start_time": "2024-05-18T07:29:11.168651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training=True\n",
    "\n",
    "# testing emb space size\n",
    "if training:\n",
    "    for emb_space_size in [32, 64, 128, 256, 512]:\n",
    "        model = ConvolutionalAE(encoding_dim=emb_space_size)\n",
    "        model = model.to(CONFIG[\"device\"])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "        measures.append(train_model(model, train_dl, val_dl, test_dl, CONFIG[\"criterion\"], optimizer, CONFIG[\"device\"], CONFIG[\"epochs\"]))\n",
    "    for emb_space_size, measure in zip([32, 64, 128, 256, 512], measures):\n",
    "        print(f\"Emb space size: {emb_space_size}, Train loss: {measure[0]}, Val loss: {measure[1]}, ROC AUC: {measure[2]}\")"
   ],
   "id": "9a992a409c40866e",
   "execution_count": 83,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T07:48:56.625428Z",
     "start_time": "2024-05-18T07:48:56.575948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# take the best one and train it for more epochs\n",
    "if training:\n",
    "    emb_space_measures=[32, 64, 128, 256, 512]\n",
    "    model = ConvolutionalAE(encoding_dim=emb_space_measures[np.argmax([measure[2] for measure in measures])])\n",
    "    model = model.to(CONFIG[\"device\"])"
   ],
   "id": "b7de0bec6b2f58e8",
   "execution_count": 84,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T08:08:10.105251Z",
     "start_time": "2024-05-18T07:48:56.626517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if training:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=+0.01)\n",
    "    train_model(model, train_dl, val_dl, test_dl, CONFIG[\"criterion\"], optimizer, CONFIG[\"device\"], 50, 20)"
   ],
   "id": "acae56582dfab61b",
   "execution_count": 85,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T08:08:10.339423Z",
     "start_time": "2024-05-18T08:08:10.106650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#save weights into weights/weights.pth\n",
    "if training:\n",
    "    torch.save(model.state_dict(), \"./weights/weights.pth\")"
   ],
   "id": "f76b4ff33a02e258",
   "execution_count": 86,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T08:08:10.417818Z",
     "start_time": "2024-05-18T08:08:10.340482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_index=np.argmax([measure[2] for measure in measures]) if training else 1\n",
    "model=ConvolutionalAE(encoding_dim=[32, 64, 128, 256, 512][best_index])\n",
    "model.load_state_dict(torch.load(\"./weights/weights.pth\"))\n",
    "model=model.to(CONFIG[\"device\"])\n",
    "train_dataset.with_id=True\n",
    "test_dataset.with_filename=True"
   ],
   "id": "26d868498eb9ddfc",
   "execution_count": 87,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T08:08:12.598980Z",
     "start_time": "2024-05-18T08:08:10.418808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute the average mse for each id in the val_dl\n",
    "mse_dict = {}\n",
    "for inputs, labels, ids in val_dl:\n",
    "    inputs, labels = inputs.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        diff=outputs-inputs\n",
    "        for id,diff in zip(ids,diff):\n",
    "            if id in mse_dict:\n",
    "                mse_dict[id.item()].append((torch.sum(diff**2)/(inputs.shape[1]*inputs.shape[2]*inputs.shape[3])).item())\n",
    "            else:\n",
    "                mse_dict[id.item()]=[(torch.sum(diff**2)/(inputs.shape[1]*inputs.shape[2]*inputs.shape[3])).item()]\n",
    "\n",
    "print({key:np.average(value) for key,value in mse_dict.items()})"
   ],
   "id": "2d235d6477c4fd0b",
   "execution_count": 88,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T08:08:13.294194Z",
     "start_time": "2024-05-18T08:08:12.600241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, labels, ids in test_dl:\n",
    "    inputs, labels = inputs.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        mse = torch.sum((outputs-inputs)**2,dim=(1,2,3))/(inputs.shape[1]*inputs.shape[2]*inputs.shape[3])\n",
    "        for name,mse in zip(ids,mse):\n",
    "            print(name,mse)\n",
    "        break"
   ],
   "id": "e6eeb8ffa5e3f4d6",
   "execution_count": 89,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
